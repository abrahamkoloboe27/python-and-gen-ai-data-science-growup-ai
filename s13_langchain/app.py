"""
S13 ‚Äî LangChain Application
Application d√©montrant les patterns LangChain essentiels:
- Chains (LLM, Sequential)
- Memory (Conversation)
- Q&A avec contexte
"""

import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_classic.chains import LLMChain, SequentialChain, SimpleSequentialChain, ConversationChain
from langchain_classic.memory import ConversationBufferMemory, ConversationBufferWindowMemory
from langchain_core.messages import HumanMessage, SystemMessage
import logging

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# Charger les variables d'environnement
load_dotenv()

# Configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL_NAME = os.getenv("MODEL_NAME", "gpt-3.5-turbo")
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.7"))
MAX_TOKENS = int(os.getenv("MAX_TOKENS", "500"))


class LangChainApp:
    """Application principale d√©montrant les patterns LangChain"""
    
    def __init__(self):
        """Initialiser l'application"""
        if not OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY non trouv√©e dans .env")
        
        # Initialiser le mod√®le
        self.llm = ChatOpenAI(
            model_name=MODEL_NAME,
            temperature=TEMPERATURE,
            max_tokens=MAX_TOKENS
        )
        
        logger.info(f"‚úÖ LangChain app initialis√©e avec {MODEL_NAME}")
    
    # ============================================================
    # PATTERN 1: Simple Chain
    # ============================================================
    
    def simple_chain_demo(self, topic: str) -> str:
        """
        D√©monstration d'une chain simple avec prompt template
        
        Args:
            topic: Sujet √† expliquer
            
        Returns:
            Explication g√©n√©r√©e
        """
        logger.info(f"Simple chain: topic='{topic}'")
        
        # Cr√©er le prompt template
        template = """Tu es un professeur expert. Explique le concept suivant de mani√®re claire et concise.

Concept: {topic}

Explication:"""
        
        prompt = PromptTemplate(
            input_variables=["topic"],
            template=template
        )
        
        # Cr√©er la chain
        chain = LLMChain(llm=self.llm, prompt=prompt)
        
        # Ex√©cuter
        response = chain.run(topic=topic)
        logger.info(f"R√©ponse re√ßue: {len(response)} caract√®res")
        
        return response
    
    # ============================================================
    # PATTERN 2: Sequential Chain
    # ============================================================
    
    def sequential_chain_demo(self, text: str) -> dict:
        """
        D√©monstration d'une sequential chain (r√©sum√© ‚Üí analyse)
        
        Args:
            text: Texte √† analyser
            
        Returns:
            Dict avec r√©sum√© et analyse
        """
        logger.info(f"Sequential chain: text length={len(text)}")
        
        # Chain 1: R√©sum√©
        summary_template = """R√©sume le texte suivant en 2-3 phrases:

Texte: {text}

R√©sum√©:"""
        
        summary_prompt = PromptTemplate(
            input_variables=["text"],
            template=summary_template
        )
        
        summary_chain = LLMChain(
            llm=self.llm,
            prompt=summary_prompt,
            output_key="summary"
        )
        
        # Chain 2: Analyse de sentiment
        analysis_template = """Analyse le sentiment du r√©sum√© suivant et donne un score de 1 √† 5.

R√©sum√©: {summary}

Analyse (sentiment et score):"""
        
        analysis_prompt = PromptTemplate(
            input_variables=["summary"],
            template=analysis_template
        )
        
        analysis_chain = LLMChain(
            llm=self.llm,
            prompt=analysis_prompt,
            output_key="analysis"
        )
        
        # Combiner les chains
        overall_chain = SequentialChain(
            chains=[summary_chain, analysis_chain],
            input_variables=["text"],
            output_variables=["summary", "analysis"],
            verbose=True
        )
        
        result = overall_chain({"text": text})
        logger.info("Sequential chain termin√©e")
        
        return result
    
    # ============================================================
    # PATTERN 3: Conversation avec M√©moire
    # ============================================================
    
    def conversation_demo(self, memory_type: str = "buffer_window"):
        """
        D√©monstration d'une conversation avec m√©moire
        
        Args:
            memory_type: Type de m√©moire ('buffer', 'buffer_window')
        """
        logger.info(f"Conversation d√©marr√©e (memory_type={memory_type})")
        
        # Choisir le type de m√©moire
        if memory_type == "buffer":
            memory = ConversationBufferMemory()
        elif memory_type == "buffer_window":
            memory = ConversationBufferWindowMemory(k=5)  # Garde 5 derniers messages
        else:
            raise ValueError(f"Type de m√©moire non support√©: {memory_type}")
        
        # Cr√©er la conversation chain
        conversation = ConversationChain(
            llm=self.llm,
            memory=memory,
            verbose=True
        )
        
        print("\n" + "="*60)
        print("üí¨ CONVERSATION INTERACTIVE")
        print("="*60)
        print("Tapez vos messages (ou 'quit' pour quitter)")
        print("La m√©moire conserve le contexte de la conversation\n")
        
        while True:
            user_input = input("Vous: ")
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                logger.info("Conversation termin√©e")
                break
            
            try:
                response = conversation.predict(input=user_input)
                print(f"\nAssistant: {response}\n")
                logger.info(f"Tour de conversation: input={len(user_input)} chars, output={len(response)} chars")
            except Exception as e:
                logger.error(f"Erreur: {e}")
                print(f"\n‚ùå Erreur: {e}\n")
    
    # ============================================================
    # PATTERN 4: Q&A avec Contexte
    # ============================================================
    
    def qa_demo(self, context: str, question: str) -> str:
        """
        D√©monstration de Q&A avec contexte fourni
        
        Args:
            context: Contexte/document pour r√©pondre
            question: Question √† r√©pondre
            
        Returns:
            R√©ponse bas√©e sur le contexte
        """
        logger.info(f"Q&A: context={len(context)} chars, question='{question}'")
        
        # Cr√©er le prompt template
        template = """R√©ponds √† la question en te basant UNIQUEMENT sur le contexte fourni.
Si la r√©ponse n'est pas dans le contexte, dis "Je ne peux pas r√©pondre d'apr√®s le contexte fourni."

Contexte: {context}

Question: {question}

R√©ponse:"""
        
        prompt = PromptTemplate(
            input_variables=["context", "question"],
            template=template
        )
        
        # Cr√©er la chain
        chain = LLMChain(llm=self.llm, prompt=prompt)
        
        # Ex√©cuter
        response = chain.run(context=context, question=question)
        logger.info(f"R√©ponse Q&A g√©n√©r√©e")
        
        return response
    
    # ============================================================
    # PATTERN 5: Few-Shot Learning
    # ============================================================
    
    def few_shot_demo(self, user_input: str) -> str:
        """
        D√©monstration de few-shot learning avec exemples
        
        Args:
            user_input: Entr√©e utilisateur √† classifier
            
        Returns:
            Classification
        """
        logger.info(f"Few-shot classification: input='{user_input}'")
        
        template = """Classifie les phrases suivantes par sentiment (positif, n√©gatif, neutre).

Exemples:
Phrase: "J'adore ce produit!"
Sentiment: positif

Phrase: "C'est d√©cevant."
Sentiment: n√©gatif

Phrase: "Le produit est arriv√©."
Sentiment: neutre

Maintenant, classifie cette phrase:
Phrase: {input}
Sentiment:"""
        
        prompt = PromptTemplate(
            input_variables=["input"],
            template=template
        )
        
        chain = LLMChain(llm=self.llm, prompt=prompt)
        response = chain.run(input=user_input)
        
        logger.info(f"Classification: {response.strip()}")
        
        return response.strip()


def main():
    """Fonction principale pour tester l'application"""
    
    print("\n" + "="*60)
    print("ü¶ú‚õìÔ∏è  LANGCHAIN PATTERNS DEMO")
    print("="*60)
    
    try:
        app = LangChainApp()
        
        # Menu interactif
        while True:
            print("\n" + "-"*60)
            print("Choisissez une d√©mo:")
            print("1. Simple Chain (explication de concept)")
            print("2. Sequential Chain (r√©sum√© + analyse)")
            print("3. Conversation avec m√©moire")
            print("4. Q&A avec contexte")
            print("5. Few-shot classification")
            print("0. Quitter")
            print("-"*60)
            
            choice = input("\nVotre choix: ").strip()
            
            if choice == "0":
                print("\nüëã Au revoir!")
                break
            
            elif choice == "1":
                topic = input("\nSujet √† expliquer: ")
                response = app.simple_chain_demo(topic)
                print(f"\nüìù Explication:\n{response}\n")
            
            elif choice == "2":
                text = input("\nTexte √† analyser: ")
                result = app.sequential_chain_demo(text)
                print(f"\nüìä R√©sum√©:\n{result['summary']}\n")
                print(f"üìä Analyse:\n{result['analysis']}\n")
            
            elif choice == "3":
                memory_type = input("\nType de m√©moire (buffer/buffer_window): ").strip()
                if memory_type not in ["buffer", "buffer_window"]:
                    memory_type = "buffer_window"
                app.conversation_demo(memory_type)
            
            elif choice == "4":
                print("\nExemple de contexte:")
                context = """Le RAG (Retrieval-Augmented Generation) est une technique qui combine 
la recherche d'information (retrieval) avec la g√©n√©ration de texte. 
Il permet aux LLMs d'acc√©der √† des connaissances externes via un syst√®me de recherche vectorielle."""
                
                question = input("\nVotre question: ")
                response = app.qa_demo(context, question)
                print(f"\nüí° R√©ponse:\n{response}\n")
            
            elif choice == "5":
                phrase = input("\nPhrase √† classifier: ")
                sentiment = app.few_shot_demo(phrase)
                print(f"\nüéØ Sentiment: {sentiment}\n")
            
            else:
                print("\n‚ùå Choix invalide\n")
    
    except Exception as e:
        logger.error(f"Erreur: {e}")
        print(f"\n‚ùå Erreur: {e}")


if __name__ == "__main__":
    main()
