"""
S14 ‚Äî LangGraph Flow Implementation
Pipeline: Retrieve ‚Üí Summarize ‚Üí Decide ‚Üí Action/Escalate
"""

import os
from typing import TypedDict, List, Literal, Annotated
from dotenv import load_dotenv
try:
    from langchain_openai import ChatOpenAI
except ImportError:
    # Fallback pour anciennes versions de LangChain
    from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import HumanMessage
from langgraph.graph import StateGraph, END
import random
import logging

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# Charger les variables d'environnement
load_dotenv()

# Configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL_NAME = os.getenv("MODEL_NAME", "gpt-3.5-turbo")
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.3"))
CONFIDENCE_THRESHOLD = float(os.getenv("CONFIDENCE_THRESHOLD", "0.7"))


# ============================================================
# D√âFINITION DE L'√âTAT
# ============================================================

class GraphState(TypedDict):
    """√âtat partag√© entre tous les n≈ìuds du graphe"""
    query: str                    # Requ√™te utilisateur
    documents: List[str]          # Documents r√©cup√©r√©s
    summary: str                  # R√©sum√© g√©n√©r√©
    confidence: float             # Score de confiance (0-1)
    action: str                   # Action finale
    metadata: dict                # M√©tadonn√©es additionnelles


# ============================================================
# SIMULATION D'UNE BASE VECTORIELLE (pour d√©mo)
# ============================================================

KNOWLEDGE_BASE = [
    {
        "id": 1,
        "text": "Pour r√©initialiser votre mot de passe, allez dans Param√®tres > S√©curit√© > R√©initialiser le mot de passe.",
        "category": "compte"
    },
    {
        "id": 2,
        "text": "Notre service client est disponible du lundi au vendredi de 9h √† 18h. Vous pouvez nous contacter par email ou t√©l√©phone.",
        "category": "support"
    },
    {
        "id": 3,
        "text": "La livraison standard prend 3-5 jours ouvrables. La livraison express est disponible pour 10‚Ç¨ suppl√©mentaires.",
        "category": "livraison"
    },
    {
        "id": 4,
        "text": "Pour annuler votre abonnement, rendez-vous dans Mon Compte > Abonnement > Annuler. Le remboursement est trait√© sous 7 jours.",
        "category": "abonnement"
    },
    {
        "id": 5,
        "text": "Nos produits sont garantis 2 ans. En cas de d√©faut, contactez notre service apr√®s-vente avec votre num√©ro de commande.",
        "category": "garantie"
    },
]


def simulate_vector_search(query: str, top_k: int = 3) -> List[dict]:
    """
    Simule une recherche vectorielle (en production, utiliser FAISS/Milvus)
    """
    # Simple keyword matching pour la d√©mo
    query_lower = query.lower()
    scored_docs = []
    
    for doc in KNOWLEDGE_BASE:
        score = 0
        text_lower = doc["text"].lower()
        
        # Score basique bas√© sur les mots communs
        query_words = set(query_lower.split())
        text_words = set(text_lower.split())
        common_words = query_words.intersection(text_words)
        score = len(common_words)
        
        if score > 0:
            scored_docs.append((score, doc))
    
    # Trier par score et prendre top_k
    scored_docs.sort(reverse=True, key=lambda x: x[0])
    results = [doc for score, doc in scored_docs[:top_k]]
    
    # Si aucun r√©sultat, retourner quelques docs al√©atoires
    if not results:
        results = random.sample(KNOWLEDGE_BASE, min(top_k, len(KNOWLEDGE_BASE)))
    
    return results


# ============================================================
# N≈íUDS DU GRAPHE
# ============================================================

def retrieve_node(state: GraphState) -> GraphState:
    """
    N≈ìud 1: R√©cup√©rer les documents pertinents
    """
    query = state["query"]
    logger.info(f"üîç RETRIEVE: Recherche pour '{query}'")
    
    # Recherche vectorielle simul√©e
    docs = simulate_vector_search(query, top_k=3)
    doc_texts = [doc["text"] for doc in docs]
    
    logger.info(f"   ‚Üí {len(doc_texts)} documents r√©cup√©r√©s")
    
    return {
        **state,
        "documents": doc_texts,
        "metadata": {
            **state.get("metadata", {}),
            "num_docs_retrieved": len(doc_texts)
        }
    }


def summarize_node(state: GraphState) -> GraphState:
    """
    N≈ìud 2: R√©sumer et g√©n√©rer une r√©ponse
    """
    query = state["query"]
    documents = state["documents"]
    
    logger.info(f"üìù SUMMARIZE: G√©n√©ration de r√©ponse")
    
    # Cr√©er le prompt
    context = "\n\n".join([f"Document {i+1}: {doc}" for i, doc in enumerate(documents)])
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", "Tu es un assistant qui r√©pond aux questions bas√©es sur des documents fournis. Sois concis et pr√©cis."),
        ("human", f"""Question: {query}

Contexte:
{context}

R√©ponds √† la question en te basant sur le contexte. Si la r√©ponse n'est pas dans le contexte, dis-le clairement.""")
    ])
    
    # Appeler le LLM
    llm = ChatOpenAI(model_name=MODEL_NAME, temperature=TEMPERATURE)
    messages = prompt.format_messages()
    response = llm(messages)
    summary = response.content
    
    # Calculer un score de confiance (simplifi√©)
    confidence = calculate_confidence(summary, documents)
    
    logger.info(f"   ‚Üí R√©sum√© g√©n√©r√© (confiance: {confidence:.2f})")
    
    return {
        **state,
        "summary": summary,
        "confidence": confidence,
        "metadata": {
            **state.get("metadata", {}),
            "summary_length": len(summary)
        }
    }


def calculate_confidence(summary: str, documents: List[str]) -> float:
    """
    Calculer un score de confiance bas√© sur la r√©ponse
    """
    # Heuristiques simples pour la d√©mo
    confidence = 0.5
    
    # Augmenter si la r√©ponse contient des informations des documents
    for doc in documents:
        doc_words = set(doc.lower().split())
        summary_words = set(summary.lower().split())
        overlap = len(doc_words.intersection(summary_words))
        if overlap > 5:
            confidence += 0.2
    
    # Diminuer si la r√©ponse contient des phrases d'incertitude
    uncertainty_phrases = ["je ne sais pas", "pas dans le contexte", "je ne peux pas"]
    for phrase in uncertainty_phrases:
        if phrase in summary.lower():
            confidence -= 0.3
    
    # Borner entre 0 et 1
    confidence = max(0.0, min(1.0, confidence))
    
    return confidence


def decide_node(state: GraphState) -> Literal["action", "escalate"]:
    """
    N≈ìud 3: D√©cider de la prochaine √©tape bas√©e sur la confiance
    """
    confidence = state["confidence"]
    
    logger.info(f"ü§î DECIDE: Confiance = {confidence:.2f}, Seuil = {CONFIDENCE_THRESHOLD}")
    
    if confidence >= CONFIDENCE_THRESHOLD:
        logger.info(f"   ‚Üí Route vers ACTION")
        return "action"
    else:
        logger.info(f"   ‚Üí Route vers ESCALATE")
        return "escalate"


def action_node(state: GraphState) -> GraphState:
    """
    N≈ìud 4: Ex√©cuter l'action appropri√©e
    """
    logger.info(f"‚úÖ ACTION: Fournir la r√©ponse")
    
    action = f"R√©ponse fournie: {state['summary']}"
    
    return {
        **state,
        "action": action,
        "metadata": {
            **state.get("metadata", {}),
            "final_node": "action"
        }
    }


def escalate_node(state: GraphState) -> GraphState:
    """
    N≈ìud 5: Escalader vers un humain
    """
    logger.info(f"‚ö†Ô∏è  ESCALATE: Confiance faible, escalade n√©cessaire")
    
    action = (
        f"Escalade vers support humain requise.\n"
        f"R√©ponse tentative: {state['summary']}\n"
        f"Confiance: {state['confidence']:.2f}"
    )
    
    return {
        **state,
        "action": action,
        "metadata": {
            **state.get("metadata", {}),
            "final_node": "escalate"
        }
    }


# ============================================================
# CONSTRUCTION DU GRAPHE
# ============================================================

class RAGWorkflow:
    """Classe principale pour le workflow RAG avec LangGraph"""
    
    def __init__(self):
        """Initialiser le workflow"""
        if not OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY non trouv√©e dans .env")
        
        # Cr√©er le graphe
        self.graph = self._build_graph()
        logger.info("‚úÖ Workflow RAG initialis√©")
    
    def _build_graph(self) -> StateGraph:
        """Construire le graphe d'√©tats"""
        
        # Cr√©er le graphe
        workflow = StateGraph(GraphState)
        
        # Ajouter les n≈ìuds
        workflow.add_node("retrieve", retrieve_node)
        workflow.add_node("summarize", summarize_node)
        workflow.add_node("action", action_node)
        workflow.add_node("escalate", escalate_node)
        
        # D√©finir le point d'entr√©e
        workflow.set_entry_point("retrieve")
        
        # Ajouter les edges
        workflow.add_edge("retrieve", "summarize")
        
        # Ajouter conditional edge (branchement)
        workflow.add_conditional_edges(
            "summarize",
            decide_node,
            {
                "action": "action",
                "escalate": "escalate"
            }
        )
        
        # Terminer les deux branches
        workflow.add_edge("action", END)
        workflow.add_edge("escalate", END)
        
        # Compiler le graphe
        return workflow.compile()
    
    def run(self, query: str) -> GraphState:
        """
        Ex√©cuter le workflow avec une requ√™te
        
        Args:
            query: Question de l'utilisateur
            
        Returns:
            √âtat final avec la r√©ponse
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"üöÄ D√âBUT DU WORKFLOW")
        logger.info(f"{'='*60}")
        logger.info(f"Query: {query}\n")
        
        # √âtat initial
        initial_state: GraphState = {
            "query": query,
            "documents": [],
            "summary": "",
            "confidence": 0.0,
            "action": "",
            "metadata": {}
        }
        
        # Ex√©cuter le graphe
        final_state = self.graph.invoke(initial_state)
        
        logger.info(f"\n{'='*60}")
        logger.info(f"‚ú® FIN DU WORKFLOW")
        logger.info(f"{'='*60}\n")
        
        return final_state
    
    def visualize(self, output_path: str = "workflow_graph.png"):
        """
        Visualiser le graphe (n√©cessite graphviz)
        
        Args:
            output_path: Chemin de sortie pour l'image
        """
        try:
            from langchain.graphs import Graph
            # Note: La visualisation n√©cessite des d√©pendances suppl√©mentaires
            logger.info(f"üí° Pour visualiser le graphe, installez: pip install pygraphviz")
        except ImportError:
            logger.warning("Visualisation non disponible (installer pygraphviz)")


# ============================================================
# FONCTION PRINCIPALE
# ============================================================

def main():
    """Fonction principale pour tester le workflow"""
    
    print("\n" + "="*60)
    print("üîÄ LANGGRAPH WORKFLOW DEMO")
    print("="*60)
    
    try:
        # Cr√©er le workflow
        workflow = RAGWorkflow()
        
        # Exemples de requ√™tes √† tester
        test_queries = [
            "Comment r√©initialiser mon mot de passe?",
            "Quels sont les d√©lais de livraison?",
            "Quelle est la couleur du ciel?",  # Question sans r√©ponse dans la KB
        ]
        
        # Menu interactif
        while True:
            print("\n" + "-"*60)
            print("Options:")
            print("1. Tester avec une requ√™te pr√©d√©finie")
            print("2. Entrer une requ√™te personnalis√©e")
            print("3. Tester toutes les requ√™tes pr√©d√©finies")
            print("0. Quitter")
            print("-"*60)
            
            choice = input("\nVotre choix: ").strip()
            
            if choice == "0":
                print("\nüëã Au revoir!")
                break
            
            elif choice == "1":
                print("\nRequ√™tes pr√©d√©finies:")
                for i, q in enumerate(test_queries, 1):
                    print(f"{i}. {q}")
                
                idx = input("\nNum√©ro de la requ√™te: ").strip()
                try:
                    query = test_queries[int(idx) - 1]
                    result = workflow.run(query)
                    print_result(result)
                except (ValueError, IndexError):
                    print("‚ùå Choix invalide")
            
            elif choice == "2":
                query = input("\nVotre requ√™te: ").strip()
                if query:
                    result = workflow.run(query)
                    print_result(result)
            
            elif choice == "3":
                for query in test_queries:
                    print(f"\n{'='*60}")
                    print(f"Test: {query}")
                    print(f"{'='*60}")
                    result = workflow.run(query)
                    print_result(result)
                    input("\nAppuyez sur Entr√©e pour continuer...")
            
            else:
                print("\n‚ùå Choix invalide")
    
    except Exception as e:
        logger.error(f"Erreur: {e}")
        print(f"\n‚ùå Erreur: {e}")


def print_result(result: GraphState):
    """Afficher le r√©sultat format√©"""
    print("\n" + "="*60)
    print("üìä R√âSULTAT")
    print("="*60)
    print(f"\nüîç Documents r√©cup√©r√©s: {len(result['documents'])}")
    print(f"\nüìù R√©sum√©:\n{result['summary']}")
    print(f"\nüìä Confiance: {result['confidence']:.2f}")
    print(f"\n‚úÖ Action:\n{result['action']}")
    print(f"\nüìà M√©tadonn√©es: {result['metadata']}")
    print("\n" + "="*60)


if __name__ == "__main__":
    main()
