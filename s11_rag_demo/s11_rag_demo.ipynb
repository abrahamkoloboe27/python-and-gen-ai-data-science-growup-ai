{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/abrahamkoloboe27/python-and-gen-ai-data-science-growup-ai/blob/main/s11_rag_demo/s11_rag_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S11 ‚Äî RAG (Retrieval-Augmented Generation) : Cours Complet & Impl√©mentation\n",
    "\n",
    "## üéØ Objectifs\n",
    "- Comprendre ce qu'est le RAG et pourquoi l'utiliser\n",
    "- Ma√Ætriser les diff√©rents composants d'un pipeline RAG\n",
    "- Conna√Ætre les diff√©rents types de RAG (Naive, Advanced, Modular/Agentic)\n",
    "- Comprendre la gestion de m√©moire dans les syst√®mes RAG\n",
    "- Impl√©menter un RAG complet **sans LangChain** (OpenAI + ChromaDB + PyMuPDF)\n",
    "- Impl√©menter le m√™me RAG **avec LangChain** pour comparer les deux approches\n",
    "\n",
    "## üìã Contenu\n",
    "1. Introduction au RAG : d√©finition, motivation, architecture\n",
    "2. Les composants du RAG\n",
    "3. Les types de RAG\n",
    "4. Gestion de la m√©moire\n",
    "5. Installation et configuration\n",
    "6. Impl√©mentation RAG sans LangChain\n",
    "7. Impl√©mentation RAG avec LangChain\n",
    "8. Comparaison et bonnes pratiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üß† Section 1 ‚Äî Introduction au RAG\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 Qu'est-ce que le RAG ?\n",
    "\n",
    "**RAG (Retrieval-Augmented Generation)** est une architecture qui am√©liore les LLMs en leur donnant acc√®s √† des sources de connaissances **externes et actualis√©es** au moment de la g√©n√©ration.\n",
    "\n",
    "Au lieu de r√©pondre uniquement avec sa m√©moire interne (les poids du mod√®le), un syst√®me RAG :\n",
    "\n",
    "1. **R√©cup√®re** des documents pertinents depuis une base de connaissances\n",
    "2. **Augmente** le prompt de l'utilisateur avec ces documents\n",
    "3. **G√©n√®re** une r√©ponse fond√©e sur ces documents\n",
    "\n",
    "### Analogie üìö\n",
    "\n",
    "Imaginez un √©tudiant qui passe un examen :\n",
    "- **LLM seul** = √©tudiant qui r√©pond de m√©moire (peut halluciner)\n",
    "- **RAG** = √©tudiant qui a acc√®s √† ses notes et peut les consulter avant de r√©pondre\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2 Pourquoi le RAG ?\n",
    "\n",
    "### Les limites des LLMs sans RAG\n",
    "\n",
    "| Probl√®me | Description | Impact |\n",
    "|----------|-------------|--------|\n",
    "| **Hallucinations** | Le LLM invente des faits | R√©ponses incorrectes |\n",
    "| **Knowledge cutoff** | Connaissance fig√©e √† la date d'entra√Ænement | Info obsol√®te |\n",
    "| **Donn√©es priv√©es** | Le mod√®le ne conna√Æt pas vos donn√©es internes | Pas d'utilisation en entreprise |\n",
    "| **Manque de tra√ßabilit√©** | On ne sait pas d'o√π vient la r√©ponse | Difficile √† auditer |\n",
    "| **Co√ªt de fine-tuning** | Re-entra√Æner pour chaque nouvelle connaissance | Tr√®s cher |\n",
    "\n",
    "### Les avantages du RAG\n",
    "\n",
    "‚úÖ **R√©ponses v√©rifiables** : chaque affirmation est ancr√©e dans un document source  \n",
    "‚úÖ **Connaissances actualis√©es** : il suffit de mettre √† jour la base de donn√©es  \n",
    "‚úÖ **Donn√©es priv√©es** : fonctionne avec vos propres documents  \n",
    "‚úÖ **Co√ªt r√©duit** : pas de re-entra√Ænement n√©cessaire  \n",
    "‚úÖ **Moins d'hallucinations** : le LLM s'appuie sur des faits fournis  \n",
    "\n",
    "---\n",
    "\n",
    "## 1.3 Architecture g√©n√©rale du RAG\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                     PHASE D'INDEXATION                          ‚îÇ\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îÇ  Documents  ‚Üí  Chunking  ‚Üí  Embeddings  ‚Üí  Vector Store        ‚îÇ\n",
    "‚îÇ  (PDF, etc)     (split)     (encode)       (ChromaDB, FAISS)   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                     PHASE DE REQU√äTE                            ‚îÇ\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îÇ  Question  ‚Üí  Embedding  ‚Üí  Retrieval  ‚Üí  Augmentation  ‚Üí  LLM ‚îÇ\n",
    "‚îÇ  (user)       (encode)      (top-k)      (prompt+context)       ‚îÇ\n",
    "‚îÇ                                                      ‚îÇ         ‚îÇ\n",
    "‚îÇ                                                      ‚ñº         ‚îÇ\n",
    "‚îÇ                                                  R√©ponse        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Flux de donn√©es\n",
    "\n",
    "```\n",
    "          INDEXATION                       REQU√äTE\n",
    "         ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                       ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    " PDF ‚îÄ‚îÄ‚ñ∫ Chunks ‚îÄ‚îÄ‚ñ∫ Embeddings          Question\n",
    "                        ‚îÇ                   ‚îÇ\n",
    "                        ‚ñº                   ‚ñº\n",
    "                   ChromaDB ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ Embedding de la question\n",
    "                        ‚îÇ\n",
    "                        ‚îÇ  (similarit√© cosinus)\n",
    "                        ‚ñº\n",
    "                   Top-K chunks\n",
    "                        ‚îÇ\n",
    "                        ‚ñº\n",
    "                   Prompt enrichi ‚îÄ‚îÄ‚ñ∫ GPT-4 ‚îÄ‚îÄ‚ñ∫ R√©ponse\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üîß Section 2 ‚Äî Les Composants du RAG\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 Document Loader (Chargeur de documents)\n",
    "\n",
    "Le point d'entr√©e du pipeline. Il charge et normalise les donn√©es sources.\n",
    "\n",
    "**Sources support√©es :**\n",
    "- üìÑ **PDF** : PyMuPDF (fitz), pypdf, PDFPlumber\n",
    "- üìù **Word/Excel** : python-docx, openpyxl\n",
    "- üåê **Web** : BeautifulSoup, Playwright\n",
    "- üóÉÔ∏è **Base de donn√©es** : SQLAlchemy, connecteurs DB\n",
    "- üìß **Emails/Slack** : connecteurs sp√©cifiques\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2 Text Splitter (D√©coupage en chunks)\n",
    "\n",
    "Un LLM a une **fen√™tre de contexte limit√©e** (ex: 8k, 128k tokens). On ne peut pas injecter des centaines de pages. Il faut donc d√©couper les documents en **chunks** (morceaux).\n",
    "\n",
    "### Strat√©gies de chunking\n",
    "\n",
    "| Strat√©gie | Description | Avantages | Inconv√©nients |\n",
    "|-----------|-------------|-----------|---------------|\n",
    "| **Fixed size** | Chunks de N caract√®res | Simple, uniforme | Coupe parfois au milieu d'une id√©e |\n",
    "| **Overlap** | Chevauchement entre chunks | Pr√©serve le contexte aux jointures | Redondance |\n",
    "| **Sentence** | D√©coupe par phrase | Pr√©serve le sens | Chunks de taille variable |\n",
    "| **Recursive** | Essaie `\\n\\n`, `\\n`, ` ` | Intelligent, adaptable | Plus complexe |\n",
    "| **Semantic** | Regroupement s√©mantique | Meilleure coh√©rence | Co√ªteux en calcul |\n",
    "\n",
    "### Param√®tres cl√©s\n",
    "- **chunk_size** : taille d'un chunk (ex: 500 tokens)\n",
    "- **chunk_overlap** : chevauchement entre chunks (ex: 50 tokens)\n",
    "\n",
    "```\n",
    "Document: [...paragraphe 1...][...paragraphe 2...][...paragraphe 3...]\n",
    "\n",
    "Chunk 1: [‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ500 tokens‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ]\n",
    "Chunk 2:               [‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ500 tokens‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ]\n",
    "                   ‚Üê50‚Üí (overlap)\n",
    "Chunk 3:                              [‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ500 tokens‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2.3 Embedding Model (Mod√®le d'embeddings)\n",
    "\n",
    "Transforme chaque chunk en **vecteur dense** (repr√©sentation num√©rique) qui capture son sens s√©mantique.\n",
    "\n",
    "### Mod√®les populaires\n",
    "\n",
    "| Mod√®le | Dimension | Co√ªt | Performance |\n",
    "|--------|-----------|------|-------------|\n",
    "| `text-embedding-3-small` (OpenAI) | 1536 | Payant | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| `text-embedding-3-large` (OpenAI) | 3072 | Payant | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| `all-MiniLM-L6-v2` (Sentence-Transformers) | 384 | Gratuit | ‚≠ê‚≠ê‚≠ê |\n",
    "| `paraphrase-multilingual-MiniLM-L12-v2` | 384 | Gratuit | ‚≠ê‚≠ê‚≠ê (multi-langue) |\n",
    "| `nomic-embed-text` (Ollama local) | 768 | Gratuit | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "\n",
    "---\n",
    "\n",
    "## 2.4 Vector Store (Base de donn√©es vectorielle)\n",
    "\n",
    "Stocke les embeddings et permet la **recherche par similarit√©** (nearest neighbor search).\n",
    "\n",
    "### Solutions disponibles\n",
    "\n",
    "| Solution | Type | Cas d'usage |\n",
    "|----------|------|-------------|\n",
    "| **ChromaDB** | Local/Cloud | D√©veloppement, MVP |\n",
    "| **FAISS** | Local (in-memory) | Recherche rapide, pas de persistance |\n",
    "| **Pinecone** | Cloud managed | Production √† grande √©chelle |\n",
    "| **Weaviate** | Open-source | Entreprise, GraphQL |\n",
    "| **Qdrant** | Open-source | Haute performance, filtrage avanc√© |\n",
    "| **Milvus** | Open-source | Milliards de vecteurs |\n",
    "\n",
    "> Dans ce notebook, nous utilisons **ChromaDB** : l√©ger, local, aucune configuration serveur requise.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.5 Retriever (R√©cup√©rateur)\n",
    "\n",
    "Prend la question de l'utilisateur, la transforme en embedding, et retrouve les **K chunks les plus similaires** dans le vector store.\n",
    "\n",
    "### Types de recherche\n",
    "\n",
    "- **Dense retrieval** : bas√© sur la similarit√© d'embeddings (s√©mantique)\n",
    "- **Sparse retrieval** : BM25/TF-IDF (mots-cl√©s exacts)\n",
    "- **Hybrid retrieval** : combinaison des deux (meilleur des deux mondes)\n",
    "\n",
    "---\n",
    "\n",
    "## 2.6 LLM / Generator (G√©n√©rateur)\n",
    "\n",
    "Re√ßoit le prompt enrichi (question + contexte r√©cup√©r√©) et g√©n√®re la r√©ponse finale.\n",
    "\n",
    "### Prompt template typique\n",
    "\n",
    "```\n",
    "Tu es un assistant expert. R√©ponds √† la question en te basant UNIQUEMENT \n",
    "sur le contexte fourni. Si l'information n'est pas dans le contexte, \n",
    "dis-le clairement.\n",
    "\n",
    "Contexte:\n",
    "{chunk_1}\n",
    "{chunk_2}\n",
    "{chunk_3}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "R√©ponse:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üóÇÔ∏è Section 3 ‚Äî Les Types de RAG\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Naive RAG (RAG Simple)\n",
    "\n",
    "L'approche de base, d√©crite dans le papier original de Lewis et al. (2020).\n",
    "\n",
    "```\n",
    "Question ‚îÄ‚îÄ‚ñ∫ Embedding ‚îÄ‚îÄ‚ñ∫ Vector Search ‚îÄ‚îÄ‚ñ∫ Prompt ‚îÄ‚îÄ‚ñ∫ LLM ‚îÄ‚îÄ‚ñ∫ R√©ponse\n",
    "```\n",
    "\n",
    "**‚úÖ Avantages :** Simple √† impl√©menter, efficace pour des cas simples  \n",
    "**‚ùå Inconv√©nients :** Peut manquer de contexte, pas de raffinement de la query, peu robuste aux questions complexes\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2 Advanced RAG\n",
    "\n",
    "Am√©liore le Naive RAG √† chaque √©tape du pipeline.\n",
    "\n",
    "### Pre-retrieval (avant la recherche)\n",
    "\n",
    "| Technique | Description |\n",
    "|-----------|-------------|\n",
    "| **Query rewriting** | Reformuler la question pour am√©liorer le retrieval |\n",
    "| **HyDE** (Hypothetical Document Embeddings) | G√©n√©rer un document hypoth√©tique, puis l'embedder |\n",
    "| **Step-back prompting** | Poser une question plus g√©n√©rale d'abord |\n",
    "| **Multi-query** | G√©n√©rer plusieurs variantes de la question |\n",
    "\n",
    "### Post-retrieval (apr√®s la recherche)\n",
    "\n",
    "| Technique | Description |\n",
    "|-----------|-------------|\n",
    "| **Reranking** | Re-classer les chunks par un mod√®le cross-encoder |\n",
    "| **Context compression** | R√©sumer/filtrer les chunks r√©cup√©r√©s |\n",
    "| **Lost in the middle** | Placer les chunks importants au d√©but/fin du contexte |\n",
    "\n",
    "### Exemple : Query Rewriting\n",
    "```\n",
    "Question originale : \"C'est quoi GPT ?\"\n",
    "           ‚Üì (LLM rewriting)\n",
    "Questions reformul√©es :\n",
    "  1. \"Qu'est-ce que GPT (Generative Pre-trained Transformer) ?\"\n",
    "  2. \"Quel est le fonctionnement du mod√®le GPT d'OpenAI ?\"\n",
    "  3. \"Quelle est l'architecture du mod√®le de langage GPT ?\"\n",
    "           ‚Üì (retrieve for each, merge results)\n",
    "Contexte enrichi ‚îÄ‚îÄ‚ñ∫ LLM ‚îÄ‚îÄ‚ñ∫ R√©ponse\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3.3 Modular RAG (RAG Modulaire / Agentic)\n",
    "\n",
    "L'approche la plus avanc√©e. Le syst√®me peut **d√©cider dynamiquement** quels modules utiliser.\n",
    "\n",
    "```\n",
    "                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                         ‚îÇ    Orchestrateur    ‚îÇ\n",
    "                         ‚îÇ     (LLM Agent)     ‚îÇ\n",
    "                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                    ‚îÇ\n",
    "               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "               ‚ñº                    ‚ñº                    ‚ñº\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  Vector Search   ‚îÇ  ‚îÇ   Web Search     ‚îÇ  ‚îÇ  SQL Database    ‚îÇ\n",
    "    ‚îÇ  (documents)     ‚îÇ  ‚îÇ  (temps r√©el)    ‚îÇ  ‚îÇ  (donn√©es struct)‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Sous-types\n",
    "\n",
    "| Type | Description | Usage |\n",
    "|------|-------------|-------|\n",
    "| **RAG-Fusion** | Fusionne r√©sultats de plusieurs retrievers | Pr√©cision maximale |\n",
    "| **Self-RAG** | Le LLM d√©cide lui-m√™me quand r√©cup√©rer | Flexibilit√© |\n",
    "| **CRAG** (Corrective RAG) | √âvalue la pertinence et corrige si n√©cessaire | Robustesse |\n",
    "| **Iterative RAG** | Plusieurs cycles de retrieval | Questions complexes |\n",
    "| **Graph RAG** | Utilise un graphe de connaissances | Relations complexes |\n",
    "\n",
    "---\n",
    "\n",
    "## 3.4 Quand utiliser quel type ?\n",
    "\n",
    "```\n",
    "Complexit√© de la question\n",
    "         ‚ñ≤\n",
    "         ‚îÇ\n",
    "  √âlev√©e ‚îÇ ‚óÑ‚îÄ‚îÄ Modular/Agentic RAG\n",
    "         ‚îÇ         (questions multi-√©tapes, sources multiples)\n",
    "  Moyenne‚îÇ ‚óÑ‚îÄ‚îÄ Advanced RAG\n",
    "         ‚îÇ         (questions n√©cessitant reformulation)\n",
    "  Faible ‚îÇ ‚óÑ‚îÄ‚îÄ Naive RAG\n",
    "         ‚îÇ         (Q&A simple sur documents)\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Maturit√© du projet\n",
    "              MVP          Prod          Entreprise\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üíæ Section 4 ‚Äî Gestion de la M√©moire\n",
    "\n",
    "---\n",
    "\n",
    "## 4.1 Pourquoi la gestion de m√©moire est-elle importante ?\n",
    "\n",
    "Un LLM a une **fen√™tre de contexte** (context window) limit√©e. Si vous essayez d'injecter trop de texte, √ßa d√©passe la limite et g√©n√®re une erreur. Il faut donc g√©rer intelligemment ce qu'on met dans le prompt.\n",
    "\n",
    "| Mod√®le | Context Window |\n",
    "|--------|---------------|\n",
    "| GPT-3.5-turbo | 16k tokens |\n",
    "| GPT-4 | 128k tokens |\n",
    "| GPT-4o | 128k tokens |\n",
    "| Claude 3.5 Sonnet | 200k tokens |\n",
    "| Gemini 1.5 Pro | 1M tokens |\n",
    "\n",
    "> üí° **R√®gle de base** : 1 token ‚âà 4 caract√®res ‚âà 0.75 mot en anglais\n",
    "\n",
    "---\n",
    "\n",
    "## 4.2 Types de m√©moire dans un syst√®me RAG\n",
    "\n",
    "### M√©moire √† court terme (In-context memory)\n",
    "Ce qui est dans le prompt courant. Limit√© par la context window.\n",
    "```\n",
    "[System prompt] + [Historique conversation] + [Contexte RAG] + [Question] ‚Üí LLM\n",
    "```\n",
    "\n",
    "### M√©moire √† long terme (External memory)\n",
    "Stock√©e en dehors du LLM :\n",
    "- **Vector store** : retrouv√©e par similarit√© s√©mantique\n",
    "- **Base de donn√©es** : retrouv√©e par ID ou requ√™te SQL\n",
    "- **Cache** : r√©sultats pr√©calcul√©s pour des queries fr√©quentes\n",
    "\n",
    "### M√©moire de conversation\n",
    "Historique des √©changes pr√©c√©dents. Plusieurs strat√©gies :\n",
    "\n",
    "| Strat√©gie | Description | Avantages | Inconv√©nients |\n",
    "|-----------|-------------|-----------|---------------|\n",
    "| **Buffer** | Garder tous les messages | Complet | D√©passe vite la context window |\n",
    "| **Window** | Garder les N derniers messages | √âquilibr√© | Perd le contexte ancien |\n",
    "| **Summary** | R√©sumer les anciens messages | Compact | Perd des d√©tails |\n",
    "| **Vector memory** | Stocker et retriever les √©changes | Scalable | Complexe |\n",
    "\n",
    "---\n",
    "\n",
    "## 4.3 Strat√©gies de gestion du contexte RAG\n",
    "\n",
    "```python\n",
    "# Budget de tokens (exemple GPT-3.5-turbo = 16k tokens)\n",
    "TOTAL_BUDGET = 16_000\n",
    "SYSTEM_PROMPT_TOKENS = 200\n",
    "CONVERSATION_HISTORY_TOKENS = 2_000\n",
    "ANSWER_RESERVE_TOKENS = 1_000\n",
    "\n",
    "# Budget disponible pour le contexte RAG\n",
    "RAG_CONTEXT_BUDGET = TOTAL_BUDGET - SYSTEM_PROMPT_TOKENS - CONVERSATION_HISTORY_TOKENS - ANSWER_RESERVE_TOKENS\n",
    "# = 12_800 tokens pour les chunks r√©cup√©r√©s\n",
    "```\n",
    "\n",
    "### Lost in the Middle Problem\n",
    "Les LLMs ont tendance √† mieux utiliser l'information plac√©e **au d√©but** ou **√† la fin** du contexte. Le milieu est souvent \"perdu\".\n",
    "\n",
    "**Solution :** Placer les chunks les plus pertinents aux extr√©mit√©s du contexte :\n",
    "```\n",
    "Contexte = [Chunk le plus pertinent] + [Chunks moyens] + [2e plus pertinent]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4.4 Conversational RAG (RAG avec historique)\n",
    "\n",
    "Pour maintenir une conversation, il faut reformuler la question en tenant compte de l'historique :\n",
    "\n",
    "```\n",
    "Tour 1:\n",
    "  User: \"Qu'est-ce que le machine learning ?\"\n",
    "  Bot: \"Le ML est...\"\n",
    "\n",
    "Tour 2:\n",
    "  User: \"Et comment √ßa s'applique √† la m√©decine ?\"\n",
    "  \n",
    "  ‚Üê Sans reformulation, \"√ßa\" est ambigu pour le retriever !\n",
    "  \n",
    "  Reformulation automatique (LLM) :\n",
    "  \"Comment le machine learning s'applique-t-il √† la m√©decine ?\"\n",
    "  \n",
    "  ‚Üê Maintenant le retriever peut trouver les bons chunks\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ‚öôÔ∏è Section 5 ‚Äî Installation et Configuration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances\n",
    "!pip install -q openai chromadb pymupdf python-dotenv tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances LangChain (pour la Section 7)\n!pip install -q langchain langchain-openai langchain-chroma langchain-community langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger les variables d'environnement depuis .env\n",
    "load_dotenv()\n",
    "\n",
    "# Cl√© API OpenAI - √† d√©finir dans votre .env ou directement ici (d√©conseill√© en prod)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    # Pour Google Colab : utiliser les secrets\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "        os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "        print(\"‚úÖ Cl√© OpenAI charg√©e depuis les secrets Colab\")\n",
    "    except Exception:\n",
    "        print(\"‚ö†Ô∏è OPENAI_API_KEY non trouv√©e. D√©finissez-la dans votre .env ou dans les secrets Colab.\")\n",
    "        print(\"Pour tester sans cl√© API, les cellules de g√©n√©ration seront simul√©es.\")\n",
    "else:\n",
    "    print(\"‚úÖ Cl√© OpenAI charg√©e depuis .env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un document PDF de d√©monstration\n",
    "# (si vous avez d√©j√† un PDF, adaptez le chemin ci-dessous)\n",
    "\n",
    "import os\n",
    "\n",
    "# V√©rifier si un PDF existe d√©j√† dans data/\n",
    "DATA_DIR = \"data\"\n",
    "PDF_PATH = os.path.join(DATA_DIR, \"document_demo.pdf\")\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Cr√©er un PDF de d√©monstration avec reportlab si aucun PDF n'est disponible\n",
    "if not os.path.exists(PDF_PATH):\n",
    "    try:\n",
    "        from reportlab.lib.pagesizes import A4\n",
    "        from reportlab.lib.styles import getSampleStyleSheet\n",
    "        from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "        from reportlab.lib.units import cm\n",
    "    except ImportError:\n",
    "        import subprocess\n",
    "        subprocess.run([\"pip\", \"install\", \"-q\", \"reportlab\"], check=True)\n",
    "        from reportlab.lib.pagesizes import A4\n",
    "        from reportlab.lib.styles import getSampleStyleSheet\n",
    "        from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "        from reportlab.lib.units import cm\n",
    "\n",
    "    doc = SimpleDocTemplate(PDF_PATH, pagesize=A4)\n",
    "    styles = getSampleStyleSheet()\n",
    "    story = []\n",
    "\n",
    "    content = [\n",
    "        (\"h1\", \"Introduction √† l'Intelligence Artificielle\"),\n",
    "        (\"body\", \"L'intelligence artificielle (IA) est un domaine de l'informatique qui vise √† cr√©er des syst√®mes capables d'effectuer des t√¢ches qui n√©cessitent normalement l'intelligence humaine. Ces t√¢ches incluent la reconnaissance vocale, la prise de d√©cision, la traduction entre langues, et la perception visuelle.\"),\n",
    "        (\"h2\", \"1. Le Machine Learning\"),\n",
    "        (\"body\", \"Le machine learning (apprentissage automatique) est une sous-discipline de l'IA qui permet aux syst√®mes d'apprendre √† partir des donn√©es sans √™tre explicitement programm√©s. Il existe trois grands types : l'apprentissage supervis√©, non supervis√© et par renforcement.\"),\n",
    "        (\"body\", \"L'apprentissage supervis√© utilise des donn√©es √©tiquet√©es pour entra√Æner un mod√®le. Par exemple, un mod√®le de classification d'images apprend √† reconna√Ætre des chats en voyant des milliers d'images annot√©es 'chat' ou 'pas chat'.\"),\n",
    "        (\"body\", \"L'apprentissage non supervis√© d√©couvre des structures cach√©es dans des donn√©es non √©tiquet√©es. Le clustering K-means est un exemple classique qui regroupe des points de donn√©es similaires ensemble.\"),\n",
    "        (\"body\", \"L'apprentissage par renforcement entra√Æne un agent √† prendre des d√©cisions en maximisant une r√©compense cumulative. AlphaGo de DeepMind, qui a battu le champion du monde de Go, utilise cette technique.\"),\n",
    "        (\"h2\", \"2. Le Deep Learning\"),\n",
    "        (\"body\", \"Le deep learning est une branche du machine learning bas√©e sur des r√©seaux de neurones artificiels √† plusieurs couches (d'o√π 'deep'). Ces r√©seaux peuvent apprendre des repr√©sentations hi√©rarchiques des donn√©es.\"),\n",
    "        (\"body\", \"Les r√©seaux de neurones convolutifs (CNN) sont particuli√®rement efficaces pour le traitement d'images. Ils utilisent des filtres pour d√©tecter des caract√©ristiques comme les bords, les textures, puis des formes plus complexes.\"),\n",
    "        (\"body\", \"Les transformers, introduits en 2017 dans le papier 'Attention Is All You Need', ont r√©volutionn√© le traitement du langage naturel. Ils utilisent un m√©canisme d'attention pour capturer les relations entre mots dans une s√©quence.\"),\n",
    "        (\"h2\", \"3. Les Grands Mod√®les de Langage (LLM)\"),\n",
    "        (\"body\", \"Les LLMs (Large Language Models) sont des mod√®les de deep learning entra√Æn√©s sur de vastes corpus de texte. GPT-4 d'OpenAI, Claude d'Anthropic, et Gemini de Google sont des exemples de LLMs de pointe.\"),\n",
    "        (\"body\", \"Ces mod√®les sont capables de g√©n√©rer du texte coh√©rent, de r√©pondre √† des questions, de traduire, de r√©sumer des documents, et m√™me d'√©crire du code. Ils sont bas√©s sur l'architecture Transformer et contiennent des milliards de param√®tres.\"),\n",
    "        (\"body\", \"Le fine-tuning permet d'adapter un LLM pr√©-entra√Æn√© √† une t√¢che sp√©cifique avec un dataset plus petit. Le RLHF (Reinforcement Learning from Human Feedback) est utilis√© pour aligner les LLMs avec les pr√©f√©rences humaines.\"),\n",
    "        (\"h2\", \"4. Le RAG (Retrieval-Augmented Generation)\"),\n",
    "        (\"body\", \"Le RAG est une technique qui am√©liore les LLMs en leur donnant acc√®s √† des sources de connaissances externes. Au lieu de r√©pondre uniquement depuis leur m√©moire interne (les poids du mod√®le), les syst√®mes RAG r√©cup√®rent d'abord des documents pertinents, puis les utilisent pour g√©n√©rer une r√©ponse.\"),\n",
    "        (\"body\", \"Un pipeline RAG typique comprend : (1) l'indexation des documents dans une base vectorielle, (2) la transformation de la question en embedding, (3) la recherche des chunks les plus similaires, (4) l'augmentation du prompt avec ces chunks, et (5) la g√©n√©ration de la r√©ponse par le LLM.\"),\n",
    "        (\"body\", \"ChromaDB est une base de donn√©es vectorielle open-source l√©g√®re, id√©ale pour les projets locaux et le prototypage. Elle supporte les embeddings de diff√©rentes dimensions et offre des fonctionnalit√©s de filtrage par m√©tadonn√©es.\"),\n",
    "        (\"h2\", \"5. Les Applications de l'IA\"),\n",
    "        (\"body\", \"L'IA est utilis√©e dans de nombreux domaines : la sant√© (diagnostic m√©dical, d√©couverte de m√©dicaments), la finance (d√©tection de fraude, trading algorithmique), l'√©ducation (tuteurs personnalis√©s, correction automatique), et les transports (v√©hicules autonomes).\"),\n",
    "        (\"body\", \"Dans la data science, l'IA aide √† automatiser l'exploration des donn√©es, √† construire des mod√®les pr√©dictifs plus pr√©cis, et √† extraire des insights de grandes quantit√©s de donn√©es non structur√©es comme les textes, images et vid√©os.\"),\n",
    "        (\"body\", \"Les assistants IA comme ChatGPT, Copilot et Gemini transforment la fa√ßon dont nous travaillons. Ils peuvent r√©diger des emails, g√©n√©rer du code, expliquer des concepts complexes, et m√™me aider √† la cr√©ativit√© artistique.\"),\n",
    "        (\"h2\", \"6. √âthique et D√©fis de l'IA\"),\n",
    "        (\"body\", \"L'IA soul√®ve des questions √©thiques importantes : biais algorithmiques (les mod√®les peuvent reproduire ou amplifier des biais pr√©sents dans les donn√©es d'entra√Ænement), confidentialit√© des donn√©es, transparence des d√©cisions automatis√©es, et impact sur l'emploi.\"),\n",
    "        (\"body\", \"La r√©glementation de l'IA est en cours de d√©veloppement dans de nombreux pays. L'Union Europ√©enne a adopt√© l'AI Act, qui classe les syst√®mes d'IA selon leur niveau de risque et impose des obligations proportionnelles.\"),\n",
    "        (\"body\", \"La s√©curit√© de l'IA (AI Safety) est un domaine de recherche qui vise √† s'assurer que les syst√®mes d'IA agissent conform√©ment aux intentions humaines, m√™me lorsqu'ils deviennent tr√®s puissants. Les concepts d'alignement et de contr√¥le sont au c≈ìur de ces recherches.\"),\n",
    "    ]\n",
    "\n",
    "    for style, text in content:\n",
    "        if style == \"h1\":\n",
    "            story.append(Paragraph(text, styles['Title']))\n",
    "        elif style == \"h2\":\n",
    "            story.append(Spacer(1, 0.5*cm))\n",
    "            story.append(Paragraph(text, styles['Heading2']))\n",
    "        else:\n",
    "            story.append(Paragraph(text, styles['Normal']))\n",
    "            story.append(Spacer(1, 0.3*cm))\n",
    "\n",
    "    doc.build(story)\n",
    "    print(f\"‚úÖ PDF de d√©monstration cr√©√© : {PDF_PATH}\")\n",
    "else:\n",
    "    print(f\"‚úÖ PDF trouv√© : {PDF_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üöÄ Section 6 ‚Äî RAG Sans LangChain\n",
    "\n",
    "> Cette impl√©mentation utilise directement : **PyMuPDF** (extraction PDF) + **OpenAI API** (embeddings + g√©n√©ration) + **ChromaDB** (vector store)\n",
    "\n",
    "---\n",
    "\n",
    "## 6.1 Imports et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import chromadb\n",
    "import tiktoken\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from openai import OpenAI\n",
    "\n",
    "# Configuration globale\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"  # Mod√®le d'embeddings OpenAI\n",
    "LLM_MODEL = \"gpt-4o-mini\"                  # Mod√®le de g√©n√©ration\n",
    "CHUNK_SIZE = 500                             # Taille des chunks en caract√®res\n",
    "CHUNK_OVERLAP = 100                          # Chevauchement entre chunks\n",
    "TOP_K = 3                                    # Nombre de chunks √† r√©cup√©rer\n",
    "CHROMA_COLLECTION = \"rag_demo\"              # Nom de la collection ChromaDB\n",
    "\n",
    "print(\"‚úÖ Imports r√©ussis\")\n",
    "print(f\"üìä Configuration : chunk_size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP}, top_k={TOP_K}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 √âtape 1 : Extraction du texte depuis le PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extrait le texte d'un PDF page par page avec PyMuPDF.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Chemin vers le fichier PDF\n",
    "        \n",
    "    Returns:\n",
    "        Liste de dicts {page_num, text, num_chars}\n",
    "    \"\"\"\n",
    "    pages = []\n",
    "    \n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        print(f\"üìÑ PDF ouvert : {doc.page_count} pages\")\n",
    "        \n",
    "        for page_num, page in enumerate(doc, start=1):\n",
    "            text = page.get_text()\n",
    "            # Nettoyer le texte\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            \n",
    "            if text:  # Ignorer les pages vides\n",
    "                pages.append({\n",
    "                    \"page_num\": page_num,\n",
    "                    \"text\": text,\n",
    "                    \"num_chars\": len(text)\n",
    "                })\n",
    "    \n",
    "    total_chars = sum(p[\"num_chars\"] for p in pages)\n",
    "    print(f\"‚úÖ Extraction termin√©e : {len(pages)} pages, {total_chars:,} caract√®res au total\")\n",
    "    return pages\n",
    "\n",
    "\n",
    "# Extraire le texte du PDF\n",
    "pages = extract_text_from_pdf(PDF_PATH)\n",
    "\n",
    "# Afficher un aper√ßu\n",
    "print(\"\\nüìù Aper√ßu de la premi√®re page :\")\n",
    "print(\"-\" * 60)\n",
    "print(pages[0][\"text\"][:300] + \"...\" if len(pages[0][\"text\"]) > 300 else pages[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 √âtape 2 : D√©coupage en Chunks (Text Splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(\n",
    "    pages: List[Dict[str, Any]],\n",
    "    chunk_size: int = CHUNK_SIZE,\n",
    "    chunk_overlap: int = CHUNK_OVERLAP\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    D√©coupe le texte en chunks avec chevauchement.\n",
    "    Strat√©gie : d√©coupage r√©cursif (d'abord par \\n\\n, puis par \\n, puis par taille fixe).\n",
    "    \n",
    "    Args:\n",
    "        pages: Liste des pages extraites du PDF\n",
    "        chunk_size: Taille maximale d'un chunk en caract√®res\n",
    "        chunk_overlap: Chevauchement entre chunks cons√©cutifs\n",
    "        \n",
    "    Returns:\n",
    "        Liste de chunks avec m√©tadonn√©es\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    chunk_id = 0\n",
    "    \n",
    "    for page in pages:\n",
    "        text = page[\"text\"]\n",
    "        page_num = page[\"page_num\"]\n",
    "        \n",
    "        # D√©couper d'abord par double saut de ligne (paragraphes)\n",
    "        paragraphs = text.split(\"  \")  # Double espace apr√®s nettoyage\n",
    "        if len(paragraphs) == 1:\n",
    "            # Pas de paragraphes, d√©couper par taille fixe avec overlap\n",
    "            start = 0\n",
    "            while start < len(text):\n",
    "                end = min(start + chunk_size, len(text))\n",
    "                chunk_text = text[start:end].strip()\n",
    "                \n",
    "                if len(chunk_text) > 50:  # Ignorer les chunks trop petits\n",
    "                    chunks.append({\n",
    "                        \"id\": f\"chunk_{chunk_id}\",\n",
    "                        \"text\": chunk_text,\n",
    "                        \"page_num\": page_num,\n",
    "                        \"start_char\": start,\n",
    "                        \"num_chars\": len(chunk_text)\n",
    "                    })\n",
    "                    chunk_id += 1\n",
    "                \n",
    "                # Avancer en tenant compte de l'overlap\n",
    "                start += chunk_size - chunk_overlap\n",
    "        else:\n",
    "            # Regrouper les paragraphes en chunks\n",
    "            current_chunk = \"\"\n",
    "            start_char = 0\n",
    "            \n",
    "            for para in paragraphs:\n",
    "                para = para.strip()\n",
    "                if not para:\n",
    "                    continue\n",
    "                \n",
    "                if len(current_chunk) + len(para) + 1 <= chunk_size:\n",
    "                    current_chunk += (\" \" if current_chunk else \"\") + para\n",
    "                else:\n",
    "                    if current_chunk:\n",
    "                        chunks.append({\n",
    "                            \"id\": f\"chunk_{chunk_id}\",\n",
    "                            \"text\": current_chunk,\n",
    "                            \"page_num\": page_num,\n",
    "                            \"start_char\": start_char,\n",
    "                            \"num_chars\": len(current_chunk)\n",
    "                        })\n",
    "                        chunk_id += 1\n",
    "                        # Overlap: reprendre depuis la fin du dernier chunk\n",
    "                        overlap_text = current_chunk[-chunk_overlap:] if len(current_chunk) > chunk_overlap else current_chunk\n",
    "                        current_chunk = overlap_text + \" \" + para\n",
    "                    else:\n",
    "                        current_chunk = para\n",
    "                    start_char = len(text) - len(para)\n",
    "            \n",
    "            if current_chunk:  # Dernier chunk\n",
    "                chunks.append({\n",
    "                    \"id\": f\"chunk_{chunk_id}\",\n",
    "                    \"text\": current_chunk,\n",
    "                    \"page_num\": page_num,\n",
    "                    \"start_char\": start_char,\n",
    "                    \"num_chars\": len(current_chunk)\n",
    "                })\n",
    "                chunk_id += 1\n",
    "    \n",
    "    print(f\"‚úÖ {len(chunks)} chunks cr√©√©s\")\n",
    "    print(f\"üìè Taille moyenne d'un chunk : {sum(c['num_chars'] for c in chunks) // len(chunks)} caract√®res\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# D√©couper en chunks\n",
    "chunks = split_text_into_chunks(pages)\n",
    "\n",
    "# Afficher quelques chunks\n",
    "print(\"\\nüì¶ Exemples de chunks :\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i+1} (page {chunk['page_num']}, {chunk['num_chars']} chars) ---\")\n",
    "    print(chunk[\"text\"][:200] + \"...\" if len(chunk[\"text\"]) > 200 else chunk[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 √âtape 3 : Cr√©ation des Embeddings et Indexation dans ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_chroma_and_index(\n",
    "    chunks: List[Dict[str, Any]],\n",
    "    collection_name: str = CHROMA_COLLECTION,\n",
    "    embedding_model: str = EMBEDDING_MODEL\n",
    ") -> Tuple[chromadb.Collection, OpenAI]:\n",
    "    \"\"\"\n",
    "    Cr√©e les embeddings via OpenAI et les indexe dans ChromaDB.\n",
    "    \n",
    "    Args:\n",
    "        chunks: Liste des chunks √† indexer\n",
    "        collection_name: Nom de la collection ChromaDB\n",
    "        embedding_model: Mod√®le d'embeddings OpenAI\n",
    "        \n",
    "    Returns:\n",
    "        (collection ChromaDB, client OpenAI)\n",
    "    \"\"\"\n",
    "    # Initialiser le client OpenAI\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    \n",
    "    # Initialiser ChromaDB (mode persistant local)\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    \n",
    "    # Supprimer la collection si elle existe d√©j√† (pour repartir propre)\n",
    "    try:\n",
    "        chroma_client.delete_collection(collection_name)\n",
    "        print(f\"üóëÔ∏è Collection existante '{collection_name}' supprim√©e\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Cr√©er une nouvelle collection\n",
    "    collection = chroma_client.create_collection(\n",
    "        name=collection_name,\n",
    "        metadata={\"hnsw:space\": \"cosine\"}  # M√©trique de similarit√© cosinus\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüî¢ G√©n√©ration des embeddings pour {len(chunks)} chunks...\")\n",
    "    print(f\"   Mod√®le : {embedding_model}\")\n",
    "    \n",
    "    # G√©n√©rer les embeddings par lots (batch) pour optimiser les appels API\n",
    "    batch_size = 20\n",
    "    all_ids = []\n",
    "    all_embeddings = []\n",
    "    all_documents = []\n",
    "    all_metadatas = []\n",
    "    \n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i:i + batch_size]\n",
    "        texts = [c[\"text\"] for c in batch]\n",
    "        \n",
    "        # Appel API OpenAI pour les embeddings\n",
    "        response = client.embeddings.create(\n",
    "            input=texts,\n",
    "            model=embedding_model\n",
    "        )\n",
    "        \n",
    "        # Collecter les r√©sultats\n",
    "        for j, chunk in enumerate(batch):\n",
    "            all_ids.append(chunk[\"id\"])\n",
    "            all_embeddings.append(response.data[j].embedding)\n",
    "            all_documents.append(chunk[\"text\"])\n",
    "            all_metadatas.append({\n",
    "                \"page_num\": chunk[\"page_num\"],\n",
    "                \"num_chars\": chunk[\"num_chars\"],\n",
    "                \"start_char\": chunk[\"start_char\"]\n",
    "            })\n",
    "        \n",
    "        print(f\"   Lot {i//batch_size + 1}/{(len(chunks)-1)//batch_size + 1} trait√© ({min(i+batch_size, len(chunks))}/{len(chunks)} chunks)\")\n",
    "    \n",
    "    # Ins√©rer tous les embeddings dans ChromaDB\n",
    "    collection.add(\n",
    "        ids=all_ids,\n",
    "        embeddings=all_embeddings,\n",
    "        documents=all_documents,\n",
    "        metadatas=all_metadatas\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Indexation termin√©e !\")\n",
    "    print(f\"   üìä {collection.count()} chunks index√©s dans ChromaDB\")\n",
    "    print(f\"   üíæ Base vectorielle sauvegard√©e dans './chroma_db'\")\n",
    "    \n",
    "    return collection, client\n",
    "\n",
    "\n",
    "# Indexer les chunks (n√©cessite une cl√© API OpenAI valide)\n",
    "if OPENAI_API_KEY:\n",
    "    collection, openai_client = setup_chroma_and_index(chunks)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cl√© API OpenAI manquante - indexation simul√©e\")\n",
    "    print(\"   Pour une d√©mo sans cl√© API, voir la section alternative ci-dessous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative sans cl√© API : utiliser des embeddings locaux (sentence-transformers)\n",
    "# D√©commentez cette cellule si vous n'avez pas de cl√© OpenAI\n",
    "\n",
    "# !pip install -q sentence-transformers\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# def setup_chroma_local(chunks, collection_name=\"rag_demo_local\"):\n",
    "#     \"\"\"Version locale sans API OpenAI, utilise sentence-transformers\"\"\"\n",
    "#     model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "#     \n",
    "#     chroma_client = chromadb.PersistentClient(path=\"./chroma_db_local\")\n",
    "#     try:\n",
    "#         chroma_client.delete_collection(collection_name)\n",
    "#     except Exception:\n",
    "#         pass\n",
    "#     \n",
    "#     collection = chroma_client.create_collection(\n",
    "#         name=collection_name,\n",
    "#         metadata={\"hnsw:space\": \"cosine\"}\n",
    "#     )\n",
    "#     \n",
    "#     texts = [c[\"text\"] for c in chunks]\n",
    "#     print(\"G√©n√©ration des embeddings locaux...\")\n",
    "#     embeddings = model.encode(texts, show_progress_bar=True).tolist()\n",
    "#     \n",
    "#     collection.add(\n",
    "#         ids=[c[\"id\"] for c in chunks],\n",
    "#         embeddings=embeddings,\n",
    "#         documents=texts,\n",
    "#         metadatas=[{\"page_num\": c[\"page_num\"]} for c in chunks]\n",
    "#     )\n",
    "#     print(f\"‚úÖ {collection.count()} chunks index√©s (local)\")\n",
    "#     return collection, model\n",
    "#\n",
    "# collection_local, st_model = setup_chroma_local(chunks)\n",
    "print(\"üí° D√©commentez la cellule ci-dessus pour une version sans cl√© API (sentence-transformers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 √âtape 4 : Retrieval (R√©cup√©ration des chunks pertinents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(\n",
    "    question: str,\n",
    "    collection: chromadb.Collection,\n",
    "    client: OpenAI,\n",
    "    top_k: int = TOP_K,\n",
    "    embedding_model: str = EMBEDDING_MODEL\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    R√©cup√®re les chunks les plus pertinents pour une question.\n",
    "    \n",
    "    Args:\n",
    "        question: La question de l'utilisateur\n",
    "        collection: Collection ChromaDB\n",
    "        client: Client OpenAI\n",
    "        top_k: Nombre de chunks √† r√©cup√©rer\n",
    "        embedding_model: Mod√®le d'embeddings\n",
    "        \n",
    "    Returns:\n",
    "        Liste des chunks r√©cup√©r√©s avec leurs scores de similarit√©\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Cr√©er l'embedding de la question\n",
    "    query_response = client.embeddings.create(\n",
    "        input=[question],\n",
    "        model=embedding_model\n",
    "    )\n",
    "    query_embedding = query_response.data[0].embedding\n",
    "    \n",
    "    # Rechercher dans ChromaDB\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    retrieval_time = (time.time() - start_time) * 1000\n",
    "    \n",
    "    # Formater les r√©sultats\n",
    "    retrieved = []\n",
    "    for i in range(len(results[\"ids\"][0])):\n",
    "        # ChromaDB retourne la distance cosinus (0 = identique, 2 = oppos√©)\n",
    "        # On convertit en score de similarit√© (0 √† 1)\n",
    "        distance = results[\"distances\"][0][i]\n",
    "        similarity = 1 - (distance / 2)  # Normalisation pour cosinus\n",
    "        \n",
    "        retrieved.append({\n",
    "            \"rank\": i + 1,\n",
    "            \"id\": results[\"ids\"][0][i],\n",
    "            \"text\": results[\"documents\"][0][i],\n",
    "            \"metadata\": results[\"metadatas\"][0][i],\n",
    "            \"distance\": distance,\n",
    "            \"similarity\": round(similarity, 4)\n",
    "        })\n",
    "    \n",
    "    print(f\"‚ö° Retrieval en {retrieval_time:.1f}ms\")\n",
    "    return retrieved\n",
    "\n",
    "\n",
    "# Test du retrieval\n",
    "if OPENAI_API_KEY:\n",
    "    test_question = \"Qu'est-ce que le machine learning ?\"\n",
    "    print(f\"üîç Question : {test_question}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    retrieved_chunks = retrieve_relevant_chunks(test_question, collection, openai_client)\n",
    "    \n",
    "    print(f\"\\nüìã Top-{TOP_K} chunks r√©cup√©r√©s :\")\n",
    "    for chunk in retrieved_chunks:\n",
    "        print(f\"\\nüèÜ Rang {chunk['rank']} | Similarit√©: {chunk['similarity']} | Page: {chunk['metadata']['page_num']}\")\n",
    "        print(f\"   {chunk['text'][:200]}...\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cl√© API OpenAI manquante\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 √âtape 5 : G√©n√©ration de la R√©ponse avec le LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question: str, retrieved_chunks: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    Construit le prompt enrichi avec le contexte r√©cup√©r√©.\n",
    "    \n",
    "    Args:\n",
    "        question: La question de l'utilisateur\n",
    "        retrieved_chunks: Les chunks r√©cup√©r√©s par le retriever\n",
    "        \n",
    "    Returns:\n",
    "        Prompt format√© pr√™t pour le LLM\n",
    "    \"\"\"\n",
    "    # Construire le contexte\n",
    "    context_parts = []\n",
    "    for chunk in retrieved_chunks:\n",
    "        context_parts.append(\n",
    "            f\"[Source - Page {chunk['metadata']['page_num']}]\\n{chunk['text']}\"\n",
    "        )\n",
    "    \n",
    "    context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "    \n",
    "    prompt = f\"\"\"Contexte extrait du document :\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Question : {question}\n",
    "\n",
    "Instructions :\n",
    "- R√©ponds √† la question en te basant UNIQUEMENT sur le contexte fourni ci-dessus.\n",
    "- Si le contexte ne contient pas l'information n√©cessaire, dis-le clairement.\n",
    "- Sois pr√©cis, structur√© et cite les sources (num√©ros de page) si pertinent.\n",
    "- R√©ponds en fran√ßais.\n",
    "\n",
    "R√©ponse :\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_answer(\n",
    "    question: str,\n",
    "    retrieved_chunks: List[Dict[str, Any]],\n",
    "    client: OpenAI,\n",
    "    model: str = LLM_MODEL,\n",
    "    temperature: float = 0.2,\n",
    "    max_tokens: int = 800\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    G√©n√®re une r√©ponse avec le LLM bas√©e sur les chunks r√©cup√©r√©s.\n",
    "    \n",
    "    Args:\n",
    "        question: La question de l'utilisateur\n",
    "        retrieved_chunks: Les chunks r√©cup√©r√©s\n",
    "        client: Client OpenAI\n",
    "        model: Mod√®le LLM √† utiliser\n",
    "        temperature: Temp√©rature (0=d√©terministe, 1=cr√©atif)\n",
    "        max_tokens: Nombre max de tokens √† g√©n√©rer\n",
    "        \n",
    "    Returns:\n",
    "        Dict avec la r√©ponse, le mod√®le utilis√© et les m√©triques\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Construire le prompt\n",
    "    user_prompt = build_prompt(question, retrieved_chunks)\n",
    "    \n",
    "    # Appel au LLM\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Tu es un assistant expert qui r√©pond aux questions en se basant exclusivement sur les documents fournis. Tu es pr√©cis, structur√© et toujours honn√™te quand une information n'est pas disponible dans le contexte.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    \n",
    "    generation_time = (time.time() - start_time) * 1000\n",
    "    \n",
    "    return {\n",
    "        \"answer\": response.choices[0].message.content,\n",
    "        \"model\": model,\n",
    "        \"generation_time_ms\": round(generation_time, 1),\n",
    "        \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "        \"completion_tokens\": response.usage.completion_tokens,\n",
    "        \"total_tokens\": response.usage.total_tokens\n",
    "    }\n",
    "\n",
    "\n",
    "# Test de la g√©n√©ration\n",
    "if OPENAI_API_KEY:\n",
    "    print(f\"ü§ñ G√©n√©ration de r√©ponse avec {LLM_MODEL}...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    generation_result = generate_answer(test_question, retrieved_chunks, openai_client)\n",
    "    \n",
    "    print(f\"‚úÖ R√©ponse g√©n√©r√©e en {generation_result['generation_time_ms']}ms\")\n",
    "    print(f\"üìä Tokens utilis√©s : {generation_result['total_tokens']} (prompt: {generation_result['prompt_tokens']}, r√©ponse: {generation_result['completion_tokens']})\")\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìù R√âPONSE :\")\n",
    "    print(\"=\" * 60)\n",
    "    print(generation_result[\"answer\"])\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cl√© API OpenAI manquante\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 Pipeline RAG Complet ‚Äî Classe SimpleRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAG:\n",
    "    \"\"\"\n",
    "    Pipeline RAG complet sans LangChain.\n",
    "    Utilise : PyMuPDF + OpenAI Embeddings + ChromaDB + OpenAI GPT\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        openai_api_key: str,\n",
    "        embedding_model: str = \"text-embedding-3-small\",\n",
    "        llm_model: str = \"gpt-4o-mini\",\n",
    "        chunk_size: int = 500,\n",
    "        chunk_overlap: int = 100,\n",
    "        top_k: int = 3,\n",
    "        chroma_path: str = \"./chroma_db\",\n",
    "        collection_name: str = \"simple_rag\"\n",
    "    ):\n",
    "        self.client = OpenAI(api_key=openai_api_key)\n",
    "        self.embedding_model = embedding_model\n",
    "        self.llm_model = llm_model\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.top_k = top_k\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        # Initialiser ChromaDB\n",
    "        self.chroma_client = chromadb.PersistentClient(path=chroma_path)\n",
    "        self.collection = None\n",
    "        \n",
    "        # M√©moire conversationnelle (fen√™tre des N derniers √©changes)\n",
    "        self.conversation_history = []\n",
    "        self.memory_window = 5  # Garder les 5 derniers √©changes\n",
    "        \n",
    "        print(f\"‚úÖ SimpleRAG initialis√©\")\n",
    "        print(f\"   Embedding: {embedding_model} | LLM: {llm_model}\")\n",
    "        print(f\"   Chunk size: {chunk_size} | Overlap: {chunk_overlap} | Top-K: {top_k}\")\n",
    "    \n",
    "    def index_pdf(self, pdf_path: str) -> int:\n",
    "        \"\"\"Indexe un PDF dans ChromaDB. Retourne le nombre de chunks.\"\"\"\n",
    "        print(f\"\\nüìÑ Indexation de : {pdf_path}\")\n",
    "        \n",
    "        # Extraire le texte\n",
    "        pages = extract_text_from_pdf(pdf_path)\n",
    "        \n",
    "        # D√©couper en chunks\n",
    "        chunks = split_text_into_chunks(pages, self.chunk_size, self.chunk_overlap)\n",
    "        \n",
    "        # Cr√©er/recr√©er la collection\n",
    "        try:\n",
    "            self.chroma_client.delete_collection(self.collection_name)\n",
    "        except Exception:\n",
    "            pass\n",
    "        self.collection = self.chroma_client.create_collection(\n",
    "            name=self.collection_name,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        \n",
    "        # G√©n√©rer et indexer les embeddings par lots\n",
    "        batch_size = 20\n",
    "        for i in range(0, len(chunks), batch_size):\n",
    "            batch = chunks[i:i + batch_size]\n",
    "            texts = [c[\"text\"] for c in batch]\n",
    "            \n",
    "            response = self.client.embeddings.create(\n",
    "                input=texts, model=self.embedding_model\n",
    "            )\n",
    "            \n",
    "            self.collection.add(\n",
    "                ids=[c[\"id\"] for c in batch],\n",
    "                embeddings=[r.embedding for r in response.data],\n",
    "                documents=texts,\n",
    "                metadatas=[{\"page_num\": c[\"page_num\"], \"num_chars\": c[\"num_chars\"]} for c in batch]\n",
    "            )\n",
    "        \n",
    "        print(f\"‚úÖ {self.collection.count()} chunks index√©s\")\n",
    "        return self.collection.count()\n",
    "    \n",
    "    def query(\n",
    "        self,\n",
    "        question: str,\n",
    "        use_memory: bool = True\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Pipeline RAG complet : retrieve + generate.\n",
    "        Supporte la m√©moire conversationnelle.\n",
    "        \n",
    "        Args:\n",
    "            question: La question de l'utilisateur\n",
    "            use_memory: Si True, inclut l'historique de conversation dans le prompt\n",
    "            \n",
    "        Returns:\n",
    "            Dict avec la r√©ponse, les sources et les m√©triques\n",
    "        \"\"\"\n",
    "        if self.collection is None:\n",
    "            raise ValueError(\"Aucun document index√©. Appelez d'abord index_pdf().\")\n",
    "        \n",
    "        start_total = time.time()\n",
    "        \n",
    "        # 1. RETRIEVAL : Trouver les chunks pertinents\n",
    "        t_retrieve = time.time()\n",
    "        query_embedding = self.client.embeddings.create(\n",
    "            input=[question], model=self.embedding_model\n",
    "        ).data[0].embedding\n",
    "        \n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=self.top_k,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "        )\n",
    "        retrieval_time = (time.time() - t_retrieve) * 1000\n",
    "        \n",
    "        retrieved_chunks = [\n",
    "            {\n",
    "                \"text\": results[\"documents\"][0][i],\n",
    "                \"metadata\": results[\"metadatas\"][0][i],\n",
    "                \"similarity\": round(1 - results[\"distances\"][0][i] / 2, 4)\n",
    "            }\n",
    "            for i in range(len(results[\"ids\"][0]))\n",
    "        ]\n",
    "        \n",
    "        # 2. AUGMENTATION : Construire le prompt enrichi\n",
    "        context = \"\\n\\n---\\n\\n\".join(\n",
    "            f\"[Page {c['metadata']['page_num']}]\\n{c['text']}\"\n",
    "            for c in retrieved_chunks\n",
    "        )\n",
    "        \n",
    "        # 3. GENERATION : Appel au LLM\n",
    "        t_generate = time.time()\n",
    "        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Tu es un assistant expert. R√©ponds uniquement en te basant sur le contexte fourni. Si l'information n'est pas dans le contexte, dis-le clairement.\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Ajouter l'historique de conversation (m√©moire window)\n",
    "        if use_memory and self.conversation_history:\n",
    "            recent_history = self.conversation_history[-self.memory_window:]\n",
    "            messages.extend(recent_history)\n",
    "        \n",
    "        # Ajouter la question avec contexte\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Contexte :\\n{context}\\n\\nQuestion : {question}\\n\\nR√©ponds en fran√ßais.\"\n",
    "        })\n",
    "        \n",
    "        llm_response = self.client.chat.completions.create(\n",
    "            model=self.llm_model,\n",
    "            messages=messages,\n",
    "            temperature=0.2,\n",
    "            max_tokens=800\n",
    "        )\n",
    "        \n",
    "        answer = llm_response.choices[0].message.content\n",
    "        generation_time = (time.time() - t_generate) * 1000\n",
    "        total_time = (time.time() - start_total) * 1000\n",
    "        \n",
    "        # Mettre √† jour la m√©moire conversationnelle\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": question})\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"sources\": [\n",
    "                {\"page\": c[\"metadata\"][\"page_num\"], \"similarity\": c[\"similarity\"], \"excerpt\": c[\"text\"][:150] + \"...\"}\n",
    "                for c in retrieved_chunks\n",
    "            ],\n",
    "            \"metrics\": {\n",
    "                \"retrieval_ms\": round(retrieval_time, 1),\n",
    "                \"generation_ms\": round(generation_time, 1),\n",
    "                \"total_ms\": round(total_time, 1),\n",
    "                \"tokens_used\": llm_response.usage.total_tokens,\n",
    "                \"model\": self.llm_model\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        \"\"\"Efface l'historique de conversation.\"\"\"\n",
    "        self.conversation_history = []\n",
    "        print(\"üßπ M√©moire conversationnelle effac√©e\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Classe SimpleRAG d√©finie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©monstration du SimpleRAG\n",
    "if OPENAI_API_KEY:\n",
    "    # Cr√©er et indexer\n",
    "    rag = SimpleRAG(\n",
    "        openai_api_key=OPENAI_API_KEY,\n",
    "        collection_name=\"simple_rag_demo\"\n",
    "    )\n",
    "    rag.index_pdf(PDF_PATH)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üéØ TEST DU PIPELINE RAG COMPLET\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Questions de test\n",
    "    questions = [\n",
    "        \"Qu'est-ce que le deep learning et comment fonctionne-t-il ?\",\n",
    "        \"Quels sont les d√©fis √©thiques de l'IA ?\",\n",
    "        \"Comment fonctionne le RAG ?\"\n",
    "    ]\n",
    "    \n",
    "    for q in questions:\n",
    "        print(f\"\\n‚ùì Question : {q}\")\n",
    "        print(\"-\" * 50)\n",
    "        result = rag.query(q)\n",
    "        print(f\"üí¨ R√©ponse : {result['answer']}\")\n",
    "        print(f\"\\nüìö Sources utilis√©es :\")\n",
    "        for src in result[\"sources\"]:\n",
    "            print(f\"   ‚Ä¢ Page {src['page']} (similarit√©: {src['similarity']}) : {src['excerpt'][:100]}...\")\n",
    "        print(f\"\\n‚è±Ô∏è M√©triques : retrieval={result['metrics']['retrieval_ms']}ms | g√©n√©ration={result['metrics']['generation_ms']}ms | tokens={result['metrics']['tokens_used']}\")\n",
    "        print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cl√© API OpenAI manquante - d√©monstration non disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de la m√©moire conversationnelle\n",
    "if OPENAI_API_KEY:\n",
    "    print(\"üí≠ TEST DE LA M√âMOIRE CONVERSATIONNELLE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    rag.clear_memory()\n",
    "    \n",
    "    # Conversation en plusieurs tours\n",
    "    conversation = [\n",
    "        \"Qu'est-ce que le machine learning ?\",\n",
    "        \"Et quels en sont les diff√©rents types ?\",  # 'en' r√©f√®re au ML ‚Üí test de la m√©moire\n",
    "        \"Donne-moi un exemple concret d'application.\"\n",
    "    ]\n",
    "    \n",
    "    for turn, question in enumerate(conversation, 1):\n",
    "        print(f\"\\n[Tour {turn}] üë§ User : {question}\")\n",
    "        result = rag.query(question, use_memory=True)\n",
    "        print(f\"[Tour {turn}] ü§ñ Bot  : {result['answer'][:300]}...\")\n",
    "    \n",
    "    print(f\"\\nüìù Historique en m√©moire : {len(rag.conversation_history)} messages\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cl√© API OpenAI manquante\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ü¶ú Section 7 ‚Äî RAG Avec LangChain\n",
    "\n",
    "> La m√™me fonctionnalit√©, mais en utilisant les abstractions de LangChain pour un code plus concis et plus facilement extensible.\n",
    "\n",
    "---\n",
    "\n",
    "## 7.1 Pourquoi LangChain ?\n",
    "\n",
    "| Aspect | Sans LangChain | Avec LangChain |\n",
    "|--------|---------------|----------------|\n",
    "| **Code** | Plus verbeux, mais explicite | Concis, abstrait |\n",
    "| **Flexibilit√©** | Contr√¥le total | Parfois contraignant |\n",
    "| **√âcosyst√®me** | √Ä construire | 100+ int√©grations pr√™tes |\n",
    "| **Debugging** | Facile | Plus difficile (abstractions) |\n",
    "| **Production** | Adapt√© | Adapt√© + outils LangSmith |\n",
    "| **Apprentissage** | Comprendre le fond | Productivit√© rapide |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports LangChain\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "import textwrap\n",
    "\n",
    "print(\"‚úÖ Imports LangChain r√©ussis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Chargement et Indexation avec LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPENAI_API_KEY:\n",
    "    # ‚îÄ‚îÄ 1. Charger le PDF ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    loader = PyMuPDFLoader(PDF_PATH)\n",
    "    documents = loader.load()\n",
    "    print(f\"üìÑ {len(documents)} pages charg√©es avec LangChain\")\n",
    "    \n",
    "    # ‚îÄ‚îÄ 2. D√©couper en chunks ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    lc_chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"‚úÖ {len(lc_chunks)} chunks cr√©√©s avec RecursiveCharacterTextSplitter\")\n",
    "    print(f\"üìè Taille moyenne : {sum(len(c.page_content) for c in lc_chunks) // len(lc_chunks)} caract√®res\")\n",
    "    \n",
    "    # Aper√ßu d'un chunk\n",
    "    print(f\"\\nüîç Exemple de chunk :\")\n",
    "    print(f\"   M√©tadonn√©es : {lc_chunks[0].metadata}\")\n",
    "    print(f\"   Contenu     : {lc_chunks[0].page_content[:200]}...\")\n",
    "    \n",
    "    # ‚îÄ‚îÄ 3. Cr√©er les embeddings et indexer dans ChromaDB ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        api_key=OPENAI_API_KEY\n",
    "    )\n",
    "    \n",
    "    # Cr√©er le vector store ChromaDB\n",
    "    lc_vectorstore = Chroma.from_documents(\n",
    "        documents=lc_chunks,\n",
    "        embedding=embeddings,\n",
    "        collection_name=\"langchain_rag\",\n",
    "        persist_directory=\"./chroma_db_langchain\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Vector store LangChain cr√©√©\")\n",
    "    print(f\"   üìä {lc_vectorstore._collection.count()} chunks dans ChromaDB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cl√© API OpenAI manquante\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Pipeline RAG Simple avec LangChain (LCEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPENAI_API_KEY:\n",
    "    # ‚îÄ‚îÄ Cr√©er le retriever ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    retriever = lc_vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 3}\n",
    "    )\n",
    "    \n",
    "    # ‚îÄ‚îÄ Cr√©er le LLM ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.2,\n",
    "        api_key=OPENAI_API_KEY\n",
    "    )\n",
    "    \n",
    "    # ‚îÄ‚îÄ D√©finir le prompt template ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    RAG_PROMPT = ChatPromptTemplate.from_template(\"\"\"\n",
    "Tu es un assistant expert. R√©ponds √† la question en te basant UNIQUEMENT sur le contexte fourni.\n",
    "Si l'information n'est pas dans le contexte, dis-le clairement.\n",
    "R√©ponds en fran√ßais.\n",
    "\n",
    "Contexte :\n",
    "{context}\n",
    "\n",
    "Question : {question}\n",
    "\n",
    "R√©ponse :\n",
    "\"\"\")\n",
    "    \n",
    "    # ‚îÄ‚îÄ Fonction pour formater les documents r√©cup√©r√©s ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(\n",
    "            f\"[Page {doc.metadata.get('page', '?')+1}]\\n{doc.page_content}\"\n",
    "            for doc in docs\n",
    "        )\n",
    "    \n",
    "    # ‚îÄ‚îÄ Construire la cha√Æne RAG avec LCEL ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # LCEL = LangChain Expression Language (syntaxe | pour cha√Æner)\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": retriever | format_docs,  # Retriever ‚Üí formatage\n",
    "            \"question\": RunnablePassthrough()     # Question pass√©e directement\n",
    "        }\n",
    "        | RAG_PROMPT      # Injection dans le prompt template\n",
    "        | llm              # Appel au LLM\n",
    "        | StrOutputParser() # Extraction du texte de r√©ponse\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Pipeline RAG LangChain cr√©√© avec LCEL\")\n",
    "    print(\"   Cha√Æne : retriever ‚Üí format_docs ‚Üí prompt ‚Üí llm ‚Üí parser\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cl√© API OpenAI manquante\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du pipeline LangChain\n",
    "if OPENAI_API_KEY:\n",
    "    test_questions_lc = [\n",
    "        \"Qu'est-ce que le deep learning ?\",\n",
    "        \"Quels mod√®les LLM sont mentionn√©s dans le document ?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"ü¶ú TEST DU RAG LANGCHAIN\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for question in test_questions_lc:\n",
    "        print(f\"\\n‚ùì Question : {question}\")\n",
    "        start = time.time()\n",
    "        answer = rag_chain.invoke(question)\n",
    "        elapsed = (time.time() - start) * 1000\n",
    "        print(f\"üí¨ R√©ponse ({elapsed:.0f}ms) :\")\n",
    "        print(answer)\n",
    "        print(\"-\" * 60)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cl√© API OpenAI manquante\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 RAG Conversationnel avec LangChain (avec m√©moire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPENAI_API_KEY:\n",
    "    # Prompt pour reformuler la question en tenant compte de l'historique\n",
    "    CONTEXTUALIZE_Q_PROMPT = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\n",
    "         \"√âtant donn√© l'historique de conversation et la derni√®re question de l'utilisateur \"\n",
    "         \"qui peut r√©f√©rencer le contexte de la conversation, reformule la question pour qu'elle \"\n",
    "         \"soit autonome et compr√©hensible sans l'historique. \"\n",
    "         \"Ne r√©ponds PAS √† la question, reformule-la seulement si n√©cessaire, sinon retourne-la telle quelle.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    \n",
    "    # Prompt pour la r√©ponse finale\n",
    "    QA_PROMPT = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\n",
    "         \"Tu es un assistant expert. R√©ponds √† la question en te basant UNIQUEMENT sur le contexte fourni. \"\n",
    "         \"Si l'information n'est pas disponible, dis-le clairement. R√©ponds en fran√ßais.\\n\\n\"\n",
    "         \"Contexte :\\n{context}\"\n",
    "        ),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    \n",
    "    # Cha√Æne de reformulation de question\n",
    "    contextualize_q_chain = CONTEXTUALIZE_Q_PROMPT | llm | StrOutputParser()\n",
    "    \n",
    "    def contextualized_question(inputs: dict):\n",
    "        \"\"\"Reformule la question si un historique est pr√©sent.\"\"\"\n",
    "        if inputs.get(\"chat_history\"):\n",
    "            return contextualize_q_chain\n",
    "        return inputs[\"input\"]\n",
    "    \n",
    "    # Pipeline RAG conversationnel\n",
    "    conversational_rag_chain = (\n",
    "        RunnablePassthrough.assign(\n",
    "            context=contextualized_question | retriever | format_docs\n",
    "        )\n",
    "        | QA_PROMPT\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ RAG Conversationnel LangChain cr√©√©\")\n",
    "    \n",
    "    # Test de conversation multi-tours\n",
    "    print(\"\\nüí≠ TEST DE CONVERSATION MULTI-TOURS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    chat_history = []\n",
    "    \n",
    "    conv_questions = [\n",
    "        \"Qu'est-ce que le machine learning ?\",\n",
    "        \"Quels en sont les diff√©rents types ?\",\n",
    "        \"Donne un exemple d'application r√©elle.\"\n",
    "    ]\n",
    "    \n",
    "    for turn, question in enumerate(conv_questions, 1):\n",
    "        print(f\"\\n[Tour {turn}] üë§ User : {question}\")\n",
    "        \n",
    "        answer = conversational_rag_chain.invoke({\n",
    "            \"input\": question,\n",
    "            \"chat_history\": chat_history\n",
    "        })\n",
    "        \n",
    "        # Mettre √† jour l'historique\n",
    "        chat_history.extend([\n",
    "            HumanMessage(content=question),\n",
    "            AIMessage(content=answer)\n",
    "        ])\n",
    "        \n",
    "        print(f\"[Tour {turn}] ü§ñ Bot  : {answer[:350]}...\" if len(answer) > 350 else f\"[Tour {turn}] ü§ñ Bot  : {answer}\")\n",
    "        print(f\"            üíæ Historique : {len(chat_history)} messages\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cl√© API OpenAI manquante\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 RAG Avanc√© avec LangChain : Multi-Query Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPENAI_API_KEY:\n",
    "    from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "    \n",
    "    # Multi-Query Retriever : g√©n√®re plusieurs variantes de la question\n",
    "    # pour maximiser le recall\n",
    "    multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "        retriever=lc_vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        llm=llm\n",
    "    )\n",
    "    \n",
    "    # Activer les logs pour voir les questions g√©n√©r√©es\n",
    "    import logging\n",
    "    logging.basicConfig()\n",
    "    logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "    \n",
    "    test_q = \"Comment l'IA est-elle utilis√©e dans la sant√© ?\"\n",
    "    print(f\"üîç Multi-Query Retriever ‚Äî Question originale : {test_q}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # R√©cup√©rer avec plusieurs requ√™tes g√©n√©r√©es automatiquement\n",
    "    multi_docs = multi_query_retriever.invoke(test_q)\n",
    "    \n",
    "    print(f\"\\nüìã {len(multi_docs)} chunks r√©cup√©r√©s (d√©dupliqu√©s) :\")\n",
    "    for i, doc in enumerate(multi_docs[:3], 1):\n",
    "        print(f\"\\n  [{i}] Page {doc.metadata.get('page', '?')+1} : {doc.page_content[:150]}...\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cl√© API OpenAI manquante\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ‚öñÔ∏è Section 8 ‚Äî Comparaison et Bonnes Pratiques\n",
    "\n",
    "---\n",
    "\n",
    "## 8.1 Sans LangChain vs Avec LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison visuelle du code pour la m√™me op√©ration\n",
    "\n",
    "print(\"\\nüìä COMPARAISON DE CODE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "SANS LANGCHAIN (code explicite, ~10 lignes pour une query)\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "  # 1. Embed la question\n",
    "  embedding = client.embeddings.create(input=[q], model=\"text-embedding-3-small\")\n",
    "  \n",
    "  # 2. Chercher dans ChromaDB\n",
    "  results = collection.query(query_embeddings=[embedding.data[0].embedding], n_results=3)\n",
    "  \n",
    "  # 3. Construire le contexte\n",
    "  context = \"\\\\n\\\\n\".join(results[\"documents\"][0])\n",
    "  \n",
    "  # 4. Appeler le LLM\n",
    "  response = client.chat.completions.create(\n",
    "      model=\"gpt-4o-mini\",\n",
    "      messages=[{\"role\": \"user\", \"content\": f\"Contexte: {context}\\\\n\\\\nQuestion: {q}\"}]\n",
    "  )\n",
    "  answer = response.choices[0].message.content\n",
    "\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "AVEC LANGCHAIN LCEL (abstrait, ~3 lignes pour une query)\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "  rag_chain = (\n",
    "      {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "      | prompt | llm | StrOutputParser()\n",
    "  )\n",
    "  answer = rag_chain.invoke(question)\n",
    "\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Guide de D√©cision : Quand Utiliser Quoi ?\n",
    "\n",
    "```\n",
    "Votre situation\n",
    "      ‚îÇ\n",
    "      ‚îú‚îÄ Prototypage rapide / apprentissage ?  ‚Üí  LangChain (productivit√©)\n",
    "      ‚îÇ\n",
    "      ‚îú‚îÄ Comprendre le fonctionnement profond ? ‚Üí  Sans LangChain (transparence)\n",
    "      ‚îÇ\n",
    "      ‚îú‚îÄ Contr√¥le total du pipeline ?           ‚Üí  Sans LangChain\n",
    "      ‚îÇ\n",
    "      ‚îú‚îÄ Int√©grer 10+ sources de donn√©es ?      ‚Üí  LangChain (connecteurs)\n",
    "      ‚îÇ\n",
    "      ‚îú‚îÄ Besoin d'agents/outils complexes ?     ‚Üí  LangGraph (sur LangChain)\n",
    "      ‚îÇ\n",
    "      ‚îî‚îÄ Production avec monitoring ?           ‚Üí  LangSmith + LangChain\n",
    "```\n",
    "\n",
    "## 8.3 Bonnes Pratiques RAG\n",
    "\n",
    "### üîß Optimisation du Chunking\n",
    "```\n",
    "Type de document       Chunk size recommand√©    Overlap\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ         ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ     ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "FAQ                    200‚Äì300 tokens           20%\n",
    "Articles/Blog          400‚Äì600 tokens           15%\n",
    "Documentation tech.    300‚Äì500 tokens           20%\n",
    "Contrats/Legal         500‚Äì800 tokens           10%\n",
    "Code source            200‚Äì400 tokens           10%\n",
    "```\n",
    "\n",
    "### üéØ Optimisation du Retrieval\n",
    "- **Top-K** : Commencer avec k=3-5, augmenter si les r√©ponses manquent d'info\n",
    "- **Hybrid search** : Combiner dense (s√©mantique) + sparse (BM25) pour plus de robustesse\n",
    "- **Reranking** : Utiliser un cross-encoder apr√®s le retrieval initial\n",
    "- **Metadata filtering** : Filtrer par date, cat√©gorie, source pour pr√©ciser la recherche\n",
    "\n",
    "### üí° Optimisation du Prompt\n",
    "- Instruire le LLM de citer les sources\n",
    "- Demander explicitement de signaler l'absence d'information\n",
    "- Utiliser temperature ‚â§ 0.3 pour les Q&A factuels\n",
    "- Tester diff√©rents formats de contexte (liste, paragraphe, JSON)\n",
    "\n",
    "### üìä √âvaluation du RAG\n",
    "\n",
    "| M√©trique | Description | Outil |\n",
    "|----------|-------------|-------|\n",
    "| **Faithfulness** | La r√©ponse est-elle fond√©e sur le contexte ? | RAGAS |\n",
    "| **Answer Relevancy** | La r√©ponse r√©pond-elle √† la question ? | RAGAS |\n",
    "| **Context Precision** | Les chunks r√©cup√©r√©s sont-ils pertinents ? | RAGAS |\n",
    "| **Context Recall** | Tous les chunks n√©cessaires ont-ils √©t√© r√©cup√©r√©s ? | RAGAS |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√© r√©capitulatif\n",
    "print(\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                    üìö R√âCAPITULATIF DU COURS RAG                    ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                                      ‚ïë\n",
    "‚ïë  RAG = Retrieval-Augmented Generation                                ‚ïë\n",
    "‚ïë  Objectif : Ancrer les r√©ponses LLM dans des faits v√©rifiables      ‚ïë\n",
    "‚ïë                                                                      ‚ïë\n",
    "‚ïë  PIPELINE :                                                          ‚ïë\n",
    "‚ïë  PDF ‚Üí Chunking ‚Üí Embeddings ‚Üí ChromaDB ‚Üí Retrieval ‚Üí LLM ‚Üí Answer  ‚ïë\n",
    "‚ïë                                                                      ‚ïë\n",
    "‚ïë  TYPES DE RAG :                                                      ‚ïë\n",
    "‚ïë  ‚Ä¢ Naive RAG     : Simple, efficace pour des Q&A basiques           ‚ïë\n",
    "‚ïë  ‚Ä¢ Advanced RAG  : Query rewriting, reranking, HyDE                 ‚ïë\n",
    "‚ïë  ‚Ä¢ Modular RAG   : Multi-sources, agents, Self-RAG, Graph RAG       ‚ïë\n",
    "‚ïë                                                                      ‚ïë\n",
    "‚ïë  M√âMOIRE :                                                           ‚ïë\n",
    "‚ïë  ‚Ä¢ Short-term    : In-context (conversation history)                 ‚ïë\n",
    "‚ïë  ‚Ä¢ Long-term     : Vector store (s√©mantique) / DB (structur√©)       ‚ïë\n",
    "‚ïë  ‚Ä¢ Strat√©gies    : Buffer, Window, Summary, Vector memory           ‚ïë\n",
    "‚ïë                                                                      ‚ïë\n",
    "‚ïë  IMPL√âMENTATIONS :                                                   ‚ïë\n",
    "‚ïë  ‚Ä¢ Sans LangChain : Contr√¥le total, code explicite                  ‚ïë\n",
    "‚ïë  ‚Ä¢ Avec LangChain : Concis, √©cosyst√®me riche, LCEL                  ‚ïë\n",
    "‚ïë                                                                      ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéì Exercices Pratiques\n",
    "\n",
    "### Exercice 1 : Tester avec votre propre PDF\n",
    "1. Remplacer `PDF_PATH` par le chemin vers votre PDF\n",
    "2. Re-ex√©cuter les cellules d'indexation\n",
    "3. Tester des questions sp√©cifiques √† votre document\n",
    "\n",
    "### Exercice 2 : Impl√©menter le reranking\n",
    "1. Installer `sentence-transformers`\n",
    "2. Apr√®s le retrieval, utiliser un cross-encoder pour re-classer les chunks\n",
    "3. Comparer les r√©ponses avant/apr√®s reranking\n",
    "\n",
    "### Exercice 3 : Hybrid Search\n",
    "1. Impl√©menter une recherche BM25 avec `rank_bm25`\n",
    "2. Combiner les scores BM25 et cosinus (Reciprocal Rank Fusion)\n",
    "3. Mesurer l'am√©lioration du recall\n",
    "\n",
    "### Exercice 4 : √âvaluer le RAG avec RAGAS\n",
    "1. Installer `ragas`\n",
    "2. Cr√©er un dataset de questions/r√©ponses de r√©f√©rence\n",
    "3. Mesurer faithfulness, answer_relevancy, context_precision\n",
    "\n",
    "### Exercice 5 : RAG avec filtrage par m√©tadonn√©es\n",
    "1. Indexer plusieurs PDFs avec des m√©tadonn√©es diff√©rentes (auteur, date, cat√©gorie)\n",
    "2. Permettre √† l'utilisateur de filtrer par source\n",
    "3. Tester `collection.query(where={\"category\": \"IA\"}, ...)` dans ChromaDB\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Ressources\n",
    "\n",
    "- üìÑ [RAG Paper original (Lewis et al., 2020)](https://arxiv.org/abs/2005.11401)\n",
    "- üìÑ [Survey on RAG (Gao et al., 2023)](https://arxiv.org/abs/2312.10997)\n",
    "- üåê [ChromaDB Documentation](https://docs.trychroma.com/)\n",
    "- üåê [LangChain RAG Guide](https://python.langchain.com/docs/tutorials/rag/)\n",
    "- üåê [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings)\n",
    "- üõ†Ô∏è [RAGAS ‚Äî Evaluation Framework](https://docs.ragas.io/)\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ F√©licitations ! Vous avez compl√©t√© le cours RAG complet !**\n",
    "\n",
    "Vous savez maintenant :\n",
    "- ‚úÖ Ce qu'est le RAG et ses composants\n",
    "- ‚úÖ Les diff√©rents types de RAG et quand les utiliser\n",
    "- ‚úÖ G√©rer la m√©moire dans un syst√®me RAG\n",
    "- ‚úÖ Impl√©menter un RAG de A √† Z sans LangChain\n",
    "- ‚úÖ Utiliser LangChain pour simplifier le pipeline"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "s11_rag_demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}