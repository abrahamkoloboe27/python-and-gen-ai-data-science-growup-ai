# S11 â€” RAG Demo: Pipeline End-to-End

## ğŸ¯ Objectifs
- Concevoir un pipeline RAG complet (ingestion â†’ embed â†’ index â†’ retrieve â†’ generate)
- ImplÃ©menter un endpoint FastAPI qui combine retrieval et gÃ©nÃ©ration
- Comprendre le chunking, les mÃ©tadatas, et la gestion du context window
- DÃ©ployer un RAG local fonctionnel

## ğŸ“‹ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Documents  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chunking   â”‚  (dÃ©coupage intelligent)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Embedding  â”‚  (sentence-transformers)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FAISS Index â”‚  (indexation vectorielle)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Retrieval â”‚â”€â”€â”€â”€â–¶â”‚   LLM    â”‚
â”‚   (top-k)   â”‚     â”‚ (GPT-3.5)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Response â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Installation

```bash
cd s11_rag_demo
pip install -r requirements.txt
```

## ğŸ“ Structure du projet

```
s11_rag_demo/
â”œâ”€â”€ README.md              # Ce fichier
â”œâ”€â”€ requirements.txt       # DÃ©pendances
â”œâ”€â”€ indexer.py            # Script d'indexation
â”œâ”€â”€ app.py                # API FastAPI
â”œâ”€â”€ rag_engine.py         # Logique RAG
â”œâ”€â”€ data/                 # DonnÃ©es sources
â”‚   â””â”€â”€ sample_docs.json
â”œâ”€â”€ index/                # Index FAISS (gÃ©nÃ©rÃ©)
â”‚   â”œâ”€â”€ faiss_index.bin
â”‚   â””â”€â”€ documents.pkl
â””â”€â”€ .env.example          # Configuration
```

## ğŸ”§ Configuration

1. CrÃ©er un fichier `.env`:
```bash
OPENAI_API_KEY=votre_clÃ©_api
EMBEDDING_MODEL=paraphrase-multilingual-MiniLM-L12-v2
CHUNK_SIZE=500
CHUNK_OVERLAP=50
TOP_K=3
```

## ğŸ“š Ã‰tape 1: Indexation

### PrÃ©parer les donnÃ©es
Placez vos documents dans `data/sample_docs.json`:
```json
[
  {
    "id": "doc1",
    "title": "Introduction au ML",
    "content": "Le machine learning est...",
    "metadata": {"category": "ML", "author": "Alice"}
  }
]
```

### Lancer l'indexation
```bash
python indexer.py --input data/sample_docs.json --output index/
```

Cette Ã©tape va:
1. Charger les documents
2. Les dÃ©couper en chunks
3. GÃ©nÃ©rer les embeddings
4. CrÃ©er l'index FAISS
5. Sauvegarder l'index sur disque

## ğŸŒ Ã‰tape 2: Lancer l'API

```bash
python app.py
# ou
uvicorn app:app --reload
```

L'API sera disponible sur `http://localhost:8000`

## ğŸ“– Utilisation de l'API

### Endpoints disponibles

#### 1. Health Check
```bash
curl http://localhost:8000/health
```

#### 2. RAG Query
```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Qu'\''est-ce que le machine learning?",
    "top_k": 3,
    "include_sources": true
  }'
```

RÃ©ponse:
```json
{
  "question": "Qu'est-ce que le machine learning?",
  "answer": "Le machine learning est une branche...",
  "sources": [
    {
      "doc_id": "doc1",
      "title": "Introduction au ML",
      "chunk": "Le machine learning est...",
      "score": 0.89
    }
  ],
  "metadata": {
    "model": "gpt-3.5-turbo",
    "retrieval_time_ms": 45,
    "generation_time_ms": 1200
  }
}
```

#### 3. Retrieve Only (sans gÃ©nÃ©ration)
```bash
curl -X POST "http://localhost:8000/retrieve" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Python pour data science",
    "top_k": 5
  }'
```

#### 4. Index Stats
```bash
curl http://localhost:8000/index/stats
```

## ğŸ” FonctionnalitÃ©s avancÃ©es

### Chunking intelligent
Le script supporte plusieurs stratÃ©gies:
- **Fixed size**: Chunks de taille fixe
- **Sentence-based**: DÃ©coupage par phrases
- **Semantic**: DÃ©coupage sÃ©mantique (expÃ©rimental)

### MÃ©tadonnÃ©es enrichies
Chaque chunk contient:
- `doc_id`: ID du document source
- `chunk_id`: ID du chunk
- `title`: Titre du document
- `category`: CatÃ©gorie
- `start_pos`: Position de dÃ©but dans le document

### Reranking (optionnel)
AmÃ©liore la pertinence des rÃ©sultats aprÃ¨s retrieval initial.

### Context window management
Gestion automatique pour ne pas dÃ©passer la limite du modÃ¨le.

## ğŸ§ª Tests

```bash
# Tester l'indexation
pytest tests/test_indexer.py

# Tester l'API
pytest tests/test_api.py

# Tester le RAG engine
pytest tests/test_rag_engine.py
```

## ğŸ“Š Monitoring et Logs

L'API logge automatiquement:
- Temps de retrieval
- Temps de gÃ©nÃ©ration
- Nombre de tokens utilisÃ©s
- CoÃ»t estimÃ©

Exemple de log:
```
[2024-01-22 10:30:45] INFO: Query received: "Qu'est-ce que le ML?"
[2024-01-22 10:30:45] INFO: Retrieved 3 documents in 45ms
[2024-01-22 10:30:46] INFO: Generated response in 1.2s (250 tokens)
[2024-01-22 10:30:46] INFO: Estimated cost: $0.0015
```

## ğŸ”’ SÃ©curitÃ©

- Validation des inputs utilisateur
- Rate limiting (10 requÃªtes/min par IP)
- Sanitization des queries
- Pas de stockage des requÃªtes sensibles

## ğŸš¢ DÃ©ploiement

### Docker
```bash
docker build -t rag-demo .
docker run -p 8000:8000 --env-file .env rag-demo
```

### Docker Compose
```bash
docker-compose up -d
```

## ğŸ’¡ Bonnes pratiques

1. **Chunking**: Ajuster `CHUNK_SIZE` selon votre use case
   - FAQ: 200-300 tokens
   - Articles: 500-1000 tokens
   - Documentation technique: 300-500 tokens

2. **Top-k**: Commencer avec 3-5, ajuster selon le besoin
   - Plus grand = plus de contexte mais plus de bruit
   - Plus petit = plus prÃ©cis mais peut manquer d'info

3. **Prompt template**: Customiser selon votre domaine
   - SpÃ©cifier le ton et le style
   - Ajouter des instructions de format
   - GÃ©rer les cas oÃ¹ rien n'est trouvÃ©

4. **Caching**: Cache les embeddings frÃ©quents
   - RÃ©duire les appels API
   - AmÃ©liorer la latence

## ğŸ› DÃ©pannage

### "Index not found"
```bash
# RÃ©indexer
python indexer.py --input data/sample_docs.json --output index/
```

### "API key invalid"
```bash
# VÃ©rifier le .env
cat .env | grep OPENAI_API_KEY
```

### "Memory error with FAISS"
```bash
# RÃ©duire la taille des chunks ou utiliser IVF index
python indexer.py --index-type ivf --nlist 100
```

## ğŸ“š Ressources

- [RAG Paper (Lewis et al.)](https://arxiv.org/abs/2005.11401)
- [LangChain RAG Guide](https://python.langchain.com/docs/use_cases/question_answering/)
- [FAISS Documentation](https://github.com/facebookresearch/faiss)
- [FastAPI Best Practices](https://fastapi.tiangolo.com/tutorial/)

## ğŸ“ Exercices

### Exercice 1: Ajouter un document
1. Ajouter un nouveau document dans `data/`
2. RÃ©indexer
3. Tester une query sur ce document

### Exercice 2: Customiser le prompt
1. Modifier le prompt template dans `rag_engine.py`
2. Tester diffÃ©rents styles de rÃ©ponse
3. Comparer les rÃ©sultats

### Exercice 3: ImplÃ©menter le reranking
1. Ajouter une Ã©tape de reranking aprÃ¨s retrieval
2. Utiliser un modÃ¨le cross-encoder
3. Mesurer l'amÃ©lioration du recall

### Exercice 4: Ajouter des filtres
1. Permettre le filtrage par catÃ©gorie
2. Filtrer par date
3. Combiner plusieurs filtres

## âœ… Checklist

- [ ] DÃ©pendances installÃ©es
- [ ] Index crÃ©Ã© avec succÃ¨s
- [ ] API lance correctement
- [ ] Health check passe
- [ ] Query RAG fonctionne
- [ ] Sources retournÃ©es correctement
- [ ] Logs fonctionnels
- [ ] Tests passent

---

**Mini-projet S11 complÃ©tÃ©! ğŸ‰**

## ğŸ“ Support

Pour toute question:
1. VÃ©rifier les logs
2. Consulter ce README
3. Tester avec `curl` les endpoints
4. VÃ©rifier le format des donnÃ©es d'entrÃ©e
