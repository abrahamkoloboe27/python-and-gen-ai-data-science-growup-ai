"""
Indexer - Script pour crÃ©er l'index FAISS Ã  partir de documents
"""
import argparse
import json
import os
from typing import List, Dict, Any
import numpy as np
import pandas as pd
import faiss
from sentence_transformers import SentenceTransformer
import pickle


def load_documents(input_path: str) -> List[Dict[str, Any]]:
    """
    Charger les documents depuis un fichier JSON
    
    Args:
        input_path: Chemin vers le fichier JSON
        
    Returns:
        Liste de documents
    """
    print(f"ğŸ“– Chargement des documents depuis: {input_path}")
    
    with open(input_path, 'r', encoding='utf-8') as f:
        documents = json.load(f)
    
    print(f"âœ… {len(documents)} documents chargÃ©s")
    return documents


def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
    """
    DÃ©couper un texte en chunks avec overlap
    
    Args:
        text: Texte Ã  dÃ©couper
        chunk_size: Taille des chunks en caractÃ¨res
        overlap: Overlap entre chunks
        
    Returns:
        Liste de chunks
    """
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        
        # Ne pas couper au milieu d'un mot
        if end < len(text) and text[end] not in [' ', '\n', '.', ',', '!', '?']:
            # Chercher le dernier espace
            last_space = chunk.rfind(' ')
            if last_space > chunk_size // 2:  # Au moins 50% du chunk
                chunk = chunk[:last_space]
                end = start + last_space
        
        chunks.append(chunk.strip())
        start = end - overlap
    
    return chunks


def process_documents(
    documents: List[Dict[str, Any]],
    chunk_size: int = 500,
    overlap: int = 50
) -> pd.DataFrame:
    """
    Traiter les documents: chunking + mÃ©tadonnÃ©es
    
    Args:
        documents: Liste de documents
        chunk_size: Taille des chunks
        overlap: Overlap entre chunks
        
    Returns:
        DataFrame avec chunks et mÃ©tadonnÃ©es
    """
    print(f"ğŸ”„ Traitement des documents (chunk_size={chunk_size}, overlap={overlap})")
    
    processed = []
    
    for doc in documents:
        doc_id = doc.get('id', f"doc_{len(processed)}")
        title = doc.get('title', 'Untitled')
        content = doc.get('content', '')
        metadata = doc.get('metadata', {})
        
        # Chunking
        chunks = chunk_text(content, chunk_size, overlap)
        
        # CrÃ©er une entrÃ©e par chunk
        for i, chunk in enumerate(chunks):
            processed.append({
                'id': doc_id,
                'chunk_id': f"{doc_id}_chunk_{i}",
                'title': title,
                'content': content,  # Garder le contenu complet
                'chunk': chunk,
                'chunk_index': i,
                'total_chunks': len(chunks),
                'metadata': metadata
            })
    
    df = pd.DataFrame(processed)
    print(f"âœ… {len(df)} chunks crÃ©Ã©s depuis {len(documents)} documents")
    
    return df


def create_embeddings(
    texts: List[str],
    model_name: str = "paraphrase-multilingual-MiniLM-L12-v2"
) -> np.ndarray:
    """
    CrÃ©er des embeddings pour une liste de textes
    
    Args:
        texts: Liste de textes
        model_name: Nom du modÃ¨le d'embeddings
        
    Returns:
        Array numpy d'embeddings
    """
    print(f"ğŸ”„ Chargement du modÃ¨le: {model_name}")
    model = SentenceTransformer(model_name)
    
    print(f"ğŸ”„ GÃ©nÃ©ration des embeddings pour {len(texts)} textes...")
    embeddings = model.encode(texts, show_progress_bar=True, batch_size=32)
    
    print(f"âœ… Embeddings crÃ©Ã©s: shape = {embeddings.shape}")
    return np.array(embeddings).astype('float32')


def create_index(embeddings: np.ndarray, index_type: str = "flat") -> faiss.Index:
    """
    CrÃ©er un index FAISS
    
    Args:
        embeddings: Embeddings Ã  indexer
        index_type: Type d'index ('flat' ou 'ivf')
        
    Returns:
        Index FAISS
    """
    dimension = embeddings.shape[1]
    
    if index_type == "flat":
        print(f"ğŸ”„ CrÃ©ation d'un index Flat (dimension={dimension})")
        index = faiss.IndexFlatL2(dimension)
    elif index_type == "ivf":
        nlist = min(100, len(embeddings) // 10)  # Nombre de clusters
        print(f"ğŸ”„ CrÃ©ation d'un index IVF (dimension={dimension}, nlist={nlist})")
        quantizer = faiss.IndexFlatL2(dimension)
        index = faiss.IndexIVFFlat(quantizer, dimension, nlist)
        index.train(embeddings)
    else:
        raise ValueError(f"Index type '{index_type}' not supported")
    
    print(f"ğŸ”„ Ajout de {len(embeddings)} vecteurs Ã  l'index...")
    index.add(embeddings)
    
    print(f"âœ… Index crÃ©Ã© avec {index.ntotal} vecteurs")
    return index


def save_index(
    index: faiss.Index,
    documents: pd.DataFrame,
    embeddings: np.ndarray,
    output_dir: str
):
    """
    Sauvegarder l'index et les documents
    
    Args:
        index: Index FAISS
        documents: DataFrame des documents
        embeddings: Embeddings
        output_dir: RÃ©pertoire de sortie
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # Sauvegarder l'index FAISS
    index_path = os.path.join(output_dir, "faiss_index.bin")
    faiss.write_index(index, index_path)
    print(f"âœ… Index sauvegardÃ©: {index_path}")
    
    # Sauvegarder les documents et embeddings
    data_path = os.path.join(output_dir, "documents.pkl")
    data = {
        "documents": documents,
        "embeddings": embeddings
    }
    with open(data_path, 'wb') as f:
        pickle.dump(data, f)
    print(f"âœ… Documents sauvegardÃ©s: {data_path}")
    
    # Sauvegarder des stats
    stats_path = os.path.join(output_dir, "stats.json")
    stats = {
        "total_documents": len(documents['id'].unique()),
        "total_chunks": len(documents),
        "embedding_dimension": embeddings.shape[1],
        "index_type": index.__class__.__name__
    }
    with open(stats_path, 'w') as f:
        json.dump(stats, f, indent=2)
    print(f"âœ… Stats sauvegardÃ©es: {stats_path}")


def main():
    parser = argparse.ArgumentParser(description="Indexer des documents pour RAG")
    parser.add_argument(
        "--input",
        type=str,
        required=True,
        help="Chemin vers le fichier JSON de documents"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="index/",
        help="RÃ©pertoire de sortie pour l'index"
    )
    parser.add_argument(
        "--embedding-model",
        type=str,
        default="paraphrase-multilingual-MiniLM-L12-v2",
        help="ModÃ¨le d'embeddings"
    )
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=500,
        help="Taille des chunks en caractÃ¨res"
    )
    parser.add_argument(
        "--chunk-overlap",
        type=int,
        default=50,
        help="Overlap entre chunks"
    )
    parser.add_argument(
        "--index-type",
        type=str,
        default="flat",
        choices=["flat", "ivf"],
        help="Type d'index FAISS"
    )
    
    args = parser.parse_args()
    
    print("="*80)
    print("ğŸš€ RAG Indexer")
    print("="*80)
    
    # Ã‰tape 1: Charger les documents
    documents = load_documents(args.input)
    
    # Ã‰tape 2: Chunking
    df = process_documents(documents, args.chunk_size, args.chunk_overlap)
    
    # Ã‰tape 3: CrÃ©er les embeddings
    texts = df['chunk'].tolist()
    embeddings = create_embeddings(texts, args.embedding_model)
    
    # Ã‰tape 4: CrÃ©er l'index
    index = create_index(embeddings, args.index_type)
    
    # Ã‰tape 5: Sauvegarder
    save_index(index, df, embeddings, args.output)
    
    print("="*80)
    print("âœ… Indexation terminÃ©e avec succÃ¨s!")
    print("="*80)


if __name__ == "__main__":
    main()
