# S8 ‚Äî Fondamentaux LLM & Transformers

## üìö Note Technique : Concepts Essentiels

### üéØ Objectifs de la s√©ance
- Comprendre l'architecture Transformer
- Ma√Ætriser les concepts d'embeddings et tokens
- Comprendre le context window et ses implications
- Apprendre les hyperparam√®tres cl√©s (temperature, top-k, etc.)

---

## 1. Architecture Transformer

### 1.1 Principe de base
Les **Transformers** sont une architecture de r√©seaux de neurones introduite en 2017 (Vaswani et al., "Attention is All You Need"). Ils ont r√©volutionn√© le traitement du langage naturel.

**Composants cl√©s:**
- **Self-Attention**: M√©canisme permettant au mod√®le de pond√©rer l'importance de chaque mot par rapport aux autres
- **Multi-Head Attention**: Plusieurs m√©canismes d'attention en parall√®le pour capturer diff√©rentes relations
- **Feed-Forward Networks**: Couches fully-connected apr√®s l'attention
- **Positional Encoding**: Injection d'information sur la position des tokens
- **Layer Normalization**: Normalisation pour stabiliser l'entra√Ænement

### 1.2 Encodeur vs D√©codeur
- **Encodeur**: Traite l'input et produit des repr√©sentations contextuelles (ex: BERT)
- **D√©codeur**: G√©n√®re l'output de mani√®re autor√©gressive (ex: GPT)
- **Encodeur-D√©codeur**: Combinaison pour t√¢ches de traduction (ex: T5, BART)

### 1.3 Architecture GPT (Generative Pre-trained Transformer)
```
Input ‚Üí Tokenization ‚Üí Embeddings ‚Üí 
‚Üí Positional Encoding ‚Üí 
‚Üí N √ó [Multi-Head Attention ‚Üí Feed-Forward] ‚Üí 
‚Üí Output Layer (Logits) ‚Üí 
‚Üí Sampling ‚Üí Generated Text
```

---

## 2. Tokenisation

### 2.1 Qu'est-ce qu'un token?
Un **token** est l'unit√© de base trait√©e par un LLM. Ce n'est pas toujours un mot!

**Types de tokenisation:**
- **Caract√®res**: Chaque caract√®re = 1 token (ex: "chat" = 4 tokens)
- **Mots**: Chaque mot = 1 token (vocabulaire √©norme)
- **Subword**: Compromis intelligent (BPE, WordPiece, SentencePiece)

### 2.2 Exemple avec GPT (BPE - Byte Pair Encoding)
```
Texte: "tokenization"
Tokens: ["token", "ization"]  # 2 tokens

Texte: "L'intelligence artificielle"
Tokens: ["L", "'", "intelligence", " art", "ific", "ielle"]  # 6 tokens
```

### 2.3 Implications pratiques
- **Co√ªt**: Factur√© au nombre de tokens (input + output)
- **Context window**: Limit√© en tokens, pas en caract√®res
- **Performance**: Mots rares = plusieurs tokens = moins efficace

**R√®gle approximative**: 
- Anglais: ~1 token = 4 caract√®res = 0.75 mots
- Fran√ßais: ~1 token = 3-3.5 caract√®res (plus de tokens pour m√™me texte)

---

## 3. Embeddings vs Logits

### 3.1 Embeddings
Les **embeddings** sont des repr√©sentations vectorielles denses de tokens.

```python
# Exemple conceptuel
token_id = 1234
embedding = embedding_layer[token_id]  # Vector de dimension 768, 1024, 4096...
# embedding = [0.12, -0.34, 0.56, ..., 0.23]
```

**Caract√©ristiques:**
- Dimension fixe (ex: 768 pour BERT-base, 12288 pour GPT-4)
- Tokens similaires ont embeddings proches (distance cosinus)
- Appris pendant l'entra√Ænement
- Contextuels (pour Transformers): m√™me mot a diff√©rents embeddings selon contexte

### 3.2 Logits
Les **logits** sont les scores bruts de sortie avant softmax.

```python
# Dernier layer du mod√®le
logits = model_output  # Vector de taille = taille vocabulaire (ex: 50,000)
# logits = [2.3, -1.2, 5.6, ..., 0.8]

# Conversion en probabilit√©s
probs = softmax(logits)
# probs = [0.002, 0.0001, 0.05, ..., 0.001]  # Somme = 1
```

**Usage:**
- **G√©n√©ration**: S√©lection du prochain token via sampling des logits
- **Classification**: Argmax des logits pour pr√©dire la classe

### 3.3 Flow complet
```
Input text ‚Üí Tokens ‚Üí Embeddings ‚Üí 
‚Üí Transformer Layers ‚Üí 
‚Üí Logits ‚Üí Sampling ‚Üí Next Token ‚Üí 
‚Üí Repeat (autoregressive)
```

---

## 4. Context Window

### 4.1 D√©finition
Le **context window** est la quantit√© maximale de tokens que le mod√®le peut traiter simultan√©ment.

**Exemples:**
- GPT-3.5-turbo: 4,096 tokens (~3,000 mots)
- GPT-4: 8,192 tokens (version standard)
- GPT-4-32k: 32,768 tokens (~24,000 mots)
- Claude 2: 100,000 tokens (~75,000 mots)

### 4.2 Implications
**Limites:**
- Input + Output ‚â§ Context Window
- Au-del√†: tokens anciens sont oubli√©s (truncation)
- M√©moire limit√©e pour conversations longues

**Strat√©gies:**
- **Summarization**: R√©sumer l'historique
- **Chunking**: D√©couper les documents longs
- **Sliding window**: Garder les N derniers tokens
- **Retrieval**: Chercher l'info pertinente (RAG)

### 4.3 Co√ªt vs Context Window
Plus le context window est grand:
- Plus le co√ªt est √©lev√© (computation O(n¬≤) avec self-attention)
- Plus la latence augmente
- Meilleure coh√©rence sur longs textes

---

## 5. Hyperparam√®tres de G√©n√©ration

### 5.1 Temperature
Contr√¥le la "cr√©ativit√©" du mod√®le en ajustant les probabilit√©s.

```python
# Avant sampling
logits = [2.0, 1.0, 0.5]

# Avec temperature = 1.0 (par d√©faut)
probs = softmax(logits / 1.0) = [0.59, 0.24, 0.17]

# Avec temperature = 0.1 (plus d√©terministe)
probs = softmax(logits / 0.1) = [0.84, 0.11, 0.05]

# Avec temperature = 2.0 (plus al√©atoire)
probs = softmax(logits / 2.0) = [0.46, 0.30, 0.24]
```

**Usage:**
- **Temperature basse (0.1-0.5)**: R√©ponses pr√©cises, factuelles, r√©p√©tables
- **Temperature moyenne (0.7-1.0)**: √âquilibre cr√©ativit√©/coh√©rence
- **Temperature haute (1.5-2.0)**: Cr√©atif, vari√©, potentiellement incoh√©rent

### 5.2 Top-k Sampling
Limite le sampling aux k tokens les plus probables.

```python
# Probabilit√©s: [0.4, 0.3, 0.15, 0.10, 0.05]
# Top-k = 3
# Sample uniquement parmi: [0.4, 0.3, 0.15]
# Renormalisation: [0.47, 0.35, 0.18]
```

**Effet:** √âvite de choisir des tokens tr√®s improbables.

### 5.3 Top-p (Nucleus Sampling)
Limite le sampling aux tokens dont la probabilit√© cumulative ‚â§ p.

```python
# Probabilit√©s: [0.4, 0.3, 0.15, 0.10, 0.05]
# Top-p = 0.8
# Cumul: [0.4, 0.7, 0.85, 0.95, 1.0]
# Sample parmi les 3 premiers (0.4 + 0.3 + 0.15 = 0.85 > 0.8)
```

**Effet:** Adaptif selon la distribution (plus intelligent que top-k).

### 5.4 Autres param√®tres
- **max_tokens**: Nombre maximum de tokens g√©n√©r√©s
- **frequency_penalty**: P√©nalise les r√©p√©titions (0 √† 2)
- **presence_penalty**: Encourage nouveaux topics (0 √† 2)
- **stop_sequences**: Liste de tokens pour arr√™ter la g√©n√©ration

---

## 6. Co√ªt et Latence

### 6.1 Facteurs de co√ªt
**Tarification OpenAI (exemple GPT-4):**
- Input: $0.03 / 1K tokens
- Output: $0.06 / 1K tokens

**Calcul:**
```
Prompt: 1,000 tokens ‚Üí $0.03
R√©ponse: 500 tokens ‚Üí $0.03
Total: $0.06 par requ√™te
```

**Optimisations:**
- Mod√®les moins chers (GPT-3.5: 10x moins cher)
- Prompts plus courts
- Caching de prompts syst√®mes
- Batch requests

### 6.2 Latence
**Time to First Token (TTFT):**
- Temps avant le premier token g√©n√©r√©
- D√©pend de: taille du prompt, charge du serveur

**Throughput:**
- Tokens par seconde
- ~20-50 tokens/sec pour GPT-4
- ~100+ tokens/sec pour GPT-3.5

**Tradeoffs:**
- **Context window large**: Plus lent, plus cher, meilleure qualit√©
- **Temperature basse**: Plus rapide (moins de sampling), moins vari√©
- **Streaming**: Meilleure UX (tokens progressifs) mais m√™me co√ªt

---

## 7. Embeddings pour Retrieval

### 7.1 Embeddings de texte
Contrairement aux embeddings de tokens, les **text embeddings** repr√©sentent des phrases/documents entiers.

**Mod√®les populaires:**
- OpenAI: text-embedding-3-small (1536 dimensions)
- Sentence-BERT: Multi-lingual embeddings
- Cohere: embed-multilingual-v3.0

### 7.2 Utilisation
```python
# Pseudo-code
text = "Comment r√©initialiser mon mot de passe?"
embedding = get_embedding(text)  # [0.12, -0.34, ..., 0.56] (1536 dims)

# Similarity search
query_emb = get_embedding("reset password")
similarity = cosine_similarity(embedding, query_emb)  # 0.87 (tr√®s similaire)
```

### 7.3 Propri√©t√©s
- **S√©mantique**: Textes similaires en sens = embeddings proches
- **Multilingue**: Certains mod√®les alignent plusieurs langues
- **Fixed dimension**: Tous les textes ‚Üí m√™me dimension

---

## 8. Conclusion

### Points cl√©s √† retenir
1. **Transformers** = architecture √† base d'attention, base des LLMs modernes
2. **Tokens** = unit√©s de traitement (‚â† mots), impactent co√ªt et capacit√©s
3. **Embeddings** = repr√©sentations vectorielles denses
4. **Logits** = scores bruts avant sampling
5. **Context window** = m√©moire limit√©e du mod√®le
6. **Temperature/Top-k/Top-p** = contr√¥le de la g√©n√©ration
7. **Co√ªt** = fonction du nombre de tokens (input + output)
8. **Latence** = tradeoff avec qualit√© et context window

### Ressources
- [Attention Is All You Need (paper)](https://arxiv.org/abs/1706.03762)
- [OpenAI Tokenizer](https://platform.openai.com/tokenizer)
- [Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)

---

## üìù Exercice Pratique

### Partie 1: Analyse de tokenisation
1. Utilisez l'outil [OpenAI Tokenizer](https://platform.openai.com/tokenizer)
2. Testez diff√©rents textes (anglais vs fran√ßais)
3. Calculez le ratio caract√®res/tokens

### Partie 2: Exp√©rimentation avec prompts
Cr√©ez 3 prompts exp√©rimentaux documentant l'impact des hyperparam√®tres:

#### Prompt 1: Impact de la temperature
**T√¢che**: G√©n√©ration cr√©ative
**Prompt**: "√âcris une histoire courte sur un robot qui d√©couvre l'art."

Tester:
- Temperature = 0.2
- Temperature = 1.0
- Temperature = 1.8

**Observations**: [√Ä documenter]

#### Prompt 2: Top-k vs Top-p
**T√¢che**: R√©ponse technique
**Prompt**: "Explique en 3 phrases comment fonctionne le gradient descent."

Tester:
- Top-k = 10
- Top-p = 0.9
- Sans restriction

**Observations**: [√Ä documenter]

#### Prompt 3: Context window
**T√¢che**: R√©sum√©
**Prompt**: [Ins√©rer un texte long de 2000 mots]
"R√©sume ce texte en 100 mots."

Tester avec:
- Texte complet (dans la limite)
- Texte tronqu√© (au-del√† de la limite)

**Observations**: [√Ä documenter]

---

**Document cr√©√© pour la s√©ance S8 ‚Äî Fondamentaux LLM & Transformers**
