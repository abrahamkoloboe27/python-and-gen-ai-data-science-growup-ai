{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S12 ‚Äî Vector Databases & Production\n",
    "\n",
    "## üéØ Objectifs\n",
    "- Comparer FAISS et Milvus pour le stockage vectoriel\n",
    "- Mesurer les m√©triques de latence et recall\n",
    "- Comprendre les tradeoffs production (scalabilit√©, persistence)\n",
    "- Benchmarker diff√©rentes configurations d'index\n",
    "\n",
    "## üìã Contenu\n",
    "1. Introduction aux Vector Databases\n",
    "2. FAISS: Index local optimis√©\n",
    "3. Milvus: Base de donn√©es vectorielle distribu√©e\n",
    "4. Comparaison des performances\n",
    "5. Choix d'architecture pour la production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Installation des d√©pendances\n",
    "# !pip install faiss-cpu pymilvus sentence-transformers pandas numpy matplotlib scikit-learn"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from pymilvus import connections, Collection, CollectionSchema, FieldSchema, DataType, utility\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Biblioth√®ques import√©es\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cr√©ation du Dataset de Test\n",
    "\n",
    "Nous allons cr√©er un dataset substantiel pour tester les performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Charger le mod√®le d'embeddings\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "print(f\"‚úÖ Mod√®le charg√©: dimension = {model.get_sentence_embedding_dimension()}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cr√©er un dataset de 10,000 documents simul√©s\n",
    "np.random.seed(42)\n",
    "\n",
    "# Templates de documents\n",
    "templates = [\n",
    "    \"Le {topic} est essentiel pour {domain}.\",\n",
    "    \"Dans le domaine de {domain}, on utilise {topic}.\",\n",
    "    \"Les techniques de {topic} am√©liorent {domain}.\",\n",
    "    \"Pour {domain}, il est important de ma√Ætriser {topic}.\",\n",
    "    \"L'application de {topic} dans {domain} montre des r√©sultats prometteurs.\",\n",
    "]\n",
    "\n",
    "topics = [\n",
    "    \"machine learning\", \"deep learning\", \"transformers\", \"embeddings\",\n",
    "    \"RAG\", \"fine-tuning\", \"prompt engineering\", \"vector search\",\n",
    "    \"attention mechanism\", \"tokenization\", \"NLP\", \"computer vision\",\n",
    "    \"reinforcement learning\", \"neural networks\", \"optimization\"\n",
    "]\n",
    "\n",
    "domains = [\n",
    "    \"intelligence artificielle\", \"data science\", \"recherche d'information\",\n",
    "    \"traitement du langage\", \"vision par ordinateur\", \"recommandation\",\n",
    "    \"analyse pr√©dictive\", \"classification\", \"g√©n√©ration de texte\"\n",
    "]\n",
    "\n",
    "documents = []\n",
    "for i in range(10000):\n",
    "    template = np.random.choice(templates)\n",
    "    topic = np.random.choice(topics)\n",
    "    domain = np.random.choice(domains)\n",
    "    text = template.format(topic=topic, domain=domain)\n",
    "    documents.append({\n",
    "        \"id\": i,\n",
    "        \"text\": text,\n",
    "        \"topic\": topic,\n",
    "        \"domain\": domain\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(documents)\n",
    "print(f\"üìä Dataset cr√©√©: {len(df)} documents\")\n",
    "print(f\"Exemple: {df.iloc[0]['text']}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# G√©n√©rer les embeddings (peut prendre quelques minutes)\n",
    "print(\"üîÑ G√©n√©ration des embeddings...\")\n",
    "start = time.time()\n",
    "embeddings = model.encode(df['text'].tolist(), show_progress_bar=True, batch_size=256)\n",
    "embeddings = np.array(embeddings).astype('float32')\n",
    "duration = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ Embeddings g√©n√©r√©s en {duration:.2f}s\")\n",
    "print(f\"Shape: {embeddings.shape}\")\n",
    "print(f\"M√©moire: {embeddings.nbytes / 1024 / 1024:.2f} MB\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FAISS: Index Local\n",
    "\n",
    "### 3.1 Index Flat (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class FAISSIndex:\n",
    "    def __init__(self, dimension: int, index_type: str = 'flat'):\n",
    "        self.dimension = dimension\n",
    "        self.index_type = index_type\n",
    "        \n",
    "        if index_type == 'flat':\n",
    "            self.index = faiss.IndexFlatL2(dimension)\n",
    "        elif index_type == 'ivf':\n",
    "            # Index IVF pour meilleure performance sur grands datasets\n",
    "            quantizer = faiss.IndexFlatL2(dimension)\n",
    "            nlist = 100  # Nombre de clusters\n",
    "            self.index = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n",
    "        elif index_type == 'hnsw':\n",
    "            # Index HNSW pour recherche rapide\n",
    "            M = 32  # Nombre de connexions par layer\n",
    "            self.index = faiss.IndexHNSWFlat(dimension, M)\n",
    "        else:\n",
    "            raise ValueError(f\"Type d'index non support√©: {index_type}\")\n",
    "        \n",
    "        self.documents = []\n",
    "    \n",
    "    def train(self, vectors: np.ndarray):\n",
    "        \"\"\"Entra√Æner l'index (n√©cessaire pour IVF)\"\"\"\n",
    "        if self.index_type == 'ivf':\n",
    "            print(f\"üîÑ Entra√Ænement de l'index IVF...\")\n",
    "            self.index.train(vectors)\n",
    "    \n",
    "    def add(self, vectors: np.ndarray, documents: List[Dict]):\n",
    "        \"\"\"Ajouter des vecteurs √† l'index\"\"\"\n",
    "        self.index.add(vectors)\n",
    "        self.documents.extend(documents)\n",
    "    \n",
    "    def search(self, query_vector: np.ndarray, k: int = 5) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "        \"\"\"Rechercher les k plus proches voisins\"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        if self.index_type == 'ivf':\n",
    "            # Param√®tre nprobe: nombre de clusters √† examiner\n",
    "            self.index.nprobe = 10\n",
    "        \n",
    "        distances, indices = self.index.search(query_vector, k)\n",
    "        latency = (time.time() - start) * 1000  # en ms\n",
    "        \n",
    "        return distances, indices, latency\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        return {\n",
    "            \"index_type\": self.index_type,\n",
    "            \"dimension\": self.dimension,\n",
    "            \"total_vectors\": self.index.ntotal,\n",
    "            \"is_trained\": self.index.is_trained\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Classe FAISSIndex d√©finie\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Tester les diff√©rents types d'index FAISS\n",
    "index_types = ['flat', 'ivf', 'hnsw']\n",
    "faiss_indexes = {}\n",
    "\n",
    "for idx_type in index_types:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Cr√©ation de l'index FAISS: {idx_type.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    index = FAISSIndex(dimension=embeddings.shape[1], index_type=idx_type)\n",
    "    \n",
    "    # Entra√Ænement si n√©cessaire\n",
    "    start = time.time()\n",
    "    index.train(embeddings)\n",
    "    train_time = time.time() - start\n",
    "    \n",
    "    # Indexation\n",
    "    start = time.time()\n",
    "    index.add(embeddings, documents)\n",
    "    index_time = time.time() - start\n",
    "    \n",
    "    print(f\"‚è±Ô∏è  Temps d'entra√Ænement: {train_time:.2f}s\")\n",
    "    print(f\"‚è±Ô∏è  Temps d'indexation: {index_time:.2f}s\")\n",
    "    print(f\"üìä Stats: {index.get_stats()}\")\n",
    "    \n",
    "    faiss_indexes[idx_type] = index\n",
    "\n",
    "print(\"\\n‚úÖ Tous les index FAISS cr√©√©s\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Benchmark FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cr√©er des requ√™tes de test\n",
    "test_queries = [\n",
    "    \"Comment utiliser les transformers pour le NLP?\",\n",
    "    \"Qu'est-ce que le machine learning?\",\n",
    "    \"Optimisation des r√©seaux de neurones\",\n",
    "    \"Applications du deep learning en vision\",\n",
    "    \"Recherche vectorielle avec embeddings\"\n",
    "]\n",
    "\n",
    "query_embeddings = model.encode(test_queries)\n",
    "query_embeddings = np.array(query_embeddings).astype('float32')\n",
    "\n",
    "print(f\"‚úÖ {len(test_queries)} requ√™tes de test cr√©√©es\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Benchmarker chaque index FAISS\n",
    "k = 10  # Top-10\n",
    "faiss_results = {}\n",
    "\n",
    "for idx_type, index in faiss_indexes.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Benchmark FAISS-{idx_type.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    latencies = []\n",
    "    \n",
    "    for i, query in enumerate(query_embeddings):\n",
    "        distances, indices, latency = index.search(query.reshape(1, -1), k)\n",
    "        latencies.append(latency)\n",
    "        \n",
    "        if i == 0:  # Afficher les r√©sultats de la premi√®re requ√™te\n",
    "            print(f\"\\nRequ√™te: '{test_queries[i]}'\")\n",
    "            print(f\"Top-3 r√©sultats:\")\n",
    "            for j in range(min(3, len(indices[0]))):\n",
    "                idx = indices[0][j]\n",
    "                dist = distances[0][j]\n",
    "                print(f\"  {j+1}. (distance={dist:.4f}) {documents[idx]['text'][:80]}...\")\n",
    "    \n",
    "    avg_latency = np.mean(latencies)\n",
    "    p95_latency = np.percentile(latencies, 95)\n",
    "    p99_latency = np.percentile(latencies, 99)\n",
    "    \n",
    "    print(f\"\\nüìä Latence moyenne: {avg_latency:.2f} ms\")\n",
    "    print(f\"üìä Latence P95: {p95_latency:.2f} ms\")\n",
    "    print(f\"üìä Latence P99: {p99_latency:.2f} ms\")\n",
    "    \n",
    "    faiss_results[idx_type] = {\n",
    "        \"avg_latency\": avg_latency,\n",
    "        \"p95_latency\": p95_latency,\n",
    "        \"p99_latency\": p99_latency,\n",
    "        \"latencies\": latencies\n",
    "    }"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Milvus: Base de Donn√©es Vectorielle\n",
    "\n",
    "### 4.1 Configuration Milvus\n",
    "\n",
    "‚ö†Ô∏è **Note**: Pour utiliser Milvus, vous devez avoir un serveur Milvus en cours d'ex√©cution.\n",
    "\n",
    "```bash\n",
    "# Avec Docker\n",
    "docker run -d --name milvus_standalone \\\n",
    "  -p 19530:19530 -p 9091:9091 \\\n",
    "  milvusdb/milvus:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class MilvusIndex:\n",
    "    def __init__(self, collection_name: str, dimension: int, host: str = \"localhost\", port: str = \"19530\"):\n",
    "        self.collection_name = collection_name\n",
    "        self.dimension = dimension\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.connected = False\n",
    "        \n",
    "    def connect(self):\n",
    "        \"\"\"Se connecter √† Milvus\"\"\"\n",
    "        try:\n",
    "            connections.connect(\"default\", host=self.host, port=self.port)\n",
    "            self.connected = True\n",
    "            print(f\"‚úÖ Connect√© √† Milvus sur {self.host}:{self.port}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur de connexion √† Milvus: {e}\")\n",
    "            print(\"üí° Lancez d'abord un serveur Milvus (voir instructions ci-dessus)\")\n",
    "            self.connected = False\n",
    "    \n",
    "    def create_collection(self):\n",
    "        \"\"\"Cr√©er une collection Milvus\"\"\"\n",
    "        if not self.connected:\n",
    "            return False\n",
    "        \n",
    "        # Supprimer la collection si elle existe\n",
    "        if utility.has_collection(self.collection_name):\n",
    "            utility.drop_collection(self.collection_name)\n",
    "        \n",
    "        # D√©finir le sch√©ma\n",
    "        fields = [\n",
    "            FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=False),\n",
    "            FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=self.dimension),\n",
    "            FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=500)\n",
    "        ]\n",
    "        \n",
    "        schema = CollectionSchema(fields, description=\"Vector search collection\")\n",
    "        self.collection = Collection(self.collection_name, schema)\n",
    "        \n",
    "        print(f\"‚úÖ Collection '{self.collection_name}' cr√©√©e\")\n",
    "        return True\n",
    "    \n",
    "    def create_index(self, index_type: str = \"IVF_FLAT\"):\n",
    "        \"\"\"Cr√©er un index sur le champ embedding\"\"\"\n",
    "        if not self.connected:\n",
    "            return False\n",
    "        \n",
    "        index_params = {\n",
    "            \"metric_type\": \"L2\",\n",
    "            \"index_type\": index_type,\n",
    "            \"params\": {\"nlist\": 128}\n",
    "        }\n",
    "        \n",
    "        self.collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "        print(f\"‚úÖ Index {index_type} cr√©√©\")\n",
    "        return True\n",
    "    \n",
    "    def insert(self, vectors: np.ndarray, documents: List[Dict]):\n",
    "        \"\"\"Ins√©rer des vecteurs dans Milvus\"\"\"\n",
    "        if not self.connected:\n",
    "            return False\n",
    "        \n",
    "        ids = [doc['id'] for doc in documents]\n",
    "        texts = [doc['text'][:500] for doc in documents]  # Limiter la longueur\n",
    "        \n",
    "        entities = [\n",
    "            ids,\n",
    "            vectors.tolist(),\n",
    "            texts\n",
    "        ]\n",
    "        \n",
    "        self.collection.insert(entities)\n",
    "        self.collection.flush()\n",
    "        print(f\"‚úÖ {len(ids)} vecteurs ins√©r√©s\")\n",
    "        return True\n",
    "    \n",
    "    def search(self, query_vector: np.ndarray, k: int = 5) -> Tuple[List, float]:\n",
    "        \"\"\"Rechercher les k plus proches voisins\"\"\"\n",
    "        if not self.connected:\n",
    "            return [], 0.0\n",
    "        \n",
    "        self.collection.load()\n",
    "        \n",
    "        search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "        \n",
    "        start = time.time()\n",
    "        results = self.collection.search(\n",
    "            data=[query_vector.tolist()],\n",
    "            anns_field=\"embedding\",\n",
    "            param=search_params,\n",
    "            limit=k,\n",
    "            output_fields=[\"text\"]\n",
    "        )\n",
    "        latency = (time.time() - start) * 1000\n",
    "        \n",
    "        return results, latency\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        if not self.connected:\n",
    "            return {}\n",
    "        \n",
    "        return {\n",
    "            \"collection_name\": self.collection_name,\n",
    "            \"num_entities\": self.collection.num_entities,\n",
    "            \"dimension\": self.dimension\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Classe MilvusIndex d√©finie\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Tester Milvus (optionnel si serveur disponible)\n",
    "USE_MILVUS = False  # Mettre √† True si Milvus est disponible\n",
    "\n",
    "if USE_MILVUS:\n",
    "    milvus_index = MilvusIndex(\n",
    "        collection_name=\"vectors_demo\",\n",
    "        dimension=embeddings.shape[1]\n",
    "    )\n",
    "    \n",
    "    milvus_index.connect()\n",
    "    \n",
    "    if milvus_index.connected:\n",
    "        milvus_index.create_collection()\n",
    "        milvus_index.create_index(\"IVF_FLAT\")\n",
    "        \n",
    "        # Ins√©rer par batch pour √©viter la surcharge\n",
    "        batch_size = 1000\n",
    "        for i in range(0, len(embeddings), batch_size):\n",
    "            end_idx = min(i + batch_size, len(embeddings))\n",
    "            batch_embeddings = embeddings[i:end_idx]\n",
    "            batch_docs = documents[i:end_idx]\n",
    "            milvus_index.insert(batch_embeddings, batch_docs)\n",
    "        \n",
    "        print(f\"‚úÖ Stats Milvus: {milvus_index.get_stats()}\")\n",
    "        \n",
    "        # Benchmark Milvus\n",
    "        milvus_latencies = []\n",
    "        for query in query_embeddings:\n",
    "            results, latency = milvus_index.search(query, k=10)\n",
    "            milvus_latencies.append(latency)\n",
    "        \n",
    "        print(f\"\\nüìä Milvus - Latence moyenne: {np.mean(milvus_latencies):.2f} ms\")\n",
    "        print(f\"üìä Milvus - Latence P95: {np.percentile(milvus_latencies, 95):.2f} ms\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Milvus non utilis√© (USE_MILVUS=False)\")\n",
    "    print(\"üí° Pour tester Milvus, lancez un serveur et mettez USE_MILVUS=True\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparaison des Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualisation des latences\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Graphique 1: Latence moyenne par type d'index\n",
    "index_names = list(faiss_results.keys())\n",
    "avg_latencies = [faiss_results[idx]['avg_latency'] for idx in index_names]\n",
    "\n",
    "axes[0].bar(index_names, avg_latencies, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "axes[0].set_ylabel('Latence moyenne (ms)')\n",
    "axes[0].set_title('Latence Moyenne par Type d\\'Index FAISS')\n",
    "axes[0].set_xlabel('Type d\\'Index')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for i, v in enumerate(avg_latencies):\n",
    "    axes[0].text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Graphique 2: Distribution des latences\n",
    "for idx_type in index_names:\n",
    "    axes[1].hist(faiss_results[idx_type]['latencies'], bins=20, alpha=0.5, label=idx_type)\n",
    "\n",
    "axes[1].set_xlabel('Latence (ms)')\n",
    "axes[1].set_ylabel('Fr√©quence')\n",
    "axes[1].set_title('Distribution des Latences')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Graphiques de performance g√©n√©r√©s\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. √âvaluation du Recall\n",
    "\n",
    "Le **recall** mesure la proportion de r√©sultats pertinents retrouv√©s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def calculate_recall(results_fast, results_exact, k=10):\n",
    "    \"\"\"\n",
    "    Calculer le recall en comparant avec les r√©sultats exacts (Flat index)\n",
    "    \"\"\"\n",
    "    recalls = []\n",
    "    \n",
    "    for i in range(len(results_fast)):\n",
    "        fast_ids = set(results_fast[i])\n",
    "        exact_ids = set(results_exact[i])\n",
    "        \n",
    "        intersection = len(fast_ids.intersection(exact_ids))\n",
    "        recall = intersection / k\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    return np.mean(recalls)\n",
    "\n",
    "# Obtenir les r√©sultats de r√©f√©rence (Flat index)\n",
    "flat_index = faiss_indexes['flat']\n",
    "flat_results = []\n",
    "\n",
    "for query in query_embeddings:\n",
    "    distances, indices, _ = flat_index.search(query.reshape(1, -1), k=10)\n",
    "    flat_results.append(indices[0].tolist())\n",
    "\n",
    "# Calculer le recall pour chaque type d'index\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"√âVALUATION DU RECALL (vs Flat Index)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for idx_type in ['ivf', 'hnsw']:\n",
    "    index = faiss_indexes[idx_type]\n",
    "    results = []\n",
    "    \n",
    "    for query in query_embeddings:\n",
    "        distances, indices, _ = index.search(query.reshape(1, -1), k=10)\n",
    "        results.append(indices[0].tolist())\n",
    "    \n",
    "    recall = calculate_recall(results, flat_results, k=10)\n",
    "    print(f\"üìä {idx_type.upper()}: Recall@10 = {recall:.4f} ({recall*100:.2f}%)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tableau Comparatif\n",
    "\n",
    "### R√©sum√© des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cr√©er un tableau comparatif\n",
    "comparison_data = []\n",
    "\n",
    "for idx_type in index_names:\n",
    "    comparison_data.append({\n",
    "        \"Index Type\": idx_type.upper(),\n",
    "        \"Latence Moy. (ms)\": f\"{faiss_results[idx_type]['avg_latency']:.2f}\",\n",
    "        \"Latence P95 (ms)\": f\"{faiss_results[idx_type]['p95_latency']:.2f}\",\n",
    "        \"M√©moire\": \"RAM\" if idx_type == 'flat' else \"RAM (optimis√©)\",\n",
    "        \"Scalabilit√©\": \"Limit√©e\" if idx_type == 'flat' else \"Moyenne\" if idx_type == 'ivf' else \"Bonne\",\n",
    "        \"Qualit√©\": \"Exacte\" if idx_type == 'flat' else \"~95%\" if idx_type == 'ivf' else \"~98%\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TABLEAU COMPARATIF - FAISS\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMANDATIONS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "üéØ FLAT Index:\n",
    "   ‚úÖ Avantages: R√©sultats exacts, simple\n",
    "   ‚ùå Inconv√©nients: Lent sur gros volumes (>100K vecteurs)\n",
    "   üí° Usage: Prototypes, petits datasets\n",
    "\n",
    "üéØ IVF Index:\n",
    "   ‚úÖ Avantages: Bon compromis vitesse/qualit√©\n",
    "   ‚ùå Inconv√©nients: N√©cessite entra√Ænement\n",
    "   üí° Usage: Production (100K-10M vecteurs)\n",
    "\n",
    "üéØ HNSW Index:\n",
    "   ‚úÖ Avantages: Tr√®s rapide, excellent recall\n",
    "   ‚ùå Inconv√©nients: M√©moire suppl√©mentaire\n",
    "   üí° Usage: Applications temps r√©el\n",
    "\n",
    "üéØ Milvus:\n",
    "   ‚úÖ Avantages: Distribu√©, scalable, persistant\n",
    "   ‚ùå Inconv√©nients: Infrastructure complexe\n",
    "   üí° Usage: Production √† grande √©chelle (>10M vecteurs)\n",
    "\"\"\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Crit√®res de Choix pour la Production\n",
    "\n",
    "### Matrice de d√©cision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "decision_matrix = pd.DataFrame({\n",
    "    \"Crit√®re\": [\n",
    "        \"Volume de donn√©es\",\n",
    "        \"Latence requise\",\n",
    "        \"Qualit√© (recall)\",\n",
    "        \"Mises √† jour fr√©quentes\",\n",
    "        \"Distribution g√©ographique\",\n",
    "        \"Budget infrastructure\",\n",
    "        \"Comp√©tences √©quipe\"\n",
    "    ],\n",
    "    \"< 100K vecteurs\": [\n",
    "        \"FAISS Flat/HNSW\",\n",
    "        \"FAISS HNSW\",\n",
    "        \"FAISS Flat\",\n",
    "        \"FAISS (reload)\",\n",
    "        \"FAISS + CDN\",\n",
    "        \"FAISS\",\n",
    "        \"FAISS\"\n",
    "    ],\n",
    "    \"100K - 10M vecteurs\": [\n",
    "        \"FAISS IVF/HNSW\",\n",
    "        \"FAISS HNSW\",\n",
    "        \"FAISS IVF\",\n",
    "        \"Milvus\",\n",
    "        \"Milvus\",\n",
    "        \"FAISS ou Milvus\",\n",
    "        \"FAISS\"\n",
    "    ],\n",
    "    \"> 10M vecteurs\": [\n",
    "        \"Milvus/Pinecone\",\n",
    "        \"Milvus optimis√©\",\n",
    "        \"Milvus\",\n",
    "        \"Milvus\",\n",
    "        \"Milvus distributed\",\n",
    "        \"Milvus\",\n",
    "        \"Milvus (DevOps)\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"MATRICE DE D√âCISION\")\n",
    "print(\"=\"*100)\n",
    "print(decision_matrix.to_string(index=False))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Consid√©rations Production\n",
    "\n",
    "### 9.1 Persistance et Sauvegarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sauvegarder un index FAISS\n",
    "import pickle\n",
    "\n",
    "def save_faiss_index(index: FAISSIndex, filepath: str):\n",
    "    \"\"\"Sauvegarder l'index FAISS sur disque\"\"\"\n",
    "    # Sauvegarder l'index FAISS\n",
    "    faiss.write_index(index.index, f\"{filepath}.index\")\n",
    "    \n",
    "    # Sauvegarder les documents\n",
    "    with open(f\"{filepath}.docs.pkl\", 'wb') as f:\n",
    "        pickle.dump(index.documents, f)\n",
    "    \n",
    "    print(f\"‚úÖ Index sauvegard√©: {filepath}\")\n",
    "\n",
    "def load_faiss_index(filepath: str, dimension: int, index_type: str) -> FAISSIndex:\n",
    "    \"\"\"Charger un index FAISS depuis le disque\"\"\"\n",
    "    index_obj = FAISSIndex(dimension, index_type)\n",
    "    \n",
    "    # Charger l'index FAISS\n",
    "    index_obj.index = faiss.read_index(f\"{filepath}.index\")\n",
    "    \n",
    "    # Charger les documents\n",
    "    with open(f\"{filepath}.docs.pkl\", 'rb') as f:\n",
    "        index_obj.documents = pickle.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Index charg√©: {filepath}\")\n",
    "    return index_obj\n",
    "\n",
    "# Exemple de sauvegarde\n",
    "save_faiss_index(faiss_indexes['hnsw'], 'index_hnsw')\n",
    "\n",
    "# Exemple de chargement\n",
    "loaded_index = load_faiss_index('index_hnsw', embeddings.shape[1], 'hnsw')\n",
    "print(f\"Index charg√©: {loaded_index.get_stats()}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Monitoring et M√©triques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class VectorDBMonitor:\n",
    "    \"\"\"Classe pour monitorer les performances en production\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.queries = []\n",
    "        self.latencies = []\n",
    "        self.results_counts = []\n",
    "    \n",
    "    def log_query(self, query: str, latency: float, num_results: int):\n",
    "        self.queries.append(query)\n",
    "        self.latencies.append(latency)\n",
    "        self.results_counts.append(num_results)\n",
    "    \n",
    "    def get_metrics(self) -> Dict:\n",
    "        if not self.latencies:\n",
    "            return {}\n",
    "        \n",
    "        return {\n",
    "            \"total_queries\": len(self.queries),\n",
    "            \"avg_latency_ms\": np.mean(self.latencies),\n",
    "            \"p50_latency_ms\": np.percentile(self.latencies, 50),\n",
    "            \"p95_latency_ms\": np.percentile(self.latencies, 95),\n",
    "            \"p99_latency_ms\": np.percentile(self.latencies, 99),\n",
    "            \"max_latency_ms\": np.max(self.latencies),\n",
    "            \"avg_results\": np.mean(self.results_counts)\n",
    "        }\n",
    "    \n",
    "    def print_report(self):\n",
    "        metrics = self.get_metrics()\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RAPPORT DE MONITORING\")\n",
    "        print(\"=\"*60)\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"{key}: {value:.2f}\" if isinstance(value, float) else f\"{key}: {value}\")\n",
    "\n",
    "# Exemple d'utilisation\n",
    "monitor = VectorDBMonitor()\n",
    "\n",
    "# Simuler des requ√™tes\n",
    "for query in query_embeddings:\n",
    "    distances, indices, latency = faiss_indexes['hnsw'].search(query.reshape(1, -1), k=10)\n",
    "    monitor.log_query(\"test query\", latency, len(indices[0]))\n",
    "\n",
    "monitor.print_report()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion et Bonnes Pratiques\n",
    "\n",
    "### ‚úÖ Points cl√©s √† retenir\n",
    "\n",
    "1. **FAISS vs Milvus**:\n",
    "   - FAISS: Id√©al pour prototypes et datasets < 10M vecteurs\n",
    "   - Milvus: N√©cessaire pour scale et persistence en production\n",
    "\n",
    "2. **Trade-offs**:\n",
    "   - Latence vs Recall: IVF/HNSW offrent 95-98% recall avec 10-100x speedup\n",
    "   - M√©moire vs Performance: HNSW utilise plus de RAM mais est plus rapide\n",
    "\n",
    "3. **M√©triques importantes**:\n",
    "   - Latence P95/P99 (pas seulement moyenne)\n",
    "   - Recall@k pour mesurer la qualit√©\n",
    "   - Throughput (requ√™tes/seconde)\n",
    "\n",
    "4. **Production**:\n",
    "   - Monitoring continu des performances\n",
    "   - Sauvegarde r√©guli√®re des index\n",
    "   - Tests de charge avant d√©ploiement\n",
    "   - Consid√©rer les mises √† jour incr√©mentales\n",
    "\n",
    "### üìö Ressources\n",
    "\n",
    "- [FAISS Wiki](https://github.com/facebookresearch/faiss/wiki)\n",
    "- [Milvus Documentation](https://milvus.io/docs)\n",
    "- [Vector Database Comparison](https://benchmark.vectorview.ai/)\n",
    "- [HNSW Paper](https://arxiv.org/abs/1603.09320)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Exercices\n",
    "\n",
    "### Exercice 1: Optimiser les param√®tres\n",
    "1. Testez diff√©rentes valeurs de `nprobe` pour IVF (5, 10, 20, 50)\n",
    "2. Mesurez l'impact sur latence et recall\n",
    "3. Trouvez le meilleur compromis\n",
    "\n",
    "### Exercice 2: Scalabilit√©\n",
    "1. Cr√©ez un dataset de 100K vecteurs\n",
    "2. Comparez les temps d'indexation\n",
    "3. Mesurez la d√©gradation de performance\n",
    "\n",
    "### Exercice 3: Production-ready API\n",
    "1. Cr√©ez une API FastAPI avec un index FAISS\n",
    "2. Ajoutez du monitoring des latences\n",
    "3. Impl√©mentez un cache pour requ√™tes fr√©quentes\n",
    "\n",
    "### Exercice 4: Milvus deployment\n",
    "1. D√©ployez Milvus avec Docker Compose\n",
    "2. Indexez 50K vecteurs\n",
    "3. Comparez avec FAISS en termes de latence et features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
