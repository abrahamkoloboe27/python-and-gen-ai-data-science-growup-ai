{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† Exploration Compl√®te des LLMs : Param√®tres, Tokens et G√©n√©ration\n",
        "\n",
        "## üéØ Objectifs de ce notebook\n",
        "\n",
        "Bienvenue dans ce notebook p√©dagogique complet d√©di√© √† la compr√©hension approfondie des **Large Language Models (LLMs)**.\n",
        "\n",
        "### Ce que vous allez apprendre :\n",
        "\n",
        "1. üî§ **Comprendre la tokenization** : Comment les mod√®les d√©coupent et encodent le texte\n",
        "2. üìä **Visualiser les embeddings** : La repr√©sentation vectorielle des mots dans l'espace s√©mantique\n",
        "3. üëÅÔ∏è **Explorer l'attention** : Le m√©canisme cl√© des Transformers (Q/K/V, multi-head)\n",
        "4. ‚öôÔ∏è **Ma√Ætriser les param√®tres de g√©n√©ration** : temperature, top_k, top_p, etc.\n",
        "5. üî¨ **Exp√©rimenter concr√®tement** : Tester l'impact de chaque param√®tre\n",
        "6. üìà **Analyser quantitativement** : Perplexit√© et m√©triques de qualit√©\n",
        "7. ‚ö†Ô∏è **Comprendre les limites** : Hallucinations, biais, bonnes pratiques\n",
        "\n",
        "---\n",
        "\n",
        "## ü§ñ Qu'est-ce qu'un LLM ?\n",
        "\n",
        "Un **Large Language Model** est un mod√®le d'IA entra√Æn√© sur d'√©normes quantit√©s de texte pour :\n",
        "\n",
        "- **Pr√©dire le prochain token** (mot ou sous-mot) dans une s√©quence\n",
        "- **G√©n√©rer du texte** de mani√®re coh√©rente et contextuelle\n",
        "- **Comprendre le langage naturel** gr√¢ce aux repr√©sentations vectorielles\n",
        "\n",
        "### Architecture : Le Transformer\n",
        "\n",
        "Les LLMs modernes (GPT, BERT, etc.) reposent sur l'architecture **Transformer** qui utilise :\n",
        "\n",
        "- **Self-Attention** : Permet au mod√®le de pond√©rer l'importance de chaque mot par rapport aux autres\n",
        "- **Embeddings** : Transformation de tokens en vecteurs denses\n",
        "- **Couches empil√©es** : Apprentissage de repr√©sentations de plus en plus abstraites\n",
        "\n",
        "### Pipeline de g√©n√©ration :\n",
        "\n",
        "```\n",
        "Texte d'entr√©e ‚Üí Tokenization ‚Üí Embeddings ‚Üí Transformer (N layers)\n",
        "    ‚Üì\n",
        "Logits ‚Üí Softmax ‚Üí Sampling/D√©codage ‚Üí Texte g√©n√©r√©\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üéì Public cible\n",
        "\n",
        "Ce notebook s'adresse aux **professionnels techniques** :\n",
        "- Data Scientists\n",
        "- ML Engineers\n",
        "- D√©veloppeurs IA\n",
        "- Chercheurs en NLP\n",
        "\n",
        "**Pr√©requis** : Connaissances de base en Python, ML et NLP.\n",
        "\n",
        "---\n",
        "\n",
        "üí° **Note** : Ce notebook utilise `distilgpt2` pour des raisons de l√©g√®ret√© et de rapidit√© d'ex√©cution sur CPU. Les concepts s'appliquent √† tous les LLMs (GPT, LLaMA, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üì¶ 2. Installation et Imports\n",
        "\n",
        "## Installation des d√©pendances\n",
        "\n",
        "Nous utilisons les biblioth√®ques suivantes avec des **versions fix√©es** pour garantir la compatibilit√© :\n",
        "\n",
        "- **transformers** : HuggingFace pour charger les mod√®les pr√©-entra√Æn√©s\n",
        "- **torch** : Backend PyTorch pour les calculs\n",
        "- **plotly** : Visualisations interactives\n",
        "- **scikit-learn** : PCA et autres outils ML\n",
        "- **umap-learn** : R√©duction de dimensionnalit√© avanc√©e\n",
        "- **numpy** & **pandas** : Manipulation de donn√©es\n",
        "\n",
        "‚ö†Ô∏è **Important** : Les versions sont fix√©es pour √©viter les probl√®mes de compatibilit√©."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Installation des packages avec versions fix√©es\n",
        "# D√©commenter les lignes suivantes pour installer dans un environnement vide\n",
        "\n",
        "# !pip install transformers==4.36.2\n",
        "# !pip install torch==2.1.2\n",
        "# !pip install plotly==5.18.0\n",
        "# !pip install scikit-learn==1.3.2\n",
        "# !pip install umap-learn==0.5.5\n",
        "# !pip install numpy==1.24.3\n",
        "# !pip install pandas==2.0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports et configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports standard\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import random\n",
        "\n",
        "# Transformers & PyTorch\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    set_seed as transformers_set_seed\n",
        ")\n",
        "\n",
        "# Visualisation\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# R√©duction de dimensionnalit√©\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "try:\n",
        "    import umap\n",
        "    UMAP_AVAILABLE = True\n",
        "except ImportError:\n",
        "    UMAP_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è UMAP non disponible. Installez avec: pip install umap-learn==0.5.5\")\n",
        "\n",
        "# Configuration\n",
        "print(\"‚úÖ Imports r√©ussis!\")\n",
        "print(f\"üì± PyTorch version: {torch.__version__}\")\n",
        "print(f\"üîß Device disponible: {'GPU (CUDA)' if torch.cuda.is_available() else 'CPU'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# ü§ñ 3. Choix et Chargement du Mod√®le\n",
        "\n",
        "## Pourquoi `distilgpt2` ?\n",
        "\n",
        "Nous utilisons **DistilGPT-2** pour ce notebook car :\n",
        "\n",
        "### ‚úÖ Avantages\n",
        "- **L√©ger** : ~82M de param√®tres (vs 1.5B pour GPT-2 large)\n",
        "- **Rapide** : Fonctionne bien sur CPU\n",
        "- **Open-source** : Libre d'utilisation\n",
        "- **P√©dagogique** : M√™me architecture que GPT-2, mais plus accessible\n",
        "\n",
        "### üìä Compromis taille / performance\n",
        "\n",
        "| Mod√®le | Param√®tres | M√©moire RAM | CPU/GPU |\n",
        "|--------|------------|-------------|--------|\n",
        "| distilgpt2 | 82M | ~350 MB | ‚úÖ CPU OK |\n",
        "| gpt2 | 124M | ~550 MB | ‚úÖ CPU OK |\n",
        "| gpt2-medium | 355M | ~1.5 GB | ‚ö†Ô∏è CPU lent |\n",
        "| gpt2-large | 774M | ~3.2 GB | ‚ùå GPU recommand√© |\n",
        "\n",
        "### üîÑ Alternatives possibles\n",
        "\n",
        "Vous pouvez facilement remplacer par d'autres mod√®les :\n",
        "```python\n",
        "model_name = \"gpt2\"           # Mod√®le standard\n",
        "model_name = \"gpt2-medium\"    # Plus puissant\n",
        "model_name = \"EleutherAI/gpt-neo-125M\"  # Alternative\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration du mod√®le\n",
        "MODEL_NAME = \"distilgpt2\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"üîß Chargement du mod√®le: {MODEL_NAME}\")\n",
        "print(f\"üì± Device: {DEVICE}\")\n",
        "\n",
        "# Chargement du tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Important : GPT-2 n'a pas de pad_token par d√©faut\n",
        "# On utilise eos_token comme pad_token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"‚úÖ pad_token configur√© = eos_token\")\n",
        "\n",
        "# Chargement du mod√®le avec options pour l'analyse\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    output_attentions=True,      # Pour r√©cup√©rer les matrices d'attention\n",
        "    output_hidden_states=True,   # Pour r√©cup√©rer les embeddings interm√©diaires\n",
        ")\n",
        "\n",
        "# D√©placer le mod√®le sur le device appropri√©\n",
        "model = model.to(DEVICE)\n",
        "model.eval()  # Mode √©valuation (pas d'entra√Ænement)\n",
        "\n",
        "print(f\"\\n‚úÖ Mod√®le charg√© avec succ√®s!\")\n",
        "print(f\"üìä Nombre de param√®tres: {model.num_parameters():,}\")\n",
        "print(f\"üìè Taille du vocabulaire: {tokenizer.vocab_size:,}\")\n",
        "print(f\"üî¢ Nombre de couches: {model.config.n_layer}\")\n",
        "print(f\"üë• Nombre de t√™tes d'attention: {model.config.n_head}\")\n",
        "print(f\"üìê Dimension des embeddings: {model.config.n_embd}\")\n",
        "print(f\"üìè Context window: {model.config.n_positions} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üî§ 4. Tokens et Tokenization\n",
        "\n",
        "## Qu'est-ce qu'un token ?\n",
        "\n",
        "Un **token** est l'unit√© de base trait√©e par un LLM. Ce n'est **pas forc√©ment un mot** !\n",
        "\n",
        "### üìö D√©finition\n",
        "\n",
        "- Un token peut √™tre :\n",
        "  - Un mot entier : `\"hello\"` ‚Üí `[15339]`\n",
        "  - Un sous-mot : `\"tokenization\"` ‚Üí `[\"token\", \"ization\"]`\n",
        "  - Un caract√®re : espaces, ponctuation\n",
        "  - Un caract√®re sp√©cial : `\\n`, `\\t`\n",
        "\n",
        "### üéØ Pourquoi tokenizer ?\n",
        "\n",
        "1. **Vocabulaire fini** : Impossible d'avoir tous les mots du monde\n",
        "2. **Gestion de l'inconnu** : D√©coupe les mots rares en morceaux connus\n",
        "3. **Efficacit√©** : Repr√©sentation num√©rique optimale\n",
        "\n",
        "### üî¢ Tokenization BPE (Byte Pair Encoding)\n",
        "\n",
        "GPT-2 utilise **BPE** :\n",
        "- Apprend les paires de caract√®res les plus fr√©quentes\n",
        "- Fusionne it√©rativement pour cr√©er un vocabulaire\n",
        "- R√©sultat : ~50,000 tokens dans le vocabulaire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemples de tokenization\n",
        "exemples_textes = [\n",
        "    \"Bonjour, comment allez-vous ?\",\n",
        "    \"L'intelligence artificielle r√©volutionne le monde.\",\n",
        "    \"Tokenization est un concept fondamental.\",\n",
        "    \"2024 sera une ann√©e exceptionnelle!\",\n",
        "    \"Machine Learning & Deep Learning\"\n",
        "]\n",
        "\n",
        "print(\"üîç Exemples de tokenization:\\n\")\n",
        "\n",
        "for texte in exemples_textes:\n",
        "    # Tokenization\n",
        "    tokens = tokenizer.tokenize(texte)\n",
        "    token_ids = tokenizer.encode(texte, add_special_tokens=False)\n",
        "    \n",
        "    print(f\"üìù Texte: {texte}\")\n",
        "    print(f\"   üî§ Tokens: {tokens}\")\n",
        "    print(f\"   üî¢ IDs: {token_ids}\")\n",
        "    print(f\"   üìä Nombre de tokens: {len(tokens)}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí∞ Impact des tokens : Co√ªt et Context Window\n",
        "\n",
        "### Co√ªt\n",
        "- Les APIs LLM facturent **au nombre de tokens** (input + output)\n",
        "- Exemple OpenAI : ~$0.002 / 1K tokens (GPT-3.5)\n",
        "- ‚ö†Ô∏è Un texte peut contenir plus de tokens que de mots !\n",
        "\n",
        "### Context Window\n",
        "- **Limite du mod√®le** : Nombre maximum de tokens en entr√©e\n",
        "- distilgpt2 : 1024 tokens\n",
        "- GPT-3.5-turbo : 4096 tokens  \n",
        "- GPT-4 : 8192 tokens (ou 32K/128K pour versions √©tendues)\n",
        "- Claude : jusqu'√† 100K tokens\n",
        "\n",
        "### Visualisation des tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyse d√©taill√©e d'un texte\n",
        "texte_analyse = \"\"\"L'apprentissage automatique et l'intelligence artificielle transforment \n",
        "radicalement notre soci√©t√©. Les mod√®les de langage comme GPT r√©volutionnent \n",
        "la g√©n√©ration de texte et la compr√©hension du langage naturel.\"\"\"\n",
        "\n",
        "tokens = tokenizer.tokenize(texte_analyse)\n",
        "token_ids = tokenizer.encode(texte_analyse, add_special_tokens=False)\n",
        "\n",
        "# Cr√©er un DataFrame pour visualisation\n",
        "df_tokens = pd.DataFrame({\n",
        "    'Position': range(len(tokens)),\n",
        "    'Token': tokens,\n",
        "    'Token_ID': token_ids,\n",
        "    'Longueur': [len(t) for t in tokens]\n",
        "})\n",
        "\n",
        "print(f\"üìä Analyse d√©taill√©e:\\n\")\n",
        "print(f\"Texte original: {len(texte_analyse)} caract√®res\")\n",
        "print(f\"Nombre de mots: {len(texte_analyse.split())}\")\n",
        "print(f\"Nombre de tokens: {len(tokens)}\")\n",
        "print(f\"Ratio tokens/mots: {len(tokens)/len(texte_analyse.split()):.2f}\")\n",
        "\n",
        "print(f\"\\nüìã Premiers tokens:\\n\")\n",
        "print(df_tokens.head(15))\n",
        "\n",
        "# Visualisation de la distribution des longueurs\n",
        "fig = px.histogram(\n",
        "    df_tokens, \n",
        "    x='Longueur',\n",
        "    title=\"Distribution de la longueur des tokens\",\n",
        "    labels={'Longueur': 'Nombre de caract√®res', 'count': 'Fr√©quence'},\n",
        "    color_discrete_sequence=['#636EFA']\n",
        ")\n",
        "fig.update_layout(showlegend=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üìä 5. Embeddings : La Repr√©sentation Vectorielle\n",
        "\n",
        "## Qu'est-ce qu'un embedding ?\n",
        "\n",
        "Un **embedding** est une repr√©sentation **vectorielle dense** d'un token dans un espace de dimension √©lev√©e.\n",
        "\n",
        "### üéØ Transformation : Token ‚Üí Vecteur\n",
        "\n",
        "```\n",
        "Token \"chat\" (ID: 34534) ‚Üí Vecteur [0.23, -0.45, 0.87, ..., 0.12]\n",
        "                                      ‚Üë\n",
        "                                768 dimensions (pour distilgpt2)\n",
        "```\n",
        "\n",
        "### üîë Propri√©t√©s cl√©s\n",
        "\n",
        "1. **Similarit√© s√©mantique** : Mots similaires = vecteurs proches\n",
        "   - \"chat\" et \"chien\" auront des embeddings proches\n",
        "   - \"chat\" et \"ordinateur\" seront √©loign√©s\n",
        "\n",
        "2. **Contexte** : Dans les Transformers, les embeddings sont **contextuels**\n",
        "   - Le mot \"banque\" aura des repr√©sentations diff√©rentes selon le contexte\n",
        "\n",
        "3. **Apprentissage** : Les embeddings sont **appris** pendant l'entra√Ænement\n",
        "\n",
        "### üßÆ R√¥le dans les LLMs\n",
        "\n",
        "- **Couche d'entr√©e** : Transforme les token IDs en vecteurs\n",
        "- **Repr√©sentation continue** : Permet les calculs du Transformer\n",
        "- **Espace s√©mantique** : Capture les relations linguistiques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extraction des embeddings\n",
        "def get_embeddings(texte: str) -> Tuple[np.ndarray, List[str]]:\n",
        "    \"\"\"\n",
        "    Extrait les embeddings d'un texte.\n",
        "    \n",
        "    Returns:\n",
        "        embeddings: Array de shape (n_tokens, embedding_dim)\n",
        "        tokens: Liste des tokens correspondants\n",
        "    \"\"\"\n",
        "    # Tokenization\n",
        "    inputs = tokenizer(texte, return_tensors=\"pt\").to(DEVICE)\n",
        "    tokens = tokenizer.tokenize(texte)\n",
        "    \n",
        "    # Forward pass avec extraction des hidden states\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    \n",
        "    # Les hidden_states contiennent les embeddings de toutes les couches\n",
        "    # hidden_states[0] = embeddings de la couche d'entr√©e\n",
        "    # hidden_states[-1] = embeddings de la derni√®re couche\n",
        "    embeddings = outputs.hidden_states[-1].squeeze(0).cpu().numpy()\n",
        "    \n",
        "    return embeddings, tokens\n",
        "\n",
        "# Exemple d'extraction\n",
        "texte_exemple = \"Les mod√®les de langage sont puissants et fascinants\"\n",
        "embeddings, tokens = get_embeddings(texte_exemple)\n",
        "\n",
        "print(f\"üîç Analyse des embeddings:\\n\")\n",
        "print(f\"Texte: {texte_exemple}\")\n",
        "print(f\"Nombre de tokens: {len(tokens)}\")\n",
        "print(f\"Shape des embeddings: {embeddings.shape}\")\n",
        "print(f\"  ‚Üí {embeddings.shape[0]} tokens\")\n",
        "print(f\"  ‚Üí {embeddings.shape[1]} dimensions\")\n",
        "print(f\"\\nPremier embedding (premier token):\")\n",
        "print(f\"  Min: {embeddings[0].min():.3f}\")\n",
        "print(f\"  Max: {embeddings[0].max():.3f}\")\n",
        "print(f\"  Mean: {embeddings[0].mean():.3f}\")\n",
        "print(f\"  Std: {embeddings[0].std():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Visualisation des Embeddings\n",
        "\n",
        "Les embeddings sont en haute dimension (768D pour distilgpt2). Pour les visualiser, on utilise des techniques de **r√©duction de dimensionnalit√©** :\n",
        "\n",
        "### 1Ô∏è‚É£ PCA (Principal Component Analysis)\n",
        "- Lin√©aire, rapide\n",
        "- Pr√©serve la variance globale\n",
        "- Bon pour un aper√ßu rapide\n",
        "\n",
        "### 2Ô∏è‚É£ t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
        "- Non-lin√©aire\n",
        "- Pr√©serve les structures locales\n",
        "- Excellent pour la visualisation de clusters\n",
        "\n",
        "### 3Ô∏è‚É£ UMAP (Uniform Manifold Approximation and Projection)\n",
        "- Non-lin√©aire, plus rapide que t-SNE\n",
        "- Pr√©serve structure globale ET locale\n",
        "- √âtat de l'art pour la visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pr√©parer un texte plus long pour la visualisation\n",
        "texte_long = \"\"\"L'intelligence artificielle transforme notre monde. Les machines apprennent \n",
        "√† comprendre le langage naturel. Les r√©seaux de neurones profonds analysent les donn√©es. \n",
        "Les algorithmes optimisent les processus. La technologie √©volue rapidement. \n",
        "Les chercheurs d√©veloppent des mod√®les innovants. Les applications pratiques se multiplient.\n",
        "L'avenir de l'IA est prometteur et fascinant.\"\"\"\n",
        "\n",
        "# Extraire les embeddings\n",
        "embeddings_viz, tokens_viz = get_embeddings(texte_long)\n",
        "\n",
        "print(f\"üìä Donn√©es pour visualisation:\")\n",
        "print(f\"  Tokens: {len(tokens_viz)}\")\n",
        "print(f\"  Dimensions originales: {embeddings_viz.shape[1]}\")\n",
        "\n",
        "# R√©duction avec PCA\n",
        "pca = PCA(n_components=2)\n",
        "embeddings_pca = pca.fit_transform(embeddings_viz)\n",
        "print(f\"\\n‚úÖ PCA: Variance expliqu√©e: {pca.explained_variance_ratio_.sum():.1%}\")\n",
        "\n",
        "# R√©duction avec t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(tokens_viz)-1))\n",
        "embeddings_tsne = tsne.fit_transform(embeddings_viz)\n",
        "print(f\"‚úÖ t-SNE: R√©duction effectu√©e\")\n",
        "\n",
        "# R√©duction avec UMAP (si disponible)\n",
        "if UMAP_AVAILABLE and len(tokens_viz) > 15:\n",
        "    umap_reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=min(15, len(tokens_viz)-1))\n",
        "    embeddings_umap = umap_reducer.fit_transform(embeddings_viz)\n",
        "    print(f\"‚úÖ UMAP: R√©duction effectu√©e\")\n",
        "else:\n",
        "    embeddings_umap = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualisation interactive avec Plotly\n",
        "def plot_embeddings_comparison(embeddings_pca, embeddings_tsne, embeddings_umap, tokens):\n",
        "    \"\"\"\n",
        "    Cr√©e une visualisation comparative des trois m√©thodes de r√©duction.\n",
        "    \"\"\"\n",
        "    # D√©terminer le nombre de subplots\n",
        "    n_plots = 3 if embeddings_umap is not None else 2\n",
        "    \n",
        "    if n_plots == 3:\n",
        "        fig = make_subplots(\n",
        "            rows=1, cols=3,\n",
        "            subplot_titles=('PCA', 't-SNE', 'UMAP'),\n",
        "            horizontal_spacing=0.1\n",
        "        )\n",
        "    else:\n",
        "        fig = make_subplots(\n",
        "            rows=1, cols=2,\n",
        "            subplot_titles=('PCA', 't-SNE'),\n",
        "            horizontal_spacing=0.15\n",
        "        )\n",
        "    \n",
        "    # PCA\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=embeddings_pca[:, 0],\n",
        "            y=embeddings_pca[:, 1],\n",
        "            mode='markers+text',\n",
        "            text=tokens,\n",
        "            textposition='top center',\n",
        "            marker=dict(size=10, color=range(len(tokens)), colorscale='Viridis'),\n",
        "            name='PCA'\n",
        "        ),\n",
        "        row=1, col=1\n",
        "    )\n",
        "    \n",
        "    # t-SNE\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=embeddings_tsne[:, 0],\n",
        "            y=embeddings_tsne[:, 1],\n",
        "            mode='markers+text',\n",
        "            text=tokens,\n",
        "            textposition='top center',\n",
        "            marker=dict(size=10, color=range(len(tokens)), colorscale='Viridis'),\n",
        "            name='t-SNE'\n",
        "        ),\n",
        "        row=1, col=2\n",
        "    )\n",
        "    \n",
        "    # UMAP (si disponible)\n",
        "    if n_plots == 3:\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=embeddings_umap[:, 0],\n",
        "                y=embeddings_umap[:, 1],\n",
        "                mode='markers+text',\n",
        "                text=tokens,\n",
        "                textposition='top center',\n",
        "                marker=dict(size=10, color=range(len(tokens)), colorscale='Viridis'),\n",
        "                name='UMAP'\n",
        "            ),\n",
        "            row=1, col=3\n",
        "        )\n",
        "    \n",
        "    fig.update_layout(\n",
        "        title_text=\"Visualisation des Embeddings - Comparaison des M√©thodes\",\n",
        "        showlegend=False,\n",
        "        height=500,\n",
        "        width=1400 if n_plots == 3 else 1000\n",
        "    )\n",
        "    \n",
        "    return fig\n",
        "\n",
        "# Cr√©er et afficher la visualisation\n",
        "fig = plot_embeddings_comparison(embeddings_pca, embeddings_tsne, embeddings_umap, tokens_viz)\n",
        "fig.show()\n",
        "\n",
        "print(\"\\nüí° Interpr√©tation:\")\n",
        "print(\"- Les tokens s√©mantiquement similaires sont proches dans l'espace\")\n",
        "print(\"- Les couleurs repr√©sentent la position dans le texte\")\n",
        "print(\"- PCA: Vue lin√©aire globale\")\n",
        "print(\"- t-SNE: Emphase sur les clusters locaux\")\n",
        "if embeddings_umap is not None:\n",
        "    print(\"- UMAP: Meilleur √©quilibre structure globale/locale\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üëÅÔ∏è 6. Attention et Self-Attention\n",
        "\n",
        "## Le M√©canisme d'Attention : C≈ìur des Transformers\n",
        "\n",
        "L'**attention** est le m√©canisme qui permet au mod√®le de **pond√©rer l'importance** de chaque token par rapport aux autres lors du traitement.\n",
        "\n",
        "### üîë Les 3 Composantes : Q, K, V\n",
        "\n",
        "Pour chaque token, on calcule trois vecteurs :\n",
        "\n",
        "1. **Q (Query)** : \"Qu'est-ce que je cherche ?\"\n",
        "   - Repr√©sente l'information que le token souhaite obtenir\n",
        "\n",
        "2. **K (Key)** : \"Qu'est-ce que je peux offrir ?\"\n",
        "   - Repr√©sente le contenu du token\n",
        "\n",
        "3. **V (Value)** : \"Quelle information je porte ?\"\n",
        "   - Repr√©sente la valeur/information r√©elle du token\n",
        "\n",
        "### üìê Calcul de l'Attention\n",
        "\n",
        "```\n",
        "1. Similarit√© : Score = Q ¬∑ K·µÄ / ‚àöd_k\n",
        "2. Normalisation : Attention_weights = softmax(Score)\n",
        "3. Agr√©gation : Output = Attention_weights ¬∑ V\n",
        "```\n",
        "\n",
        "### üé≠ Multi-Head Attention\n",
        "\n",
        "Au lieu d'une seule attention, on calcule **plusieurs attentions en parall√®le** (\"t√™tes\") :\n",
        "\n",
        "- Chaque t√™te se sp√©cialise sur diff√©rents aspects (syntaxe, s√©mantique, etc.)\n",
        "- distilgpt2 : **12 t√™tes** d'attention\n",
        "- Les sorties sont concat√©n√©es et projet√©es\n",
        "\n",
        "### üîÑ Self-Attention\n",
        "\n",
        "Dans un LLM, c'est de la **self-attention** : chaque token attend aux autres tokens de la **m√™me s√©quence**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extraction des matrices d'attention\n",
        "def get_attention_weights(texte: str):\n",
        "    \"\"\"\n",
        "    Extrait les poids d'attention pour un texte.\n",
        "    \n",
        "    Returns:\n",
        "        attentions: Tuple de tenseurs (n_layers, batch, n_heads, seq_len, seq_len)\n",
        "        tokens: Liste des tokens\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(texte, return_tensors=\"pt\").to(DEVICE)\n",
        "    tokens = tokenizer.tokenize(texte)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    \n",
        "    # attentions est un tuple de tenseurs, un par couche\n",
        "    attentions = outputs.attentions\n",
        "    \n",
        "    return attentions, tokens\n",
        "\n",
        "# Exemple\n",
        "texte_attention = \"L'intelligence artificielle transforme le monde\"\n",
        "attentions, tokens_att = get_attention_weights(texte_attention)\n",
        "\n",
        "print(f\"üîç Analyse des matrices d'attention:\\n\")\n",
        "print(f\"Texte: {texte_attention}\")\n",
        "print(f\"Tokens: {tokens_att}\")\n",
        "print(f\"Nombre de couches: {len(attentions)}\")\n",
        "print(f\"Shape d'une matrice d'attention: {attentions[0].shape}\")\n",
        "print(f\"  ‚Üí Batch size: {attentions[0].shape[0]}\")\n",
        "print(f\"  ‚Üí Nombre de t√™tes: {attentions[0].shape[1]}\")\n",
        "print(f\"  ‚Üí Longueur s√©quence: {attentions[0].shape[2]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî• Visualisation des Heatmaps d'Attention\n",
        "\n",
        "Les **heatmaps** montrent quels tokens le mod√®le \"regarde\" pour comprendre chaque token.\n",
        "\n",
        "### Interpr√©tation\n",
        "- **Lignes** : Token qui \"attend\" (Query)\n",
        "- **Colonnes** : Tokens auxquels on pr√™te attention (Key)\n",
        "- **Couleur** : Intensit√© de l'attention (0 √† 1)\n",
        "- **Patterns typiques** :\n",
        "  - Diagonale : Auto-attention\n",
        "  - Vertical : Attention vers tokens sp√©cifiques (ex: ponctuation)\n",
        "  - Diffus : Attention distribu√©e sur tout le contexte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_attention_heatmap(attention_weights, tokens, layer_idx=0, head_idx=0):\n",
        "    \"\"\"\n",
        "    Visualise une heatmap d'attention pour une couche et t√™te sp√©cifiques.\n",
        "    \"\"\"\n",
        "    # Extraire la matrice pour la couche et t√™te sp√©cifi√©es\n",
        "    attention_matrix = attention_weights[layer_idx][0, head_idx].cpu().numpy()\n",
        "    \n",
        "    fig = go.Figure(data=go.Heatmap(\n",
        "        z=attention_matrix,\n",
        "        x=tokens,\n",
        "        y=tokens,\n",
        "        colorscale='Viridis',\n",
        "        text=np.round(attention_matrix, 2),\n",
        "        texttemplate='%{text}',\n",
        "        textfont={\"size\": 8},\n",
        "        colorbar=dict(title=\"Attention\")\n",
        "    ))\n",
        "    \n",
        "    fig.update_layout(\n",
        "        title=f\"Matrice d'Attention - Couche {layer_idx} - T√™te {head_idx}\",\n",
        "        xaxis_title=\"Tokens (Keys)\",\n",
        "        yaxis_title=\"Tokens (Queries)\",\n",
        "        width=800,\n",
        "        height=700\n",
        "    )\n",
        "    \n",
        "    return fig\n",
        "\n",
        "# Visualisation pour la premi√®re couche, premi√®re t√™te\n",
        "fig = plot_attention_heatmap(attentions, tokens_att, layer_idx=0, head_idx=0)\n",
        "fig.show()\n",
        "\n",
        "print(\"\\nüí° Que voir dans cette heatmap:\")\n",
        "print(\"- Chaque cellule (i,j) = Attention du token i vers le token j\")\n",
        "print(\"- Les valeurs sont normalis√©es (somme = 1 par ligne)\")\n",
        "print(\"- Zones claires = forte attention\")\n",
        "print(\"- Zones sombres = faible attention\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparaison de plusieurs t√™tes et couches\n",
        "def plot_multi_head_attention(attention_weights, tokens, layer_idx=0, n_heads=4):\n",
        "    \"\"\"\n",
        "    Visualise plusieurs t√™tes d'attention c√¥te √† c√¥te.\n",
        "    \"\"\"\n",
        "    n_heads = min(n_heads, attention_weights[layer_idx].shape[1])\n",
        "    \n",
        "    fig = make_subplots(\n",
        "        rows=1, cols=n_heads,\n",
        "        subplot_titles=[f'T√™te {i}' for i in range(n_heads)],\n",
        "        horizontal_spacing=0.05\n",
        "    )\n",
        "    \n",
        "    for head_idx in range(n_heads):\n",
        "        attention_matrix = attention_weights[layer_idx][0, head_idx].cpu().numpy()\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Heatmap(\n",
        "                z=attention_matrix,\n",
        "                x=tokens,\n",
        "                y=tokens if head_idx == 0 else [''] * len(tokens),\n",
        "                colorscale='Viridis',\n",
        "                showscale=(head_idx == n_heads - 1),\n",
        "                colorbar=dict(title=\"Attention\") if head_idx == n_heads - 1 else None\n",
        "            ),\n",
        "            row=1, col=head_idx + 1\n",
        "        )\n",
        "    \n",
        "    fig.update_layout(\n",
        "        title_text=f\"Multi-Head Attention - Couche {layer_idx}\",\n",
        "        height=500,\n",
        "        width=300 * n_heads\n",
        "    )\n",
        "    \n",
        "    return fig\n",
        "\n",
        "# Afficher 4 t√™tes de la premi√®re couche\n",
        "fig = plot_multi_head_attention(attentions, tokens_att, layer_idx=0, n_heads=4)\n",
        "fig.show()\n",
        "\n",
        "print(\"\\nüéØ Observation:\")\n",
        "print(\"- Chaque t√™te capture des patterns diff√©rents\")\n",
        "print(\"- Certaines t√™tes se concentrent sur la structure syntaxique\")\n",
        "print(\"- D'autres capturent les relations s√©mantiques\")\n",
        "print(\"- La diversit√© des t√™tes enrichit la repr√©sentation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# ‚öôÔ∏è 7. Param√®tres de G√©n√©ration\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Les **param√®tres de g√©n√©ration** contr√¥lent comment le mod√®le produit du texte. Ils ont un impact majeur sur :\n",
        "- La qualit√© du texte\n",
        "- La cr√©ativit√© vs d√©terminisme\n",
        "- La coh√©rence\n",
        "- La diversit√© des sorties\n",
        "\n",
        "Comprendre ces param√®tres est **essentiel** pour utiliser efficacement les LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî¢ 1. `max_new_tokens`\n",
        "\n",
        "**D√©finition** : Nombre maximum de tokens √† g√©n√©rer.\n",
        "\n",
        "### Comportement\n",
        "- Limite la longueur de la sortie\n",
        "- Le mod√®le peut s'arr√™ter avant si il g√©n√®re un token de fin (`<eos>`)\n",
        "\n",
        "### Cas d'usage\n",
        "- Contr√¥le de la longueur des r√©ponses\n",
        "- Gestion des co√ªts (APIs payantes)\n",
        "- Respect des limites du context window\n",
        "\n",
        "### Valeurs typiques\n",
        "- R√©sum√© court : 50-100\n",
        "- Paragraphe : 150-300\n",
        "- Article : 500-1000\n",
        "\n",
        "```python\n",
        "generate(prompt, max_new_tokens=50)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üå°Ô∏è 2. `temperature`\n",
        "\n",
        "**D√©finition** : Contr√¥le l'al√©atoire dans la s√©lection des tokens.\n",
        "\n",
        "### Comportement\n",
        "\n",
        "La temp√©rature modifie les probabilit√©s avant le sampling :\n",
        "\n",
        "```\n",
        "P'(token_i) = exp(logit_i / T) / Œ£ exp(logit_j / T)\n",
        "```\n",
        "\n",
        "- **T ‚Üí 0** : Distribution tr√®s \"peaky\" ‚Üí D√©terministe\n",
        "- **T = 1** : Distribution originale du mod√®le\n",
        "- **T > 1** : Distribution plus \"plate\" ‚Üí Plus al√©atoire\n",
        "\n",
        "### Impact pratique\n",
        "\n",
        "| Temperature | Effet | Cas d'usage |\n",
        "|-------------|-------|-------------|\n",
        "| 0.0 - 0.3 | Tr√®s d√©terministe, r√©p√©titif | T√¢ches factuelles, traductions |\n",
        "| 0.5 - 0.7 | √âquilibr√©, cr√©atif mais coh√©rent | Usage g√©n√©ral, chatbots |\n",
        "| 0.8 - 1.2 | Cr√©atif, diversifi√© | Brainstorming, fiction |\n",
        "| > 1.5 | Tr√®s al√©atoire, parfois incoh√©rent | G√©n√©ration artistique |\n",
        "\n",
        "### Exemple\n",
        "```python\n",
        "# D√©terministe\n",
        "generate(prompt, temperature=0.1)\n",
        "\n",
        "# Cr√©atif\n",
        "generate(prompt, temperature=1.0)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîù 3. `top_k`\n",
        "\n",
        "**D√©finition** : Ne consid√®re que les `k` tokens les plus probables √† chaque √©tape.\n",
        "\n",
        "### Comportement\n",
        "\n",
        "1. Trier les tokens par probabilit√© d√©croissante\n",
        "2. Garder seulement les `k` premiers\n",
        "3. Renormaliser les probabilit√©s\n",
        "4. Sampler parmi ces `k` tokens\n",
        "\n",
        "### Impact\n",
        "\n",
        "- **top_k faible (10-20)** : Conservatif, pr√©visible\n",
        "- **top_k moyen (40-50)** : √âquilibr√©\n",
        "- **top_k √©lev√© (100+)** : Plus de diversit√©\n",
        "\n",
        "### Exemple\n",
        "```python\n",
        "generate(prompt, do_sample=True, top_k=50)\n",
        "```\n",
        "\n",
        "‚ö†Ô∏è **Limitation** : top_k est fixe, peut inclure des tokens tr√®s improbables dans certains contextes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ 4. `top_p` (Nucleus Sampling)\n",
        "\n",
        "**D√©finition** : Sampling dans le \"noyau\" de probabilit√© cumul√©e `p`.\n",
        "\n",
        "### Comportement\n",
        "\n",
        "1. Trier les tokens par probabilit√© d√©croissante\n",
        "2. Calculer la probabilit√© cumul√©e\n",
        "3. Garder les tokens jusqu'√† atteindre `p` (ex: 0.9)\n",
        "4. Sampler parmi ces tokens\n",
        "\n",
        "### Avantage sur top_k\n",
        "\n",
        "- **Dynamique** : Adapte le nombre de tokens selon la distribution\n",
        "- Si le mod√®le est tr√®s s√ªr (1 token √† 95%) ‚Üí peu de tokens\n",
        "- Si le mod√®le h√©site ‚Üí plus de tokens\n",
        "\n",
        "### Valeurs recommand√©es\n",
        "\n",
        "| top_p | Effet |\n",
        "|-------|-------|\n",
        "| 0.1-0.5 | Tr√®s conservatif |\n",
        "| 0.7-0.9 | **Recommand√©** pour la plupart des cas |\n",
        "| 0.95+ | Maximum de diversit√© |\n",
        "\n",
        "### Exemple\n",
        "```python\n",
        "generate(prompt, do_sample=True, top_p=0.9)\n",
        "```\n",
        "\n",
        "üí° **Best practice** : Utiliser `top_p` plut√¥t que `top_k` dans la plupart des cas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé≤ 5. `do_sample`\n",
        "\n",
        "**D√©finition** : Active/d√©sactive le sampling probabiliste.\n",
        "\n",
        "### Comportement\n",
        "\n",
        "- **`do_sample=False`** (d√©faut) : **Greedy decoding**\n",
        "  - Toujours choisir le token le plus probable\n",
        "  - Compl√®tement d√©terministe\n",
        "  - Ignor√© `temperature`, `top_k`, `top_p`\n",
        "\n",
        "- **`do_sample=True`** : **Sampling**\n",
        "  - √âchantillonner selon les probabilit√©s\n",
        "  - Permet la diversit√©\n",
        "  - N√©cessaire pour utiliser temperature, top_k, top_p\n",
        "\n",
        "### Exemple\n",
        "```python\n",
        "# Greedy (d√©terministe)\n",
        "generate(prompt, do_sample=False)\n",
        "\n",
        "# Sampling (probabiliste)\n",
        "generate(prompt, do_sample=True, temperature=0.8)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ 6. `num_return_sequences`\n",
        "\n",
        "**D√©finition** : Nombre de s√©quences diff√©rentes √† g√©n√©rer.\n",
        "\n",
        "### Comportement\n",
        "\n",
        "- G√©n√®re plusieurs compl√©tions pour le m√™me prompt\n",
        "- **N√©cessite `do_sample=True`** pour avoir de la diversit√©\n",
        "\n",
        "### Cas d'usage\n",
        "\n",
        "- G√©n√©rer plusieurs options\n",
        "- S√©lectionner la meilleure\n",
        "- A/B testing\n",
        "\n",
        "### Exemple\n",
        "```python\n",
        "outputs = generate(\n",
        "    prompt, \n",
        "    do_sample=True,\n",
        "    num_return_sequences=3,\n",
        "    temperature=0.8\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üö´ 7. `repetition_penalty`\n",
        "\n",
        "**D√©finition** : P√©nalise les tokens d√©j√† g√©n√©r√©s pour √©viter les r√©p√©titions.\n",
        "\n",
        "### Comportement\n",
        "\n",
        "- **Valeur > 1.0** : P√©nalise les r√©p√©titions\n",
        "- **Valeur = 1.0** : Pas de p√©nalit√© (d√©faut)\n",
        "- **Valeur < 1.0** : Encourage les r√©p√©titions (rare)\n",
        "\n",
        "### Valeurs recommand√©es\n",
        "\n",
        "- 1.0 - 1.2 : L√©ger contr√¥le\n",
        "- 1.2 - 1.5 : Contr√¥le mod√©r√© (recommand√©)\n",
        "- 1.5+ : Fort contr√¥le (peut devenir artificiel)\n",
        "\n",
        "### Exemple\n",
        "```python\n",
        "generate(prompt, repetition_penalty=1.2)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ 8. `seed` (via transformers.set_seed)\n",
        "\n",
        "**D√©finition** : Graine al√©atoire pour la reproductibilit√©.\n",
        "\n",
        "### Comportement\n",
        "\n",
        "- Fixe les g√©n√©rateurs al√©atoires de PyTorch, numpy, etc.\n",
        "- Permet de reproduire exactement les m√™mes r√©sultats\n",
        "\n",
        "### Cas d'usage\n",
        "\n",
        "- Debug\n",
        "- Tests automatis√©s\n",
        "- Comparaisons √©quitables\n",
        "\n",
        "### Exemple\n",
        "```python\n",
        "transformers_set_seed(42)\n",
        "output = generate(prompt, do_sample=True)\n",
        "# M√™me r√©sultat √† chaque ex√©cution\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìè 9. Context Window\n",
        "\n",
        "**D√©finition** : Limite maximale du nombre de tokens (input + output).\n",
        "\n",
        "### Limites par mod√®le\n",
        "\n",
        "| Mod√®le | Context Window |\n",
        "|--------|----------------|\n",
        "| distilgpt2 | 1024 tokens |\n",
        "| GPT-2 | 1024 tokens |\n",
        "| GPT-3 | 4096 tokens |\n",
        "| GPT-3.5-turbo | 4096 tokens |\n",
        "| GPT-4 | 8192 / 32K / 128K |\n",
        "| Claude 2 | 100K tokens |\n",
        "\n",
        "### Gestion\n",
        "\n",
        "```python\n",
        "max_length = model.config.n_positions\n",
        "input_length = len(tokenizer.encode(prompt))\n",
        "max_new_tokens = max_length - input_length - safety_margin\n",
        "```\n",
        "\n",
        "‚ö†Ô∏è **Attention** : D√©passer la limite ‚Üí Erreur ou troncature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Tableau R√©capitulatif\n",
        "\n",
        "| Param√®tre | Valeur par d√©faut | Impact | Quand l'utiliser |\n",
        "|-----------|-------------------|--------|------------------|\n",
        "| `max_new_tokens` | 20 | Longueur de sortie | Toujours |\n",
        "| `temperature` | 1.0 | Cr√©ativit√© | Avec `do_sample=True` |\n",
        "| `top_k` | 50 | Filtrage des tokens | Alternative √† top_p |\n",
        "| `top_p` | 1.0 | Filtrage adaptatif | **Recommand√©** avec sampling |\n",
        "| `do_sample` | False | Active le sampling | Pour diversit√© |\n",
        "| `num_return_sequences` | 1 | Nombre de sorties | Pour options multiples |\n",
        "| `repetition_penalty` | 1.0 | Contr√¥le r√©p√©titions | Si texte r√©p√©titif |\n",
        "\n",
        "### üéØ Configurations Recommand√©es\n",
        "\n",
        "**1. T√¢ches factuelles / Traduction**\n",
        "```python\n",
        "do_sample=False  # Greedy\n",
        "# OU\n",
        "do_sample=True\n",
        "temperature=0.3\n",
        "top_p=0.9\n",
        "```\n",
        "\n",
        "**2. Usage g√©n√©ral / Chatbot**\n",
        "```python\n",
        "do_sample=True\n",
        "temperature=0.7\n",
        "top_p=0.9\n",
        "repetition_penalty=1.2\n",
        "```\n",
        "\n",
        "**3. Cr√©ativit√© / Fiction**\n",
        "```python\n",
        "do_sample=True\n",
        "temperature=1.0\n",
        "top_p=0.95\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üõ†Ô∏è 8. Fonctions Utilitaires\n",
        "\n",
        "Cr√©ons des fonctions r√©utilisables pour faciliter nos exp√©rimentations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seed(seed: int = 42):\n",
        "    \"\"\"\n",
        "    Configure la graine al√©atoire pour la reproductibilit√©.\n",
        "    \n",
        "    Args:\n",
        "        seed: Valeur de la graine (d√©faut: 42)\n",
        "    \"\"\"\n",
        "    transformers_set_seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    print(f\"‚úÖ Seed fix√© √† {seed}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text(\n",
        "    prompt: str,\n",
        "    max_new_tokens: int = 50,\n",
        "    temperature: float = 1.0,\n",
        "    top_k: int = 50,\n",
        "    top_p: float = 0.9,\n",
        "    do_sample: bool = True,\n",
        "    num_return_sequences: int = 1,\n",
        "    repetition_penalty: float = 1.0,\n",
        "    seed: Optional[int] = None\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    G√©n√®re du texte avec le mod√®le charg√©.\n",
        "    \n",
        "    Args:\n",
        "        prompt: Texte d'entr√©e\n",
        "        max_new_tokens: Nombre maximum de tokens √† g√©n√©rer\n",
        "        temperature: Contr√¥le la cr√©ativit√© (0.0-2.0)\n",
        "        top_k: Filtre top-k tokens\n",
        "        top_p: Nucleus sampling (0.0-1.0)\n",
        "        do_sample: Active le sampling probabiliste\n",
        "        num_return_sequences: Nombre de s√©quences √† g√©n√©rer\n",
        "        repetition_penalty: P√©nalit√© pour les r√©p√©titions\n",
        "        seed: Graine al√©atoire (optionnel)\n",
        "    \n",
        "    Returns:\n",
        "        Liste des textes g√©n√©r√©s\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        set_seed(seed)\n",
        "    \n",
        "    # Encoder le prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "    \n",
        "    # G√©n√©rer\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature if do_sample else 1.0,\n",
        "            top_k=top_k if do_sample else None,\n",
        "            top_p=top_p if do_sample else None,\n",
        "            do_sample=do_sample,\n",
        "            num_return_sequences=num_return_sequences,\n",
        "            repetition_penalty=repetition_penalty,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    \n",
        "    # D√©coder\n",
        "    generated_texts = [\n",
        "        tokenizer.decode(output, skip_special_tokens=True)\n",
        "        for output in outputs\n",
        "    ]\n",
        "    \n",
        "    return generated_texts\n",
        "\n",
        "# Test de la fonction\n",
        "set_seed(42)\n",
        "test_prompt = \"L'intelligence artificielle est\"\n",
        "result = generate_text(test_prompt, max_new_tokens=20, temperature=0.7)\n",
        "print(f\"\\nüìù Prompt: {test_prompt}\")\n",
        "print(f\"ü§ñ G√©n√©ration: {result[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_next_token_probs(\n",
        "    prompt: str,\n",
        "    top_n: int = 10\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calcule les probabilit√©s des prochains tokens.\n",
        "    \n",
        "    Args:\n",
        "        prompt: Texte d'entr√©e\n",
        "        top_n: Nombre de top tokens √† retourner\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame avec tokens et probabilit√©s\n",
        "    \"\"\"\n",
        "    # Encoder\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "    \n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    \n",
        "    # Logits du dernier token\n",
        "    logits = outputs.logits[0, -1, :]\n",
        "    \n",
        "    # Softmax pour obtenir les probabilit√©s\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    \n",
        "    # Top-N\n",
        "    top_probs, top_indices = torch.topk(probs, top_n)\n",
        "    \n",
        "    # Cr√©er DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'Token': [tokenizer.decode([idx]) for idx in top_indices.cpu().numpy()],\n",
        "        'Token_ID': top_indices.cpu().numpy(),\n",
        "        'Probabilit√©': top_probs.cpu().numpy(),\n",
        "        'Probabilit√©_%': (top_probs.cpu().numpy() * 100)\n",
        "    })\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Test\n",
        "test_prompt = \"Paris est la capitale de la\"\n",
        "probs_df = get_next_token_probs(test_prompt, top_n=10)\n",
        "print(f\"\\nüìä Top 10 tokens pour: '{test_prompt}'\\n\")\n",
        "print(probs_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üî¨ 9. Exp√©rimentations Guid√©es\n",
        "\n",
        "Testons concr√®tement l'impact des param√®tres de g√©n√©ration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exp√©rience 1 : Impact de la Temperature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prompt commun\n",
        "prompt_exp = \"L'avenir de l'intelligence artificielle sera\"\n",
        "\n",
        "# Tester diff√©rentes temp√©ratures\n",
        "temperatures = [0.0, 0.5, 0.7, 1.0, 1.5]\n",
        "results_temp = {}\n",
        "\n",
        "print(f\"üî¨ Exp√©rience: Impact de la temp√©rature\\n\")\n",
        "print(f\"üìù Prompt: '{prompt_exp}'\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for temp in temperatures:\n",
        "    # G√©n√©rer 3 s√©quences pour voir la diversit√©\n",
        "    outputs = generate_text(\n",
        "        prompt_exp,\n",
        "        max_new_tokens=30,\n",
        "        temperature=temp if temp > 0 else 1.0,  # √âviter temp=0 exact\n",
        "        do_sample=(temp > 0.01),\n",
        "        num_return_sequences=3 if temp > 0.01 else 1,\n",
        "        seed=42\n",
        "    )\n",
        "    \n",
        "    results_temp[temp] = outputs\n",
        "    \n",
        "    print(f\"\\nüå°Ô∏è  Temperature = {temp}\")\n",
        "    print(\"-\" * 80)\n",
        "    for i, output in enumerate(outputs, 1):\n",
        "        print(f\"   {i}. {output}\")\n",
        "    \n",
        "    # Mesure de diversit√© : nombre de sorties uniques\n",
        "    unique_outputs = len(set(outputs))\n",
        "    print(f\"   üìä Sorties uniques: {unique_outputs}/{len(outputs)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\\nüí° Observations:\")\n",
        "print(\"- Temperature ‚âà 0 : Tr√®s d√©terministe, toujours la m√™me sortie\")\n",
        "print(\"- Temperature = 0.7 : Bon √©quilibre entre coh√©rence et diversit√©\")\n",
        "print(\"- Temperature √©lev√©e : Plus de vari√©t√©, parfois moins coh√©rent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exp√©rience 2 : Visualisation des Probabilit√©s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyser les probabilit√©s avec diff√©rentes temp√©ratures\n",
        "prompt_prob = \"Le capital de France est\"\n",
        "\n",
        "# Obtenir les probabilit√©s de base\n",
        "df_probs = get_next_token_probs(prompt_prob, top_n=15)\n",
        "\n",
        "# Cr√©er un bar chart\n",
        "fig = px.bar(\n",
        "    df_probs,\n",
        "    x='Token',\n",
        "    y='Probabilit√©_%',\n",
        "    title=f\"Distribution des probabilit√©s pour: '{prompt_prob}'\",\n",
        "    labels={'Probabilit√©_%': 'Probabilit√© (%)'},\n",
        "    color='Probabilit√©_%',\n",
        "    color_continuous_scale='Viridis'\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_tickangle=-45,\n",
        "    height=500,\n",
        "    showlegend=False\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "print(\"\\nüìä Analyse:\")\n",
        "print(f\"- Token le plus probable: '{df_probs.iloc[0]['Token']}' ({df_probs.iloc[0]['Probabilit√©_%']:.2f}%)\")\n",
        "print(f\"- Entropie: {'Faible' if df_probs.iloc[0]['Probabilit√©_%'] > 50 else '√âlev√©e'}\")\n",
        "print(f\"- Le mod√®le est {'tr√®s confiant' if df_probs.iloc[0]['Probabilit√©_%'] > 70 else 'incertain'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exp√©rience 3 : Comparaison top_k vs top_p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_exp3 = \"Dans le futur, les robots pourront\"\n",
        "\n",
        "print(f\"üî¨ Exp√©rience: top_k vs top_p\\n\")\n",
        "print(f\"üìù Prompt: '{prompt_exp3}'\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Test avec top_k\n",
        "print(\"\\nüîù Avec top_k=10\")\n",
        "outputs_topk = generate_text(\n",
        "    prompt_exp3,\n",
        "    max_new_tokens=25,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    top_p=1.0,  # D√©sactiver top_p\n",
        "    temperature=0.8,\n",
        "    num_return_sequences=3,\n",
        "    seed=42\n",
        ")\n",
        "for i, output in enumerate(outputs_topk, 1):\n",
        "    print(f\"   {i}. {output}\")\n",
        "\n",
        "# Test avec top_p\n",
        "print(\"\\nüéØ Avec top_p=0.9\")\n",
        "outputs_topp = generate_text(\n",
        "    prompt_exp3,\n",
        "    max_new_tokens=25,\n",
        "    do_sample=True,\n",
        "    top_k=0,  # D√©sactiver top_k (mettre √† 0 ou tr√®s grand)\n",
        "    top_p=0.9,\n",
        "    temperature=0.8,\n",
        "    num_return_sequences=3,\n",
        "    seed=42\n",
        ")\n",
        "for i, output in enumerate(outputs_topp, 1):\n",
        "    print(f\"   {i}. {output}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\\nüí° Diff√©rence:\")\n",
        "print(\"- top_k : Fixe, peut √™tre trop restrictif ou trop permissif\")\n",
        "print(\"- top_p : Adaptif, s'ajuste selon la confiance du mod√®le\")\n",
        "print(\"- top_p est g√©n√©ralement pr√©f√©r√© pour un meilleur √©quilibre\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exp√©rience 4 : Effet de repetition_penalty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_exp4 = \"Les avantages de l'IA sont nombreux. Premi√®rement\"\n",
        "\n",
        "print(f\"üî¨ Exp√©rience: Repetition Penalty\\n\")\n",
        "print(f\"üìù Prompt: '{prompt_exp4}'\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Sans p√©nalit√©\n",
        "print(\"\\nüìù Sans p√©nalit√© (1.0)\")\n",
        "output_no_penalty = generate_text(\n",
        "    prompt_exp4,\n",
        "    max_new_tokens=40,\n",
        "    do_sample=True,\n",
        "    temperature=0.8,\n",
        "    repetition_penalty=1.0,\n",
        "    seed=42\n",
        ")[0]\n",
        "print(f\"   {output_no_penalty}\")\n",
        "\n",
        "# Avec p√©nalit√© mod√©r√©e\n",
        "print(\"\\nüö´ Avec p√©nalit√© mod√©r√©e (1.2)\")\n",
        "output_penalty = generate_text(\n",
        "    prompt_exp4,\n",
        "    max_new_tokens=40,\n",
        "    do_sample=True,\n",
        "    temperature=0.8,\n",
        "    repetition_penalty=1.2,\n",
        "    seed=42\n",
        ")[0]\n",
        "print(f\"   {output_penalty}\")\n",
        "\n",
        "# Avec forte p√©nalit√©\n",
        "print(\"\\n‚õî Avec forte p√©nalit√© (1.5)\")\n",
        "output_high_penalty = generate_text(\n",
        "    prompt_exp4,\n",
        "    max_new_tokens=40,\n",
        "    do_sample=True,\n",
        "    temperature=0.8,\n",
        "    repetition_penalty=1.5,\n",
        "    seed=42\n",
        ")[0]\n",
        "print(f\"   {output_high_penalty}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\\nüí° Impact:\")\n",
        "print(\"- P√©nalit√© faible : Peut r√©p√©ter des mots/phrases\")\n",
        "print(\"- P√©nalit√© mod√©r√©e : Bon √©quilibre, texte naturel\")\n",
        "print(\"- P√©nalit√© forte : √âvite les r√©p√©titions mais peut √™tre artificiel\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üîÑ 10. Pipeline Complet d'une Requ√™te\n",
        "\n",
        "Illustrons le **parcours complet** d'un texte dans un LLM, √©tape par √©tape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Texte d'exemple\n",
        "user_text = \"L'intelligence artificielle transforme\"\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üîÑ PIPELINE COMPLET : TEXTE ‚Üí G√âN√âRATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# √âTAPE 1: Texte d'entr√©e\n",
        "print(f\"\\n1Ô∏è‚É£  TEXTE UTILISATEUR\")\n",
        "print(f\"   üìù Input: '{user_text}'\")\n",
        "print(f\"   üìè Longueur: {len(user_text)} caract√®res\")\n",
        "\n",
        "# √âTAPE 2: Tokenization\n",
        "print(f\"\\n2Ô∏è‚É£  TOKENIZATION\")\n",
        "tokens = tokenizer.tokenize(user_text)\n",
        "token_ids = tokenizer.encode(user_text, return_tensors=\"pt\").to(DEVICE)\n",
        "print(f\"   üî§ Tokens: {tokens}\")\n",
        "print(f\"   üî¢ IDs: {token_ids.cpu().numpy()[0]}\")\n",
        "print(f\"   üìä Nombre de tokens: {len(tokens)}\")\n",
        "\n",
        "# √âTAPE 3: Embeddings\n",
        "print(f\"\\n3Ô∏è‚É£  EMBEDDINGS\")\n",
        "with torch.no_grad():\n",
        "    outputs_full = model(token_ids, output_hidden_states=True)\n",
        "embeddings_input = outputs_full.hidden_states[0].squeeze().cpu().numpy()\n",
        "print(f\"   üìê Shape: {embeddings_input.shape}\")\n",
        "print(f\"   ‚Üí {embeddings_input.shape[0]} tokens\")\n",
        "print(f\"   ‚Üí {embeddings_input.shape[1]} dimensions par token\")\n",
        "\n",
        "# √âTAPE 4: Transformer (N couches)\n",
        "print(f\"\\n4Ô∏è‚É£  TRANSFORMER ({model.config.n_layer} couches)\")\n",
        "print(f\"   üîÑ Passage √† travers {model.config.n_layer} couches\")\n",
        "print(f\"   üëÅÔ∏è  Chaque couche avec {model.config.n_head} t√™tes d'attention\")\n",
        "print(f\"   üìä Hidden states √† chaque couche\")\n",
        "\n",
        "# √âTAPE 5: Logits\n",
        "print(f\"\\n5Ô∏è‚É£  LOGITS (scores bruts)\")\n",
        "logits = outputs_full.logits[0, -1, :]  # Logits du dernier token\n",
        "print(f\"   üìè Shape: {logits.shape}\")\n",
        "print(f\"   ‚Üí Un score pour chaque token du vocabulaire ({tokenizer.vocab_size:,})\")\n",
        "print(f\"   üìà Min: {logits.min():.2f}, Max: {logits.max():.2f}, Mean: {logits.mean():.2f}\")\n",
        "\n",
        "# √âTAPE 6: Softmax ‚Üí Probabilit√©s\n",
        "print(f\"\\n6Ô∏è‚É£  SOFTMAX ‚Üí PROBABILIT√âS\")\n",
        "probs = torch.softmax(logits, dim=-1)\n",
        "print(f\"   üìä Somme des probabilit√©s: {probs.sum():.6f} (‚âà 1.0)\")\n",
        "top5_probs, top5_indices = torch.topk(probs, 5)\n",
        "print(f\"   üèÜ Top 5 tokens:\")\n",
        "for i, (prob, idx) in enumerate(zip(top5_probs, top5_indices), 1):\n",
        "    token_str = tokenizer.decode([idx.item()])\n",
        "    print(f\"      {i}. '{token_str}' : {prob.item()*100:.2f}%\")\n",
        "\n",
        "# √âTAPE 7: Sampling/D√©codage\n",
        "print(f\"\\n7Ô∏è‚É£  SAMPLING / D√âCODAGE\")\n",
        "print(f\"   üé≤ S√©lection du prochain token selon les param√®tres\")\n",
        "print(f\"   üîÑ R√©p√©tition du processus pour chaque nouveau token\")\n",
        "\n",
        "# √âTAPE 8: G√©n√©ration compl√®te\n",
        "print(f\"\\n8Ô∏è‚É£  TEXTE G√âN√âR√â\")\n",
        "generated = generate_text(\n",
        "    user_text,\n",
        "    max_new_tokens=15,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    seed=42\n",
        ")[0]\n",
        "print(f\"   ‚ú® R√©sultat: '{generated}'\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\\nüí° Points cl√©s:\")\n",
        "print(\"- Chaque token g√©n√©r√© devient input pour le suivant\")\n",
        "print(\"- Le processus est auto-r√©gressif (un token √† la fois)\")\n",
        "print(\"- Les param√®tres affectent l'√©tape de sampling (√©tape 7)\")\n",
        "print(\"- Le mod√®le ne 'voit' que les tokens pr√©c√©dents (causal)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üìà 11. Analyse Quantitative : Perplexit√©\n",
        "\n",
        "## Qu'est-ce que la Perplexit√© ?\n",
        "\n",
        "La **perplexit√©** est une m√©trique qui mesure la **qualit√© d'un mod√®le de langage**.\n",
        "\n",
        "### D√©finition math√©matique\n",
        "\n",
        "```\n",
        "Perplexit√© = exp(- (1/N) * Œ£ log P(token_i | contexte))\n",
        "```\n",
        "\n",
        "O√π :\n",
        "- N = nombre de tokens\n",
        "- P(token_i | contexte) = probabilit√© du token i donn√© le contexte\n",
        "\n",
        "### Interpr√©tation\n",
        "\n",
        "- **Perplexit√© faible** : Le mod√®le est confiant, pr√©dit bien\n",
        "- **Perplexit√© √©lev√©e** : Le mod√®le est incertain\n",
        "\n",
        "üí° **Intuition** : La perplexit√© repr√©sente le nombre moyen de choix √©quiprobables √† chaque √©tape.\n",
        "\n",
        "### Valeurs typiques\n",
        "\n",
        "- GPT-2 (1.5B) : ~20-30 sur des textes standards\n",
        "- Texte al√©atoire : ~50,000 (taille du vocabulaire)\n",
        "- Texte tr√®s pr√©visible : ~5-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_perplexity(text: str) -> float:\n",
        "    \"\"\"\n",
        "    Calcule la perplexit√© approximative du mod√®le sur un texte.\n",
        "    \n",
        "    Args:\n",
        "        text: Texte √† √©valuer\n",
        "    \n",
        "    Returns:\n",
        "        Perplexit√© (float)\n",
        "    \"\"\"\n",
        "    # Encoder le texte\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
        "    input_ids = inputs['input_ids']\n",
        "    \n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=input_ids)\n",
        "    \n",
        "    # La loss est d√©j√† la cross-entropy moyenne\n",
        "    loss = outputs.loss\n",
        "    \n",
        "    # Perplexit√© = exp(loss)\n",
        "    perplexity = torch.exp(loss).item()\n",
        "    \n",
        "    return perplexity\n",
        "\n",
        "# Tests sur diff√©rents textes\n",
        "textes_test = [\n",
        "    (\"Paris est la capitale de la France.\", \"Texte factuel simple\"),\n",
        "    (\"L'intelligence artificielle transforme radicalement notre soci√©t√© moderne.\", \"Texte coh√©rent\"),\n",
        "    (\"xkzp qwerty asdfgh zxcvbn uiop jklm\", \"Texte al√©atoire\"),\n",
        "    (\"Le le le le le le le le le le\", \"R√©p√©titions\"),\n",
        "]\n",
        "\n",
        "print(\"üìä Calcul de Perplexit√© sur diff√©rents textes\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results_perplexity = []\n",
        "\n",
        "for text, description in textes_test:\n",
        "    perplexity = calculate_perplexity(text)\n",
        "    results_perplexity.append({\n",
        "        'Description': description,\n",
        "        'Texte': text[:50] + '...' if len(text) > 50 else text,\n",
        "        'Perplexit√©': perplexity\n",
        "    })\n",
        "    print(f\"\\nüìù {description}\")\n",
        "    print(f\"   Texte: '{text}'\")\n",
        "    print(f\"   üìä Perplexit√©: {perplexity:.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Cr√©er un DataFrame et visualiser\n",
        "df_perplexity = pd.DataFrame(results_perplexity)\n",
        "\n",
        "fig = px.bar(\n",
        "    df_perplexity,\n",
        "    x='Description',\n",
        "    y='Perplexit√©',\n",
        "    title=\"Comparaison des Perplexit√©s\",\n",
        "    labels={'Perplexit√©': 'Perplexit√©', 'Description': 'Type de texte'},\n",
        "    color='Perplexit√©',\n",
        "    color_continuous_scale='RdYlGn_r'\n",
        ")\n",
        "\n",
        "fig.update_layout(showlegend=False, height=500)\n",
        "fig.show()\n",
        "\n",
        "print(\"\\nüí° Interpr√©tation:\")\n",
        "print(\"- Texte naturel et coh√©rent ‚Üí Perplexit√© faible\")\n",
        "print(\"- Texte al√©atoire ‚Üí Perplexit√© tr√®s √©lev√©e\")\n",
        "print(\"- R√©p√©titions ‚Üí Perplexit√© faible (trop pr√©visible)\")\n",
        "print(\"\\nüéØ La perplexit√© mesure la 'surprise' du mod√®le face au texte\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# ‚ö†Ô∏è 12. Bonnes Pratiques et Limites\n",
        "\n",
        "## üö® Limites des LLMs\n",
        "\n",
        "### 1. Hallucinations\n",
        "\n",
        "Les LLMs peuvent **inventer des informations** qui semblent plausibles mais sont fausses.\n",
        "\n",
        "**Causes** :\n",
        "- Le mod√®le g√©n√®re du texte plausible, pas n√©cessairement vrai\n",
        "- Donn√©es d'entra√Ænement incompl√®tes ou erron√©es\n",
        "- Manque de connaissance du monde r√©el\n",
        "\n",
        "**Mitigation** :\n",
        "- ‚úÖ V√©rifier les faits importants\n",
        "- ‚úÖ Utiliser RAG (Retrieval Augmented Generation)\n",
        "- ‚úÖ Demander des sources\n",
        "- ‚úÖ Temperature plus basse pour t√¢ches factuelles\n",
        "\n",
        "### 2. Biais\n",
        "\n",
        "Les LLMs refl√®tent les **biais pr√©sents dans leurs donn√©es d'entra√Ænement**.\n",
        "\n",
        "**Types de biais** :\n",
        "- Biais sociaux (genre, race, culture)\n",
        "- Biais temporels (donn√©es anciennes)\n",
        "- Biais de repr√©sentation (sur/sous-repr√©sentation)\n",
        "\n",
        "**Mitigation** :\n",
        "- ‚úÖ √ätre conscient des biais\n",
        "- ‚úÖ Tester sur des cas divers\n",
        "- ‚úÖ Utiliser des garde-fous\n",
        "- ‚úÖ Mod√©ration humaine\n",
        "\n",
        "### 3. Context Window Limit√©\n",
        "\n",
        "**Probl√®me** : Le mod√®le ne peut traiter qu'un nombre limit√© de tokens.\n",
        "\n",
        "- distilgpt2 : 1024 tokens (~750 mots)\n",
        "- GPT-4 : 8K-128K tokens\n",
        "\n",
        "**Solutions** :\n",
        "- ‚úÖ D√©coupage intelligent du texte\n",
        "- ‚úÖ R√©sum√©s progressifs\n",
        "- ‚úÖ Embeddings + recherche s√©mantique\n",
        "\n",
        "### 4. Pas de Connaissance du Monde R√©el\n",
        "\n",
        "- ‚ùå Pas d'acc√®s √† Internet\n",
        "- ‚ùå Pas de mise √† jour en temps r√©el\n",
        "- ‚ùå Pas de v√©rification des faits\n",
        "\n",
        "**Solutions** :\n",
        "- ‚úÖ RAG avec base de connaissances\n",
        "- ‚úÖ Int√©gration d'APIs externes\n",
        "- ‚úÖ Fine-tuning sur donn√©es r√©centes\n",
        "\n",
        "### 5. Co√ªt Computationnel\n",
        "\n",
        "**Ressources n√©cessaires** :\n",
        "- M√©moire GPU importante\n",
        "- Latence (temps de r√©ponse)\n",
        "- Co√ªt mon√©taire (APIs)\n",
        "\n",
        "**Optimisations** :\n",
        "- ‚úÖ Utiliser des mod√®les plus petits quand possible\n",
        "- ‚úÖ Quantization (r√©duction de pr√©cision)\n",
        "- ‚úÖ Batching des requ√™tes\n",
        "- ‚úÖ Caching des r√©sultats\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Bonnes Pratiques\n",
        "\n",
        "### 1. Prompt Engineering\n",
        "\n",
        "```python\n",
        "# ‚ùå Mauvais prompt\n",
        "\"Explique l'IA\"\n",
        "\n",
        "# ‚úÖ Bon prompt\n",
        "\"\"\"Explique l'intelligence artificielle en 3 paragraphes,\n",
        "en utilisant des analogies simples pour un public non-technique.\n",
        "Structure: 1) D√©finition, 2) Applications, 3) D√©fis.\"\"\"\n",
        "```\n",
        "\n",
        "### 2. Choix des Param√®tres\n",
        "\n",
        "**T√¢ches factuelles** :\n",
        "```python\n",
        "temperature=0.3\n",
        "top_p=0.9\n",
        "do_sample=True\n",
        "```\n",
        "\n",
        "**T√¢ches cr√©atives** :\n",
        "```python\n",
        "temperature=0.9\n",
        "top_p=0.95\n",
        "do_sample=True\n",
        "```\n",
        "\n",
        "### 3. Validation et Tests\n",
        "\n",
        "- ‚úÖ Tester sur des cas limites\n",
        "- ‚úÖ Mesurer la qualit√© (perplexit√©, coh√©rence)\n",
        "- ‚úÖ √âvaluation humaine\n",
        "- ‚úÖ Monitoring en production\n",
        "\n",
        "### 4. S√©curit√© des Prompts\n",
        "\n",
        "**Risques** :\n",
        "- Prompt injection\n",
        "- Extraction de donn√©es sensibles\n",
        "- G√©n√©ration de contenu nuisible\n",
        "\n",
        "**Protection** :\n",
        "```python\n",
        "# Validation des inputs\n",
        "def sanitize_prompt(prompt: str) -> str:\n",
        "    # Limiter la longueur\n",
        "    max_length = 1000\n",
        "    prompt = prompt[:max_length]\n",
        "    \n",
        "    # Filtrer les instructions dangereuses\n",
        "    dangerous_patterns = [\"ignore previous\", \"disregard\"]\n",
        "    # ... filtrage\n",
        "    \n",
        "    return prompt\n",
        "```\n",
        "\n",
        "### 5. Recommandations pour la Production\n",
        "\n",
        "#### Infrastructure\n",
        "- ‚úÖ Load balancing\n",
        "- ‚úÖ Rate limiting\n",
        "- ‚úÖ Caching intelligent\n",
        "- ‚úÖ Monitoring et alertes\n",
        "\n",
        "#### Qualit√©\n",
        "- ‚úÖ A/B testing des prompts\n",
        "- ‚úÖ Logging des g√©n√©rations\n",
        "- ‚úÖ Feedback utilisateurs\n",
        "- ‚úÖ Revue manuelle √©chantillonn√©e\n",
        "\n",
        "#### Co√ªts\n",
        "- ‚úÖ Optimiser max_tokens\n",
        "- ‚úÖ Utiliser des mod√®les adapt√©s √† la t√¢che\n",
        "- ‚úÖ Caching agressif\n",
        "- ‚úÖ Batch processing quand possible\n",
        "\n",
        "---\n",
        "\n",
        "## üéì Pour Aller Plus Loin\n",
        "\n",
        "### Concepts Avanc√©s\n",
        "\n",
        "1. **Fine-tuning** : Adapter un mod√®le pr√©-entra√Æn√© √† votre domaine\n",
        "2. **RAG** : Retrieval Augmented Generation pour donn√©es factuelles\n",
        "3. **Agents** : LLMs avec outils et planification\n",
        "4. **Multi-modal** : Mod√®les texte + images\n",
        "\n",
        "### Ressources\n",
        "\n",
        "- üìö [HuggingFace Documentation](https://huggingface.co/docs)\n",
        "- üìÑ [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
        "- üéì [Stanford CS224N: NLP with Deep Learning](http://web.stanford.edu/class/cs224n/)\n",
        "- üîß [OpenAI Cookbook](https://github.com/openai/openai-cookbook)\n",
        "\n",
        "### Outils\n",
        "\n",
        "- **LangChain** : Framework pour applications LLM\n",
        "- **LlamaIndex** : Framework pour RAG\n",
        "- **PEFT** : Parameter-Efficient Fine-Tuning\n",
        "- **vLLM** : Serving rapide de LLMs\n",
        "\n",
        "---\n",
        "\n",
        "## üéâ Conclusion\n",
        "\n",
        "### Ce que vous avez appris\n",
        "\n",
        "‚úÖ **Tokens & Tokenization** : Comment les LLMs d√©coupent le texte  \n",
        "‚úÖ **Embeddings** : Repr√©sentation vectorielle et visualisation  \n",
        "‚úÖ **Attention** : Le m√©canisme cl√© des Transformers  \n",
        "‚úÖ **Param√®tres de g√©n√©ration** : Temperature, top_k, top_p, etc.  \n",
        "‚úÖ **Exp√©rimentation** : Impact pratique des param√®tres  \n",
        "‚úÖ **Pipeline complet** : Du texte √† la g√©n√©ration  \n",
        "‚úÖ **Perplexit√©** : M√©trique de qualit√©  \n",
        "‚úÖ **Bonnes pratiques** : Production et limites  \n",
        "\n",
        "### Prochaines √âtapes\n",
        "\n",
        "1. üî¨ **Exp√©rimenter** avec diff√©rents mod√®les (gpt2, gpt2-medium, etc.)\n",
        "2. üéØ **Tester** vos propres use cases\n",
        "3. üìö **Approfondir** avec RAG et fine-tuning\n",
        "4. üöÄ **D√©ployer** en production avec les bonnes pratiques\n",
        "\n",
        "---\n",
        "\n",
        "üí° **N'oubliez pas** : Les LLMs sont des outils puissants mais imparfaits. Comprendre leur fonctionnement vous permet de les utiliser efficacement et responsablement.\n",
        "\n",
        "üôè **Merci d'avoir suivi ce notebook !**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}