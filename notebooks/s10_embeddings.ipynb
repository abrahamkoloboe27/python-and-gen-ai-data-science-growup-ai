{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S10 â€” Embeddings & Moteurs de Recherche Vectorielle (RAG Intro)\n",
    "\n",
    "## ğŸ¯ Objectifs\n",
    "- Comprendre les embeddings et leur crÃ©ation\n",
    "- MaÃ®triser la recherche de similaritÃ© (nearest neighbour)\n",
    "- ImplÃ©menter un index FAISS local\n",
    "- Ã‰valuer recall et precision du retrieval\n",
    "\n",
    "## ğŸ“‹ Contenu\n",
    "0. Introduction complÃ¨te aux embeddings\n",
    "0.2 La recherche vectorielle â€” concepts et cas d'usage\n",
    "0.3 Les moteurs de recherche vectorielle â€” comparaison\n",
    "1. Installation et Configuration\n",
    "2. CrÃ©ation du Dataset\n",
    "3. GÃ©nÃ©ration d'Embeddings\n",
    "4. MÃ©triques de SimilaritÃ© (thÃ©orie + code)\n",
    "5. FAISS â€” thÃ©orie approfondie + implÃ©mentation\n",
    "6. Index IVFPQ\n",
    "7. Ã‰valuation: Recall et Precision\n",
    "8. Sauvegarde et Chargement\n",
    "9. Exercices Pratiques\n",
    "10. Concepts AvancÃ©s\n",
    "11. Guide de DÃ©cision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ§  Section 0 â€” Introduction ComplÃ¨te aux Embeddings\n",
    "\n",
    "> *\"Un embedding, c'est l'art de transformer du sens en coordonnÃ©es.\"*\n",
    "\n",
    "---\n",
    "\n",
    "## 0.1 Qu'est-ce qu'un Embedding ?\n",
    "\n",
    "Un **embedding** (plongement vectoriel) est une reprÃ©sentation numÃ©rique dense d'un objet â€” mot, phrase, image, produit â€” sous forme d'un **vecteur de nombres rÃ©els** dans un espace Ã  haute dimension.\n",
    "\n",
    "### Analogie intuitive ğŸ—ºï¸\n",
    "\n",
    "Imaginez une carte gÃ©ographique : chaque ville possÃ¨de des coordonnÃ©es GPS (latitude, longitude). Des villes proches gÃ©ographiquement ont des coordonnÃ©es similaires. Les embeddings font la mÃªme chose avec du **sens** :\n",
    "\n",
    "- `roi` â†’ `[0.82, -0.12, 0.45, ..., 0.33]`  *(384 dimensions)*\n",
    "- `reine` â†’ `[0.79, -0.08, 0.41, ..., 0.35]`  *(coordonnÃ©es proches !)*\n",
    "- `chien` â†’ `[-0.21, 0.66, -0.18, ..., -0.42]`  *(coordonnÃ©es Ã©loignÃ©es)*\n",
    "\n",
    "La magie est que **la proximitÃ© dans l'espace vectoriel = la similaritÃ© sÃ©mantique dans le monde rÃ©el**.\n",
    "\n",
    "### La propriÃ©tÃ© arithmÃ©tique fondamentale\n",
    "\n",
    "Les embeddings capturent des relations analogiques :\n",
    "\n",
    "$$\\text{roi} - \\text{homme} + \\text{femme} \\approx \\text{reine}$$\n",
    "\n",
    "$$\\text{Paris} - \\text{France} + \\text{Italie} \\approx \\text{Rome}$$\n",
    "\n",
    "Cette propriÃ©tÃ©, dÃ©couverte avec Word2Vec en 2013, a rÃ©volutionnÃ© le traitement du langage naturel.\n",
    "\n",
    "---\n",
    "\n",
    "## 0.2 Comment sont-ils crÃ©Ã©s ?\n",
    "\n",
    "Les embeddings sont appris par des **rÃ©seaux de neurones** entraÃ®nÃ©s sur de grandes quantitÃ©s de donnÃ©es. Le principe gÃ©nÃ©ral :\n",
    "\n",
    "1. **PrÃ©-entraÃ®nement** : le modÃ¨le apprend Ã  prÃ©dire des mots/tokens manquants (tÃ¢che auto-supervisÃ©e)\n",
    "2. **ReprÃ©sentation intermÃ©diaire** : les couches cachÃ©es du rÃ©seau forment naturellement des reprÃ©sentations compactes et riches\n",
    "3. **Extraction** : on extrait ces reprÃ©sentations intermÃ©diaires â€” ce sont les embeddings\n",
    "\n",
    "### Ã‰volution des techniques\n",
    "\n",
    "| Ã‰poque | Technique | Innovation clÃ© |\n",
    "|--------|-----------|----------------|\n",
    "| 2013 | **Word2Vec** (Google) | Embeddings de mots, analogies vectorielles |\n",
    "| 2014 | **GloVe** (Stanford) | Co-occurrence globale des mots |\n",
    "| 2017 | **FastText** (Facebook) | Sous-mots, gestion des mots rares |\n",
    "| 2018 | **ELMo** | Embeddings contextuels (BiLSTM) |\n",
    "| 2018 | **BERT** (Google) | Transformers, contexte bidirectionnel |\n",
    "| 2019 | **Sentence-BERT** | Embeddings de phrases sÃ©mantiques |\n",
    "| 2022+ | **OpenAI Embeddings** | text-embedding-ada-002, 1536 dims |\n",
    "| 2023+ | **Multilingual E5, BGE** | Multilingue, performance SOTA |\n",
    "\n",
    "---\n",
    "\n",
    "## 0.3 Visualisation conceptuelle de l'espace vectoriel\n",
    "\n",
    "```\n",
    "        ^ dimension 2 (ex: \"royautÃ©\")\n",
    "        |\n",
    "  reine *          * roi\n",
    "        |\n",
    "        |  * femme          * homme\n",
    "        |\n",
    "        +-------------------------> dimension 1 (ex: \"genre\")\n",
    "       /\n",
    "      / dimension 3 (ex: \"Ã¢ge\")\n",
    "```\n",
    "\n",
    "Dans la rÃ©alitÃ©, les espaces ont **384, 768 ou 1536 dimensions** â€” impossible Ã  visualiser directement. On utilise des techniques comme **t-SNE** ou **UMAP** pour rÃ©duire Ã  2D et visualiser.\n",
    "\n",
    "---\n",
    "\n",
    "## 0.4 Applications des Embeddings dans l'IA\n",
    "\n",
    "Les embeddings sont la fondation de nombreuses applications modernes :\n",
    "\n",
    "### NLP (Traitement du Langage Naturel)\n",
    "- ğŸ” **Recherche sÃ©mantique** : trouver des documents par sens, pas par mots-clÃ©s\n",
    "- ğŸ’¬ **Chatbots & LLMs** : RAG (Retrieval-Augmented Generation)\n",
    "- ğŸŒ **Traduction automatique** : espaces multilingues alignÃ©s\n",
    "- ğŸ“Š **Classification de texte** : spam, sentiment, catÃ©gorisation\n",
    "- ğŸ”— **DÃ©tection de paraphrase/plagiat**\n",
    "\n",
    "### Vision par Ordinateur\n",
    "- ğŸ–¼ï¸ **Recherche d'images similaires** (Pinterest, Google Images)\n",
    "- ğŸ‘¤ **Reconnaissance faciale** (FaceNet)\n",
    "- ğŸ·ï¸ **DÃ©tection d'objets** (CLIP : texte + image dans le mÃªme espace !)\n",
    "\n",
    "### SystÃ¨mes de Recommandation\n",
    "- ğŸ¬ **Netflix, Spotify** : embeddings d'utilisateurs et de contenus\n",
    "- ğŸ›ï¸ **E-commerce** : produits similaires, \"les clients ont aussi achetÃ©\"\n",
    "- ğŸ“° **Fils d'actualitÃ© personnalisÃ©s**\n",
    "\n",
    "### Autres domaines\n",
    "- ğŸ§¬ **Bio-informatique** : embeddings de protÃ©ines (AlphaFold)\n",
    "- ğŸµ **Musique** : similitude de morceaux, gÃ©nÃ©ration\n",
    "- ğŸ”’ **CybersÃ©curitÃ©** : dÃ©tection d'anomalies comportementales\n",
    "- ğŸ’Š **MÃ©decine** : similaritÃ© de molÃ©cules, dossiers patients\n",
    "\n",
    "---\n",
    "\n",
    "## 0.5 Pourquoi les Embeddings sont RÃ©volutionnaires\n",
    "\n",
    "### Avant les embeddings : reprÃ©sentation one-hot\n",
    "\n",
    "```\n",
    "vocabulaire = [\"chat\", \"chien\", \"voiture\", \"avion\"]\n",
    "\"chat\"    â†’ [1, 0, 0, 0]\n",
    "\"chien\"   â†’ [0, 1, 0, 0]\n",
    "\"voiture\" â†’ [0, 0, 1, 0]\n",
    "```\n",
    "\n",
    "**ProblÃ¨mes** :\n",
    "- Aucune information sÃ©mantique (chat et chien aussi distants que chat et voiture)\n",
    "- Dimension = taille du vocabulaire (100k+) â†’ trÃ¨s sparse\n",
    "- Ne gÃ¨re pas les nouveaux mots\n",
    "\n",
    "### Avec les embeddings : reprÃ©sentation dense\n",
    "\n",
    "```\n",
    "\"chat\"    â†’ [0.82, 0.45, -0.12, ..., 0.33]  (384 dims, dense)\n",
    "\"chien\"   â†’ [0.79, 0.41, -0.08, ..., 0.35]  (trÃ¨s similaire !)\n",
    "\"voiture\" â†’ [-0.21, -0.18, 0.66, ..., -0.42] (trÃ¨s diffÃ©rent)\n",
    "```\n",
    "\n",
    "**Avantages** :\n",
    "- âœ… SimilaritÃ© sÃ©mantique encodÃ©e gÃ©omÃ©triquement\n",
    "- âœ… Dimension fixe et compacte (pas d'explosion)\n",
    "- âœ… GÃ©nÃ©ralisation sur des donnÃ©es jamais vues\n",
    "- âœ… Transfert d'apprentissage possible (fine-tuning)\n",
    "\n",
    "---\n",
    "\n",
    "## 0.6 Tableau RÃ©capitulatif â€” ModÃ¨les d'Embeddings\n",
    "\n",
    "| ModÃ¨le | Dimensions | Multilingue | Open Source | Cas d'usage principal |\n",
    "|--------|-----------|-------------|-------------|----------------------|\n",
    "| **Word2Vec** | 50-300 | âŒ | âœ… | Embeddings de mots, baseline |\n",
    "| **GloVe** | 50-300 | âŒ | âœ… | NLP classique, analogies |\n",
    "| **FastText** | 300 | âœ… partiel | âœ… | Textes courts, mots rares |\n",
    "| **BERT** | 768 | âŒ (mBERT oui) | âœ… | Classification, NER |\n",
    "| **Sentence-BERT** | 384-768 | âœ… | âœ… | SimilaritÃ© de phrases |\n",
    "| **text-embedding-ada-002** | 1536 | âœ… | âŒ (OpenAI) | Usage gÃ©nÃ©ral, RAG |\n",
    "| **text-embedding-3-small** | 1536 | âœ… | âŒ (OpenAI) | Efficace, moins cher |\n",
    "| **text-embedding-3-large** | 3072 | âœ… | âŒ (OpenAI) | Haute performance |\n",
    "| **E5-large** | 1024 | âœ… | âœ… | SOTA open source |\n",
    "| **BGE-M3** | 1024 | âœ… | âœ… | Multilingue SOTA |\n",
    "| **paraphrase-multilingual** | 384 | âœ… (50+ langues) | âœ… | Ce notebook ! |\n",
    "\n",
    "---\n",
    "\n",
    "## 0.7 Notion de Dimension\n",
    "\n",
    "La **dimension** d'un embedding reprÃ©sente le nombre de coordonnÃ©es dans l'espace vectoriel. \n",
    "\n",
    "- **Plus de dimensions** â†’ Plus d'information potentielle â†’ Meilleure prÃ©cision\n",
    "- **Moins de dimensions** â†’ Plus rapide Ã  comparer â†’ Moins de mÃ©moire\n",
    "- **Compromis typique** : 384-1536 dimensions pour la plupart des applications\n",
    "\n",
    "Un embedding de 384 dimensions = un point dans un espace Ã  384 dimensions = un vecteur de 384 nombres flottants = **384 Ã— 4 bytes = ~1.5 Ko par vecteur**. Un million de documents = ~1.5 Go de vecteurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ” Section 0.2 â€” La Recherche Vectorielle\n",
    "\n",
    "## Qu'est-ce que la Recherche Vectorielle ?\n",
    "\n",
    "La **recherche vectorielle** (vector search, similarity search) est une technique permettant de trouver les Ã©lÃ©ments les plus similaires Ã  une requÃªte dans une base de vecteurs, en mesurant leur **proximitÃ© gÃ©omÃ©trique** dans l'espace vectoriel.\n",
    "\n",
    "Contrairement Ã  la recherche traditionnelle par mots-clÃ©s (Elasticsearch, SQL LIKE), la recherche vectorielle comprend le **sens** des requÃªtes.\n",
    "\n",
    "---\n",
    "\n",
    "## Pourquoi la Recherche Traditionnelle Ã‰choue\n",
    "\n",
    "### Exemple concret\n",
    "\n",
    "RequÃªte : *\"Comment soigner un rhume ?\"*\n",
    "\n",
    "**Recherche par mots-clÃ©s** :\n",
    "- âœ… Trouve : \"RemÃ¨des contre le rhume\"\n",
    "- âœ… Trouve : \"Comment guÃ©rir d'un rhume ?\"\n",
    "- âŒ Rate : \"Traitement des infections respiratoires lÃ©gÃ¨res\"\n",
    "- âŒ Rate : \"Que faire quand on est enrhumÃ© ?\"\n",
    "- âŒ Rate : \"Antibiotiques vs repos pour les maladies virales\"\n",
    "\n",
    "**Recherche vectorielle** :\n",
    "- âœ… Trouve tous les exemples ci-dessus !\n",
    "- âœ… Comprend la relation sÃ©mantique entre rhume / infection / viral\n",
    "- âœ… Fonctionne mÃªme si aucun mot de la requÃªte n'est dans le document\n",
    "\n",
    "---\n",
    "\n",
    "## Comment Fonctionne la Recherche Vectorielle\n",
    "\n",
    "### Ã‰tape 1 : Indexation (offline)\n",
    "\n",
    "```\n",
    "Document â†’ [ModÃ¨le d'embedding] â†’ Vecteur â†’ [Index vectoriel]\n",
    "\"Le chat mange\" â†’ BERT â†’ [0.82, -0.12, ..., 0.45] â†’ stockÃ©\n",
    "```\n",
    "\n",
    "### Ã‰tape 2 : RequÃªte (online)\n",
    "\n",
    "```\n",
    "RequÃªte â†’ [ModÃ¨le d'embedding] â†’ Vecteur requÃªte\n",
    "\"animal qui mange\" â†’ BERT â†’ [0.79, -0.10, ..., 0.43]\n",
    "```\n",
    "\n",
    "### Ã‰tape 3 : Comparaison\n",
    "\n",
    "Calculer la distance/similaritÃ© entre le vecteur requÃªte et **tous les vecteurs indexÃ©s**, puis retourner les k plus proches (k-NN : k Nearest Neighbors).\n",
    "\n",
    "---\n",
    "\n",
    "## Nearest Neighbor Search : Exact vs ApprochÃ©\n",
    "\n",
    "### Exact Nearest Neighbor (kNN)\n",
    "- Compare la requÃªte avec **TOUS** les vecteurs\n",
    "- Garantit de trouver les vrais k plus proches voisins\n",
    "- ComplexitÃ© : $O(n \\cdot d)$ oÃ¹ n = nb vecteurs, d = dimensions\n",
    "- **ProblÃ¨me** : Pour 10 millions de vecteurs de 768 dims â†’ des milliards d'opÃ©rations !\n",
    "\n",
    "### Approximate Nearest Neighbor (ANN) âš¡\n",
    "- Utilise des structures d'index intelligentes pour Ã©viter de tout parcourir\n",
    "- Peut manquer quelques voisins vrais, mais trÃ¨s rare en pratique\n",
    "- ComplexitÃ© : $O(\\log n)$ Ã  $O(\\sqrt{n})$ selon la mÃ©thode\n",
    "- **Gain typique** : 10x Ã  1000x plus rapide pour >0.95 de recall\n",
    "- C'est ce qu'utilisent FAISS (IVF, HNSW), Milvus, Qdrant, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## Le Compromis Fondamental : Vitesse vs PrÃ©cision (Recall)\n",
    "\n",
    "```\n",
    "PrÃ©cision (Recall)\n",
    "100% |â—  Exact kNN\n",
    "     |  \\\n",
    " 99% |   â— HNSW (ef=200)\n",
    "     |    \\\n",
    " 95% |     â— HNSW (ef=100)\n",
    "     |      \\\n",
    " 90% |       â— IVF (nprobe=10)\n",
    "     |        \\\n",
    " 80% |         â— IVF (nprobe=5)\n",
    "     +----------------------------> Vitesse (req/sec)\n",
    "       lent          trÃ¨s rapide\n",
    "```\n",
    "\n",
    "Dans la plupart des applications rÃ©elles, **95-99% de recall avec 10-100x de speedup** est un excellent compromis.\n",
    "\n",
    "---\n",
    "\n",
    "## Cas d'Usage de la Recherche Vectorielle\n",
    "\n",
    "| Domaine | Application | Exemple concret |\n",
    "|---------|-------------|-----------------|\n",
    "| ğŸ” **RAG / LLMs** | Retrieval-Augmented Generation | ChatGPT avec contexte entreprise |\n",
    "| ğŸ“š **Recherche sÃ©mantique** | Moteur de recherche interne | Trouver des docs RH par sens |\n",
    "| ğŸ¬ **Recommandation** | Contenus similaires | Netflix \"parce que vous avez aimÃ©...\" |\n",
    "| ğŸ–¼ï¸ **Recherche d'images** | Image-to-image | Pinterest visuellement similaire |\n",
    "| ğŸ”’ **DÃ©tection de fraude** | Anomalie comportementale | Transaction inhabituelle |\n",
    "| ğŸ§¬ **Bio-informatique** | SÃ©quences similaires | ProtÃ©ines homologues |\n",
    "| ğŸ’¬ **DÃ©duplication** | Documents en double | Articles de presse dupliquÃ©s |\n",
    "| ğŸŒ **Traduction** | Alignement multilingue | Phrases Ã©quivalentes EN/FR |\n",
    "| ğŸ’Š **Pharma** | MolÃ©cules similaires | Drug discovery |\n",
    "| ğŸ›¡ï¸ **CybersÃ©curitÃ©** | Malwares similaires | Variantes de virus |\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture Typique d'un SystÃ¨me RAG avec Recherche Vectorielle\n",
    "\n",
    "```\n",
    "OFFLINE (Indexation)                    ONLINE (RequÃªte)\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”                   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "Documents          Query utilisateur\n",
    "    â†“                   â†“\n",
    "[Chunking]         [Embedding model]\n",
    "    â†“                   â†“\n",
    "[Embedding model]  Vecteur requÃªte\n",
    "    â†“                   â†“\n",
    "[Index vectoriel]  [Recherche ANN]  â†’  Top-k documents\n",
    "                        â†“\n",
    "                   [LLM (GPT-4...)]\n",
    "                        â†“\n",
    "                   RÃ©ponse enrichie\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## MÃ©triques de Performance en Recherche Vectorielle\n",
    "\n",
    "| MÃ©trique | DÃ©finition | Formule | InterprÃ©tation |\n",
    "|----------|-----------|---------|----------------|\n",
    "| **Recall@k** | % de vrais pertinents dans les k rÃ©sultats | $\\frac{|Pertinents \\cap RÃ©cupÃ©rÃ©s@k|}{|Pertinents|}$ | 1.0 = parfait |\n",
    "| **Precision@k** | % des k rÃ©sultats qui sont pertinents | $\\frac{|Pertinents \\cap RÃ©cupÃ©rÃ©s@k|}{k}$ | 1.0 = parfait |\n",
    "| **MRR** | Position moyenne du 1er rÃ©sultat pertinent | $\\frac{1}{|Q|}\\sum_{i=1}^{|Q|}\\frac{1}{rank_i}$ | 1.0 = toujours 1er |\n",
    "| **NDCG@k** | Normalise selon la position du rÃ©sultat | Complexe | Tient compte de l'ordre |\n",
    "| **Latence p99** | 99e percentile du temps de rÃ©ponse | - | <100ms typiquement |\n",
    "| **QPS** | Queries Per Second (dÃ©bit) | - | Selon l'infrastructure |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ­ Section 0.3 â€” Les Moteurs de Recherche Vectorielle\n",
    "\n",
    "## Qu'est-ce qu'un Moteur de Recherche Vectorielle ?\n",
    "\n",
    "Un **moteur de recherche vectorielle** (vector search engine ou vector database) est un systÃ¨me spÃ©cialisÃ© pour :\n",
    "1. **Stocker** des vecteurs (embeddings) de maniÃ¨re persistante et scalable\n",
    "2. **Indexer** ces vecteurs avec des structures ANN efficaces\n",
    "3. **Rechercher** les k plus proches voisins d'une requÃªte vectorielle, trÃ¨s rapidement\n",
    "4. **Filtrer** par mÃ©tadonnÃ©es (ex: chercher dans les docs d'une seule entreprise)\n",
    "5. **GÃ©rer** la mise Ã  jour, la suppression, la rÃ©plication des donnÃ©es\n",
    "\n",
    "Un simple tableau NumPy suffit pour 1000 vecteurs. Pour 100 millions, il faut un vrai moteur.\n",
    "\n",
    "---\n",
    "\n",
    "## Comparaison ComplÃ¨te des Moteurs de Recherche Vectorielle\n",
    "\n",
    "| Moteur | Type | Langage | ScalabilitÃ© | Filtrage | Prix | Cas d'usage idÃ©al |\n",
    "|--------|------|---------|-------------|----------|------|-------------------|\n",
    "| **FAISS** | Local (lib) | C++/Python | Mono-nÅ“ud | âŒ natif | Gratuit | Recherche locale, prototypage, recherche acadÃ©mique |\n",
    "| **Milvus** | Open source / Cloud | Go/C++ | Multi-nÅ“uds âœ… | âœ… | Gratuit (self-hosted) / Zilliz Cloud | Production large Ã©chelle, milliards de vecteurs |\n",
    "| **Weaviate** | Open source / Cloud | Go | Multi-nÅ“uds âœ… | âœ… | Gratuit / Cloud payant | Recherche hybride (vectorielle + BM25), GraphQL |\n",
    "| **Pinecone** | Cloud managed | - | Auto-scale âœ… | âœ… | Gratuit jusqu'Ã  1M / Payant | RapiditÃ© de dÃ©ploiement, serverless, startup |\n",
    "| **Qdrant** | Open source / Cloud | Rust | Multi-nÅ“uds âœ… | âœ… avancÃ© | Gratuit / Cloud payant | Haute performance, filtrage complexe |\n",
    "| **Chroma** | Local / Cloud | Python | Mono-nÅ“ud | âœ… | Gratuit | Prototypage rapide, RAG avec LangChain |\n",
    "| **Redis Vector** | Hybrid | C | Horizontal âœ… | âœ… | Redis open source / Enterprise | Cas temps rÃ©el, dÃ©jÃ  utilisateurs Redis |\n",
    "| **Elasticsearch** | Hybrid | Java | Horizontal âœ… | âœ… | Open source / Elastic Cloud | Recherche hybride (lexicale + vectorielle) |\n",
    "| **pgvector** | Extension SQL | C | Mono-nÅ“ud | âœ… SQL | Gratuit | DÃ©jÃ  sur PostgreSQL, petits volumes |\n",
    "| **Vespa** | Open source | Java/C++ | Multi-nÅ“uds âœ… | âœ… | Gratuit / Managed | Recherche complexe + ML en temps rÃ©el |\n",
    "\n",
    "---\n",
    "\n",
    "## Comparaison Technique Approfondie\n",
    "\n",
    "| Moteur | Index ANN | Distance supportÃ©e | Persistance | API |\n",
    "|--------|-----------|-------------------|-------------|-----|\n",
    "| **FAISS** | Flat, IVF, HNSW, PQ | L2, IP, Cosine | Fichier binaire | Python/C++ |\n",
    "| **Milvus** | IVF_FLAT, HNSW, IVF_PQ, SCANN | L2, IP, Cosine, Jaccard | Distributed storage | gRPC / REST |\n",
    "| **Weaviate** | HNSW | Cosine, L2, Dot | Disk persistent | GraphQL / REST |\n",
    "| **Pinecone** | PropriÃ©taire (Pinecone Graph) | Cosine, Dot, Euclidean | Cloud managed | REST / gRPC |\n",
    "| **Qdrant** | HNSW | Cosine, Dot, L2, Manhattan | Disk + WAL | REST / gRPC |\n",
    "| **Chroma** | HNSW (via hnswlib) | L2, Cosine, IP | SQLite / Parquet | Python / REST |\n",
    "| **Redis Vector** | FLAT, HNSW | L2, IP, Cosine | RDB / AOF | Redis protocol |\n",
    "| **Elasticsearch** | HNSW | Cosine, Dot, L2 | Lucene segments | REST |\n",
    "\n",
    "---\n",
    "\n",
    "## Quand Choisir Quel Moteur ?\n",
    "\n",
    "### ğŸ§ª Pour du prototypage et de la R&D\n",
    "â†’ **FAISS** ou **Chroma** : simple, rapide Ã  mettre en place, pas de serveur\n",
    "\n",
    "### ğŸ­ Pour la production petite/moyenne Ã©chelle (<10M vecteurs)\n",
    "â†’ **Qdrant** ou **Weaviate** : excellent Ã©quilibre performance/fonctionnalitÃ©s, Docker-ready\n",
    "\n",
    "### ğŸš€ Pour la production grande Ã©chelle (>100M vecteurs)\n",
    "â†’ **Milvus** : conÃ§u pour Ã§a, trÃ¨s scalable\n",
    "\n",
    "### âš¡ Pour le temps rÃ©el ou dÃ©jÃ  sur Redis\n",
    "â†’ **Redis Vector Search** : faible latence, intÃ©gration transparente\n",
    "\n",
    "### ğŸŒ Pour la recherche hybride (textuelle + vectorielle)\n",
    "â†’ **Elasticsearch** avec kNN ou **Weaviate** avec BM25 hybrid\n",
    "\n",
    "### ğŸ’° Pour une startup qui veut dÃ©ployer vite sans gÃ©rer l'infra\n",
    "â†’ **Pinecone** : serverless, zÃ©ro gestion\n",
    "\n",
    "### ğŸ—„ï¸ Pour des Ã©quipes dÃ©jÃ  sur PostgreSQL\n",
    "â†’ **pgvector** : extension PostgreSQL native, SQL familier\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture de DÃ©ploiement Typique\n",
    "\n",
    "```\n",
    "Application\n",
    "    â†“  requÃªte texte\n",
    "[ModÃ¨le d'embedding]   â† GPU/CPU server\n",
    "    â†“  vecteur 768-dim\n",
    "[Moteur vectoriel]     â† Qdrant / Milvus / Pinecone\n",
    "    â†“  top-k vecteurs + mÃ©tadonnÃ©es\n",
    "[Application]\n",
    "    â†“  documents rÃ©cupÃ©rÃ©s\n",
    "[LLM]                  â† OpenAI / Llama / Mistral\n",
    "    â†“  rÃ©ponse finale\n",
    "Utilisateur\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Points de Vigilance en Production\n",
    "\n",
    "| Aspect | Recommandation |\n",
    "|--------|----------------|\n",
    "| **CohÃ©rence** | MÃªme modÃ¨le d'embedding pour indexation et requÃªte ! |\n",
    "| **Versioning** | GÃ©rer les changements de modÃ¨le (rÃ©indexation complÃ¨te) |\n",
    "| **Monitoring** | Surveiller la latence, le recall, la dÃ©rive des distributions |\n",
    "| **SÃ©curitÃ©** | Isolation par tenant, chiffrement des vecteurs sensibles |\n",
    "| **Backup** | Les index ANN peuvent Ãªtre reconstruits mais c'est long |\n",
    "| **Mise Ã  jour** | Ajout de vecteurs = facile ; suppression = selon le moteur |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Installation et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dÃ©pendances\n",
    "# !pip install faiss-cpu openai sentence-transformers pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "import pickle\n",
    "\n",
    "print(\"âœ… BibliothÃ¨ques importÃ©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CrÃ©ation du Dataset\n",
    "\n",
    "Nous allons crÃ©er un dataset simulÃ© de documents techniques pour la dÃ©monstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de documents (simulÃ©)\n",
    "documents = [\n",
    "    {\"id\": 1, \"text\": \"Le machine learning est une branche de l'intelligence artificielle.\", \"category\": \"ML\"},\n",
    "    {\"id\": 2, \"text\": \"Les rÃ©seaux de neurones profonds sont utilisÃ©s pour l'apprentissage profond.\", \"category\": \"DL\"},\n",
    "    {\"id\": 3, \"text\": \"Python est un langage de programmation populaire pour la data science.\", \"category\": \"Programming\"},\n",
    "    {\"id\": 4, \"text\": \"FastAPI permet de crÃ©er des APIs REST rapidement avec Python.\", \"category\": \"Programming\"},\n",
    "    {\"id\": 5, \"text\": \"Les transformers rÃ©volutionnent le traitement du langage naturel.\", \"category\": \"NLP\"},\n",
    "    {\"id\": 6, \"text\": \"BERT est un modÃ¨le prÃ©-entraÃ®nÃ© basÃ© sur l'architecture transformer.\", \"category\": \"NLP\"},\n",
    "    {\"id\": 7, \"text\": \"Le gradient descent est un algorithme d'optimisation fondamental.\", \"category\": \"ML\"},\n",
    "    {\"id\": 8, \"text\": \"Les embeddings capturent la sÃ©mantique des mots dans un espace vectoriel.\", \"category\": \"NLP\"},\n",
    "    {\"id\": 9, \"text\": \"Docker permet de conteneuriser des applications pour un dÃ©ploiement facile.\", \"category\": \"DevOps\"},\n",
    "    {\"id\": 10, \"text\": \"Kubernetes orchestre des conteneurs Ã  grande Ã©chelle.\", \"category\": \"DevOps\"},\n",
    "    {\"id\": 11, \"text\": \"Les CNNs sont particuliÃ¨rement efficaces pour la vision par ordinateur.\", \"category\": \"DL\"},\n",
    "    {\"id\": 12, \"text\": \"Le fine-tuning adapte un modÃ¨le prÃ©-entraÃ®nÃ© Ã  une tÃ¢che spÃ©cifique.\", \"category\": \"ML\"},\n",
    "    {\"id\": 13, \"text\": \"GPT-4 est un modÃ¨le de langage gÃ©nÃ©ratif dÃ©veloppÃ© par OpenAI.\", \"category\": \"NLP\"},\n",
    "    {\"id\": 14, \"text\": \"Les API REST utilisent HTTP pour communiquer entre services.\", \"category\": \"Programming\"},\n",
    "    {\"id\": 15, \"text\": \"L'attention est le mÃ©canisme clÃ© des architectures transformer.\", \"category\": \"DL\"},\n",
    "    {\"id\": 16, \"text\": \"Pandas est une bibliothÃ¨que Python pour l'analyse de donnÃ©es.\", \"category\": \"Programming\"},\n",
    "    {\"id\": 17, \"text\": \"Le RAG combine retrieval et gÃ©nÃ©ration pour amÃ©liorer les LLMs.\", \"category\": \"NLP\"},\n",
    "    {\"id\": 18, \"text\": \"FAISS est une bibliothÃ¨que pour la recherche de similaritÃ© vectorielle.\", \"category\": \"ML\"},\n",
    "    {\"id\": 19, \"text\": \"Les bases de donnÃ©es vectorielles stockent et recherchent des embeddings.\", \"category\": \"ML\"},\n",
    "    {\"id\": 20, \"text\": \"Le tokenisation dÃ©coupe le texte en unitÃ©s traitables par les modÃ¨les.\", \"category\": \"NLP\"},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(documents)\n",
    "print(f\"ğŸ“Š Dataset crÃ©Ã©: {len(df)} documents\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GÃ©nÃ©ration d'Embeddings\n",
    "\n",
    "### 3.1 Chargement du modÃ¨le d'embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger un modÃ¨le d'embeddings multilingue\n",
    "# Options: 'paraphrase-multilingual-MiniLM-L12-v2', 'distiluse-base-multilingual-cased-v2'\n",
    "embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "print(f\"âœ… ModÃ¨le chargÃ©: {embedding_model.get_sentence_embedding_dimension()} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 CrÃ©ation des embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts: List[str], model: SentenceTransformer) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    CrÃ©er des embeddings pour une liste de textes\n",
    "    \"\"\"\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    return np.array(embeddings).astype('float32')\n",
    "\n",
    "# GÃ©nÃ©rer les embeddings\n",
    "texts = df['text'].tolist()\n",
    "embeddings = create_embeddings(texts, embedding_model)\n",
    "\n",
    "print(f\"âœ… Embeddings crÃ©Ã©s: shape = {embeddings.shape}\")\n",
    "print(f\"   - {embeddings.shape[0]} documents\")\n",
    "print(f\"   - {embeddings.shape[1]} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ“ Section 4 â€” MÃ©triques de SimilaritÃ© : ThÃ©orie Approfondie\n",
    "\n",
    "Avant de coder, comprenons les mÃ©triques de distance et de similaritÃ© qui sont au cÅ“ur de la recherche vectorielle. Le **choix de la mÃ©trique est crucial** et dÃ©pend du type de donnÃ©es, du modÃ¨le d'embedding utilisÃ©, et du cas d'usage.\n",
    "\n",
    "---\n",
    "\n",
    "## 4.1 SimilaritÃ© Cosinus (Cosine Similarity)\n",
    "\n",
    "### DÃ©finition\n",
    "\n",
    "La similaritÃ© cosinus mesure le **cosinus de l'angle** entre deux vecteurs, indÃ©pendamment de leur magnitude (longueur). Elle varie entre **-1 et +1** (ou 0 et 1 pour des vecteurs positifs).\n",
    "\n",
    "$$\\text{cos}(\\vec{A}, \\vec{B}) = \\frac{\\vec{A} \\cdot \\vec{B}}{\\|\\vec{A}\\| \\cdot \\|\\vec{B}\\|}$$\n",
    "\n",
    "oÃ¹ $\\vec{A} \\cdot \\vec{B} = \\sum_{i=1}^{n} A_i B_i$ est le produit scalaire.\n",
    "\n",
    "### InterprÃ©tation\n",
    "\n",
    "| Valeur | Signification |\n",
    "|--------|--------------|\n",
    "| **1.0** | Vecteurs identiques (mÃªme direction) |\n",
    "| **0.0** | Vecteurs perpendiculaires (aucune relation) |\n",
    "| **-1.0** | Vecteurs opposÃ©s |\n",
    "\n",
    "### Quand utiliser la similaritÃ© cosinus ?\n",
    "\n",
    "âœ… **IdÃ©ale pour** :\n",
    "- Embeddings de texte en gÃ©nÃ©ral (la magnitude peut varier selon la longueur du texte)\n",
    "- Comparaison de documents de longueurs diffÃ©rentes\n",
    "- ModÃ¨les Sentence-BERT, Word2Vec, GloVe\n",
    "\n",
    "âš ï¸ **Limitations** :\n",
    "- Ignore la magnitude (un texte de 1 mot et 1000 mots avec le mÃªme thÃ¨me = mÃªme score)\n",
    "- CoÃ»t calcul $O(d)$ par paire\n",
    "\n",
    "---\n",
    "\n",
    "## 4.2 Distance Euclidienne (L2)\n",
    "\n",
    "### DÃ©finition\n",
    "\n",
    "La distance euclidienne (ou norme L2) mesure la **distance en ligne droite** dans l'espace Ã  n dimensions. C'est l'extension de la formule de Pythagore.\n",
    "\n",
    "$$d_{L2}(\\vec{A}, \\vec{B}) = \\|\\vec{A} - \\vec{B}\\|_2 = \\sqrt{\\sum_{i=1}^{n} (A_i - B_i)^2}$$\n",
    "\n",
    "### PropriÃ©tÃ©s\n",
    "\n",
    "- Distance de 0 = vecteurs identiques\n",
    "- Plus la distance est grande, moins les vecteurs sont similaires\n",
    "- **Sensible Ã  la magnitude** des vecteurs\n",
    "\n",
    "### Quand utiliser L2 ?\n",
    "\n",
    "âœ… **IdÃ©ale pour** :\n",
    "- Vecteurs normalisÃ©s (alors L2 âˆ cosine)\n",
    "- Images et donnÃ©es continues oÃ¹ la magnitude a du sens\n",
    "- Clustering (k-means utilise L2)\n",
    "- FAISS par dÃ©faut utilise L2\n",
    "\n",
    "âš ï¸ **Limitations** :\n",
    "- Sensible Ã  l'Ã©chelle des features (normalisation recommandÃ©e)\n",
    "- **MalÃ©diction de la dimensionnalitÃ©** : en haute dimension, toutes les distances tendent Ã  devenir similaires\n",
    "\n",
    "> **Note** : Si les vecteurs sont normalisÃ©s (norme = 1), alors : $d_{L2}^2 = 2(1 - \\cos(\\vec{A}, \\vec{B}))$  \n",
    "> i.e. minimiser L2 = maximiser cosine sur des vecteurs unitaires.\n",
    "\n",
    "---\n",
    "\n",
    "## 4.3 Produit Scalaire / Inner Product (IP)\n",
    "\n",
    "### DÃ©finition\n",
    "\n",
    "Le produit scalaire mesure Ã  la fois l'**angle** ET la **magnitude** des vecteurs.\n",
    "\n",
    "$$\\text{IP}(\\vec{A}, \\vec{B}) = \\vec{A} \\cdot \\vec{B} = \\sum_{i=1}^{n} A_i B_i = \\|\\vec{A}\\| \\cdot \\|\\vec{B}\\| \\cdot \\cos(\\theta)$$\n",
    "\n",
    "### Quand utiliser IP ?\n",
    "\n",
    "âœ… **IdÃ©al pour** :\n",
    "- ModÃ¨les entraÃ®nÃ©s explicitement pour le produit scalaire (OpenAI text-embedding-ada-002)\n",
    "- SystÃ¨mes de recommandation (magnitude = popularitÃ© ou confiance)\n",
    "- Quand la magnitude du vecteur encode une information utile\n",
    "\n",
    "âš ï¸ **Limitations** :\n",
    "- RÃ©sultats non bornÃ©s â†’ moins intuitif\n",
    "- Si les vecteurs ne sont pas normalisÃ©s, favorise les vecteurs de grande magnitude\n",
    "\n",
    "> **Conseil** : VÃ©rifiez toujours la documentation du modÃ¨le d'embedding pour savoir quelle mÃ©trique il prÃ©conise !\n",
    "\n",
    "---\n",
    "\n",
    "## 4.4 Distance Manhattan (L1)\n",
    "\n",
    "### DÃ©finition\n",
    "\n",
    "La distance Manhattan (taxicab distance) mesure la somme des **valeurs absolues des diffÃ©rences** composante par composante. Imagine te dÃ©placer dans une grille de rues comme Ã  Manhattan.\n",
    "\n",
    "$$d_{L1}(\\vec{A}, \\vec{B}) = \\|\\vec{A} - \\vec{B}\\|_1 = \\sum_{i=1}^{n} |A_i - B_i|$$\n",
    "\n",
    "### Quand utiliser L1 ?\n",
    "\n",
    "âœ… **IdÃ©ale pour** :\n",
    "- DonnÃ©es avec des **outliers** (plus robuste que L2)\n",
    "- DonnÃ©es sparse (beaucoup de zÃ©ros)\n",
    "- Features catÃ©gorielles encodÃ©es numÃ©riquement\n",
    "\n",
    "âš ï¸ **Limitations** :\n",
    "- Moins populaire pour les embeddings\n",
    "- Pas supportÃ©e nativement par tous les moteurs vectoriels\n",
    "\n",
    "---\n",
    "\n",
    "## 4.5 Distance de Hamming\n",
    "\n",
    "### DÃ©finition\n",
    "\n",
    "La distance de Hamming compte le nombre de **positions diffÃ©rentes** entre deux vecteurs binaires ou deux chaÃ®nes de mÃªme longueur.\n",
    "\n",
    "$$d_{Ham}(\\vec{A}, \\vec{B}) = \\sum_{i=1}^{n} \\mathbb{1}[A_i \\neq B_i]$$\n",
    "\n",
    "### Quand utiliser Hamming ?\n",
    "\n",
    "âœ… **IdÃ©ale pour** :\n",
    "- **Embeddings binaires** (compact, trÃ¨s rapide Ã  comparer via XOR)\n",
    "- Codes de correction d'erreurs\n",
    "- Comparaison de sÃ©quences ADN\n",
    "- Hashing perceptuel d'images\n",
    "\n",
    "âš ï¸ **Limitations** :\n",
    "- Seulement applicable Ã  des vecteurs binaires ou discrets\n",
    "- Perd les nuances des valeurs continues\n",
    "\n",
    "---\n",
    "\n",
    "## 4.6 Distance de Jaccard\n",
    "\n",
    "### DÃ©finition\n",
    "\n",
    "Mesure la similaritÃ© entre deux **ensembles** (sets).\n",
    "\n",
    "$$J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "\n",
    "Varie entre 0 (aucune intersection) et 1 (ensembles identiques).\n",
    "\n",
    "### Quand utiliser Jaccard ?\n",
    "\n",
    "âœ… **IdÃ©ale pour** :\n",
    "- Comparaison de documents par ensembles de mots (bag-of-words)\n",
    "- DÃ©tection de plagiat\n",
    "- Recommandation basÃ©e sur des achats communs\n",
    "\n",
    "---\n",
    "\n",
    "## 4.7 Tableau Comparatif Complet des MÃ©triques\n",
    "\n",
    "| MÃ©trique | Formule | Plage | Normalisation req. | Robuste aux outliers | Haute dimension | Cas d'usage |\n",
    "|----------|---------|-------|-------------------|---------------------|-----------------|-------------|\n",
    "| **Cosine** | $\\frac{\\vec{A}\\cdot\\vec{B}}{\\|A\\|\\|B\\|}$ | [-1, 1] | âŒ non req. | Moyen | âœ… TrÃ¨s bon | NLP, texte, Sentence-BERT |\n",
    "| **L2 (Euclidean)** | $\\sqrt{\\sum(A_i-B_i)^2}$ | [0, +âˆ) | âœ… recommandÃ©e | âŒ Sensible | âš ï¸ Moyen | Images, donnÃ©es continues |\n",
    "| **Inner Product** | $\\sum A_i B_i$ | (-âˆ, +âˆ) | âŒ non req. | âŒ Sensible | âœ… Bon | OpenAI embeddings, recommandation |\n",
    "| **L1 (Manhattan)** | $\\sum|A_i-B_i|$ | [0, +âˆ) | âœ… recommandÃ©e | âœ… Bon | âš ï¸ Moyen | DonnÃ©es sparse, robustesse |\n",
    "| **Hamming** | $\\sum \\mathbb{1}[A_i\\neq B_i]$ | [0, d] | N/A (binaire) | N/A | âœ… Excellent | Binaire, ADN, hashing |\n",
    "| **Jaccard** | $\\frac{|Aâˆ©B|}{|AâˆªB|}$ | [0, 1] | N/A (ensembles) | N/A | âŒ Mauvais | Sets, NLP bag-of-words |\n",
    "\n",
    "---\n",
    "\n",
    "## 4.8 MÃ©triques vs ModÃ¨les d'Embedding\n",
    "\n",
    "| ModÃ¨le | MÃ©trique recommandÃ©e | Raison |\n",
    "|--------|---------------------|--------|\n",
    "| **Sentence-BERT** | Cosine | EntraÃ®nÃ© avec cosine |\n",
    "| **paraphrase-multilingual** | Cosine | Embeddings normalisÃ©s |\n",
    "| **text-embedding-ada-002 (OpenAI)** | Cosine ou Inner Product | Vecteurs normalisÃ©s |\n",
    "| **Word2Vec / GloVe** | Cosine | Standard pour word embeddings |\n",
    "| **CLIP (images+texte)** | Cosine | Espace normalisÃ© partagÃ© |\n",
    "| **ANN Binary Codes** | Hamming | XOR bit-Ã -bit trÃ¨s rapide |\n",
    "| **Collaborative Filtering** | Inner Product | Magnitude = confiance |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MÃ©triques de SimilaritÃ©\n",
    "\n",
    "### 4.1 Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(embedding1: np.ndarray, embedding2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculer la similaritÃ© cosinus entre deux embeddings\n",
    "    \"\"\"\n",
    "    return cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "\n",
    "# Exemple: Comparer deux documents\n",
    "doc1_emb = embeddings[0]  # \"Le machine learning...\"\n",
    "doc2_emb = embeddings[1]  # \"Les rÃ©seaux de neurones...\"\n",
    "doc3_emb = embeddings[2]  # \"Python est un langage...\"\n",
    "\n",
    "sim_1_2 = compute_cosine_similarity(doc1_emb, doc2_emb)\n",
    "sim_1_3 = compute_cosine_similarity(doc1_emb, doc3_emb)\n",
    "\n",
    "print(f\"SimilaritÃ© entre doc1 et doc2 (tous deux ML): {sim_1_2:.4f}\")\n",
    "print(f\"SimilaritÃ© entre doc1 et doc3 (ML vs Programming): {sim_1_3:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Matrice de similaritÃ©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer la matrice de similaritÃ© pour les 10 premiers documents\n",
    "sample_embeddings = embeddings[:10]\n",
    "similarity_matrix = cosine_similarity(sample_embeddings)\n",
    "\n",
    "# Visualiser\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(similarity_matrix, cmap='YlOrRd', aspect='auto')\n",
    "plt.colorbar(label='Cosine Similarity')\n",
    "plt.xlabel('Document ID')\n",
    "plt.ylabel('Document ID')\n",
    "plt.title('Matrice de SimilaritÃ© (10 premiers documents)')\n",
    "plt.xticks(range(10), [f\"Doc {i+1}\" for i in range(10)], rotation=45)\n",
    "plt.yticks(range(10), [f\"Doc {i+1}\" for i in range(10)])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 DÃ©monstration Comparative des MÃ©triques de Distance\n",
    "\n",
    "Comparons visuellement les diffÃ©rentes mÃ©triques sur nos embeddings rÃ©els."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# DÃ©monstration comparative des mÃ©triques de distance\n",
    "# =====================================================\n",
    "from scipy.spatial.distance import euclidean, cityblock, hamming\n",
    "\n",
    "def compute_all_metrics(vec_a: np.ndarray, vec_b: np.ndarray) -> dict:\n",
    "    \"\"\"Calcule toutes les mÃ©triques de distance/similaritÃ© entre deux vecteurs\"\"\"\n",
    "    # Normaliser pour cosine\n",
    "    norm_a_val = np.linalg.norm(vec_a)\n",
    "    norm_b_val = np.linalg.norm(vec_b)\n",
    "    if norm_a_val < 1e-9 or norm_b_val < 1e-9:\n",
    "        import warnings\n",
    "        warnings.warn(\"Vecteur quasi-nul dÃ©tectÃ© â€” embedding potentiellement invalide\")\n",
    "    norm_a = vec_a / (norm_a_val + 1e-9)\n",
    "    norm_b = vec_b / (norm_b_val + 1e-9)\n",
    "    \n",
    "    cosine_sim = float(np.dot(norm_a, norm_b))\n",
    "    l2_dist = float(euclidean(vec_a, vec_b))\n",
    "    l1_dist = float(cityblock(vec_a, vec_b))\n",
    "    dot_product = float(np.dot(vec_a, vec_b))\n",
    "    \n",
    "    return {\n",
    "        \"Cosine Similarity\": cosine_sim,\n",
    "        \"L2 Distance\": l2_dist,\n",
    "        \"L1 Distance\": l1_dist,\n",
    "        \"Inner Product\": dot_product,\n",
    "    }\n",
    "\n",
    "# Paires de documents Ã  comparer\n",
    "pairs = [\n",
    "    (0, 1, \"ML vs Deep Learning (domaine proche)\"),\n",
    "    (0, 2, \"ML vs Python/Programming (domaine Ã©loignÃ©)\"),\n",
    "    (4, 5, \"Transformers vs BERT (trÃ¨s similaires)\"),\n",
    "    (8, 9, \"Docker vs Kubernetes (mÃªme domaine DevOps)\"),\n",
    "    (0, 16, \"ML vs RAG (lien indirect via NLP)\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Paire':<45} {'Cosine':>8} {'L2':>8} {'L1':>10} {'Dot':>10}\")\n",
    "print(\"=\" * 85)\n",
    "\n",
    "results_list = []\n",
    "for i, j, label in pairs:\n",
    "    metrics = compute_all_metrics(embeddings[i], embeddings[j])\n",
    "    results_list.append((label, metrics))\n",
    "    print(f\"{label:<45} {metrics['Cosine Similarity']:>8.4f} {metrics['L2 Distance']:>8.4f} \"\n",
    "          f\"{metrics['L1 Distance']:>10.2f} {metrics['Inner Product']:>10.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"âœ… Observation: Cosine et Inner Product montrent des tendances similaires\")\n",
    "print(\"   (car les embeddings Sentence-BERT sont quasi-normalisÃ©s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation comparative des mÃ©triques\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Comparaison des MÃ©triques de Distance/SimilaritÃ©\\n(sur paires de documents)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "MAX_LABEL_LENGTH = 35  # Longueur max des Ã©tiquettes pour l'affichage\n",
    "labels = [p[2][:MAX_LABEL_LENGTH] + '...' if len(p[2]) > MAX_LABEL_LENGTH else p[2] for p in pairs]\n",
    "metric_names = [\"Cosine Similarity\", \"L2 Distance\", \"L1 Distance\", \"Inner Product\"]\n",
    "colors = ['#2196F3', '#F44336', '#FF9800', '#4CAF50']\n",
    "\n",
    "for ax, metric, color in zip(axes.flatten(), metric_names, colors):\n",
    "    values = [r[1][metric] for r in results_list]\n",
    "    bars = ax.barh(labels, values, color=color, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    ax.set_xlabel(metric)\n",
    "    ax.set_title(metric)\n",
    "    # Ajouter les valeurs sur les barres\n",
    "    for bar, val in zip(bars, values):\n",
    "        ax.text(bar.get_width() * 1.01, bar.get_y() + bar.get_height()/2, \n",
    "                f'{val:.3f}', va='center', fontsize=8)\n",
    "    ax.tick_params(axis='y', labelsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Notes d'interprÃ©tation:\")\n",
    "print(\"  - Cosine Similarity : plus proche de 1 = plus similaire\")\n",
    "print(\"  - L2 Distance       : plus proche de 0 = plus similaire\")\n",
    "print(\"  - L1 Distance       : plus proche de 0 = plus similaire\")\n",
    "print(\"  - Inner Product     : plus grand = plus similaire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap des mÃ©triques normalisÃ©es pour tous les documents (12 premiers)\n",
    "n_docs = 12\n",
    "subset = embeddings[:n_docs]\n",
    "\n",
    "# Calcul de toutes les mÃ©triques pour la heatmap\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Cosine (sur vecteurs normalisÃ©s = produit scalaire)\n",
    "normalized = normalize(subset)\n",
    "cosine_mat = normalized @ normalized.T\n",
    "\n",
    "# L2 distances\n",
    "from sklearn.metrics import pairwise_distances\n",
    "l2_mat = pairwise_distances(subset, metric='euclidean')\n",
    "# Convertir en similaritÃ© (0=similaire) â†’ normalisÃ© entre 0 et 1\n",
    "l2_sim = 1 - (l2_mat / l2_mat.max())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "categories = df['category'].values[:n_docs]\n",
    "doc_labels = [f\"D{i+1}\\n[{c[:3]}]\" for i, c in enumerate(categories)]\n",
    "\n",
    "# Cosine heatmap\n",
    "im1 = axes[0].imshow(cosine_mat, cmap='RdYlGn', vmin=-0.1, vmax=1.0)\n",
    "axes[0].set_title('SimilaritÃ© Cosinus', fontsize=13)\n",
    "axes[0].set_xticks(range(n_docs))\n",
    "axes[0].set_yticks(range(n_docs))\n",
    "axes[0].set_xticklabels(doc_labels, fontsize=7)\n",
    "axes[0].set_yticklabels(doc_labels, fontsize=7)\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# L2 heatmap (normalisÃ©e en similaritÃ©)\n",
    "im2 = axes[1].imshow(l2_sim, cmap='RdYlGn', vmin=0.0, vmax=1.0)\n",
    "axes[1].set_title('SimilaritÃ© L2 (normalisÃ©e)', fontsize=13)\n",
    "axes[1].set_xticks(range(n_docs))\n",
    "axes[1].set_yticks(range(n_docs))\n",
    "axes[1].set_xticklabels(doc_labels, fontsize=7)\n",
    "axes[1].set_yticklabels(doc_labels, fontsize=7)\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.suptitle('Comparaison Cosine vs L2 Similarity\\n(vert = similaire, rouge = diffÃ©rent)', \n",
    "             fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ’¡ Les deux heatmaps montrent des patterns similaires sur des vecteurs quasi-normalisÃ©s.\")\n",
    "print(\"   Des diffÃ©rences apparaissent sur des embeddings non normalisÃ©s (ex: Word2Vec brut).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ”¬ Section 5 â€” FAISS : ThÃ©orie Approfondie\n",
    "\n",
    "## Qu'est-ce que FAISS ?\n",
    "\n",
    "**FAISS** (Facebook AI Similarity Search) est une bibliothÃ¨que open source dÃ©veloppÃ©e par **Meta AI Research** en 2017. Elle fournit des algorithmes efficaces pour la **recherche de similaritÃ©** dans de grandes collections de vecteurs denses.\n",
    "\n",
    "### CaractÃ©ristiques clÃ©s\n",
    "- ğŸš€ OptimisÃ©e pour les GPU (et CPU)\n",
    "- ğŸ“¦ ImplÃ©mentÃ©e en C++ avec une interface Python\n",
    "- ğŸ¯ Supporte des milliards de vecteurs\n",
    "- ğŸ› ï¸ Multiples types d'index selon les besoins\n",
    "- ğŸ“Š MÃ©triques : L2 (euclidienne) et Inner Product\n",
    "\n",
    "---\n",
    "\n",
    "## Les Principes de l'ANN (Approximate Nearest Neighbor)\n",
    "\n",
    "Le problÃ¨me fondamental : Ã©tant donnÃ© N vecteurs de dimension D et une requÃªte q, trouver les k vecteurs les plus proches.\n",
    "\n",
    "**Approche naÃ¯ve (Brute Force)** : comparer q avec tous les N vecteurs â†’ $O(N \\cdot D)$ opÃ©rations.\n",
    "\n",
    "Pour N=10M, D=768 : **7.68 milliards** d'opÃ©rations par requÃªte â†’ trop lent !\n",
    "\n",
    "Les algorithmes ANN utilisent des **structures de donnÃ©es prÃ©-construites** pour rÃ©duire le nombre de comparaisons nÃ©cessaires, au prix d'une lÃ©gÃ¨re perte de prÃ©cision.\n",
    "\n",
    "---\n",
    "\n",
    "## Les 4 Grands Types d'Index FAISS\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“¦ 1. IndexFlatL2 / IndexFlatIP â€” Recherche Exacte par Force Brute\n",
    "\n",
    "#### Principe\n",
    "C'est le plus simple : **comparer la requÃªte avec TOUS les vecteurs** stockÃ©s, sans aucune optimisation. FAISS utilise nÃ©anmoins du SIMD (SSE/AVX) pour vectoriser les calculs et les GPU si disponibles.\n",
    "\n",
    "#### Algorithme\n",
    "```\n",
    "Pour chaque requÃªte q:\n",
    "    Pour chaque vecteur v_i dans l'index:\n",
    "        Calculer d(q, v_i)\n",
    "    Retourner les k vecteurs avec la plus petite distance\n",
    "```\n",
    "\n",
    "#### CaractÃ©ristiques\n",
    "| Attribut | Valeur |\n",
    "|----------|--------|\n",
    "| ComplexitÃ© | $O(N \\cdot D)$ |\n",
    "| PrÃ©cision | **100% exact** |\n",
    "| EntraÃ®nement | Non requis |\n",
    "| MÃ©moire | $N \\cdot D \\cdot 4$ bytes (float32) |\n",
    "| ScalabilitÃ© | Jusqu'Ã  ~1M vecteurs (dÃ©pend du temps tolÃ©rable) |\n",
    "\n",
    "#### Quand utiliser ?\n",
    "âœ… Datasets < 100K vecteurs  \n",
    "âœ… Quand la prÃ©cision absolue est requise  \n",
    "âœ… Pour valider d'autres index (benchmark de rÃ©fÃ©rence)  \n",
    "âœ… Prototypage rapide  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ—‚ï¸ 2. IndexIVFFlat â€” Inverted File Index\n",
    "\n",
    "#### Principe\n",
    "\n",
    "L'idÃ©e vient de l'**index inversÃ©** en recherche d'information (comme dans un moteur de recherche), adaptÃ© Ã  la gÃ©omÃ©trie vectorielle.\n",
    "\n",
    "**Ã‰tape d'entraÃ®nement :** Appliquer k-means sur les N vecteurs pour crÃ©er `nlist` **centroides** (cellules de VoronoÃ¯). Chaque vecteur est assignÃ© Ã  son centroÃ¯de le plus proche.\n",
    "\n",
    "**Ã‰tape de recherche :** Au lieu de parcourir tous les N vecteurs :\n",
    "1. Trouver les `nprobe` cellules de VoronoÃ¯ les plus proches de la requÃªte\n",
    "2. Chercher uniquement dans ces cellules\n",
    "3. Retourner les k meilleurs vecteurs trouvÃ©s\n",
    "\n",
    "#### Visualisation des cellules de VoronoÃ¯\n",
    "\n",
    "```\n",
    "Espace vectoriel 2D\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Ã—   Ã—  â”‚  Ã—   Ã— â”‚ Ã—       â”‚\n",
    "â”‚    â˜…â‚   â”‚   â˜…â‚‚   â”‚  â˜…â‚ƒ    â”‚\n",
    "â”‚ Ã—  Ã— Ã— â”‚Ã— Ã—  Ã— â”‚   Ã—  Ã—  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”‚\n",
    "â”‚     Ã—   â”‚   Ã—  Ã—â”‚  Ã—  Ã—   â”‚\n",
    "â”‚   â˜…â‚„    â”‚   â˜…â‚…  â”‚   â˜…â‚†   â”‚\n",
    "â”‚Ã— Ã—  Ã—   â”‚ Ã—   Ã— â”‚Ã—   Ã—  Ã— â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "â˜… = centroÃ¯de,  Ã— = vecteur,  â”‚ = frontiÃ¨re VoronoÃ¯\n",
    "\n",
    "Pour une requÃªte q proche de â˜…â‚‚:\n",
    "â†’ Chercher uniquement dans les cellules de â˜…â‚‚ (et â˜…â‚,â˜…â‚ƒ si nprobe=3)\n",
    "â†’ Ã‰viter de chercher dans â˜…â‚„,â˜…â‚…,â˜…â‚†\n",
    "```\n",
    "\n",
    "#### HyperparamÃ¨tres clÃ©s\n",
    "\n",
    "| ParamÃ¨tre | RÃ´le | Valeur typique |\n",
    "|-----------|------|----------------|\n",
    "| `nlist` | Nombre de cellules VoronoÃ¯ | $\\sqrt{N}$ Ã  $4\\sqrt{N}$ |\n",
    "| `nprobe` | Cellules explorÃ©es Ã  la requÃªte | 1-64 (plus = plus prÃ©cis, plus lent) |\n",
    "\n",
    "#### RÃ¨gle empirique FAISS\n",
    "- EntraÃ®nement : il faut **au moins 39 Ã— nlist** vecteurs (idÃ©alement 256 Ã— nlist)\n",
    "- Pour N=1M : nlist=1000, nprobe=10-50 est un bon point de dÃ©part\n",
    "\n",
    "#### Quand utiliser ?\n",
    "âœ… Datasets 100K Ã  10M vecteurs  \n",
    "âœ… Bon compromis vitesse/prÃ©cision  \n",
    "âœ… Quand les donnÃ©es sont disponibles pour l'entraÃ®nement  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ—œï¸ 3. IndexIVFPQ â€” IVF + Product Quantization\n",
    "\n",
    "#### Principe\n",
    "\n",
    "Combine l'IVF avec la **quantification par produit (PQ)** pour une **compression massive** des vecteurs.\n",
    "\n",
    "**Product Quantization :**\n",
    "1. Diviser le vecteur de dimension D en `M` sous-vecteurs de dimension D/M\n",
    "2. Pour chaque sous-espace : apprendre un codebook de `ksub` centroides via k-means\n",
    "3. Encoder chaque vecteur comme M indices de centroÃ¯des â†’ vecteur de M octets !\n",
    "\n",
    "$$\\vec{v} \\in \\mathbb{R}^D \\rightarrow (c_1, c_2, ..., c_M) \\in \\{0,...,255\\}^M$$\n",
    "\n",
    "**Exemple de compression :**\n",
    "- Vecteur original : 768 dims Ã— 4 bytes = **3072 bytes**\n",
    "- AprÃ¨s PQ (M=96, ksub=256) : 96 Ã— 1 byte = **96 bytes** â†’ compression **32Ã—** !\n",
    "\n",
    "#### Visualisation de PQ\n",
    "\n",
    "```\n",
    "Vecteur original 768-dim:\n",
    "[vâ‚, vâ‚‚, ..., vâ‚â‚‚â‚ˆ | vâ‚â‚‚â‚‰, ..., vâ‚‚â‚…â‚† | ... | vâ‚†â‚„â‚, ..., vâ‚‡â‚†â‚ˆ]\n",
    "     sous-vecteur 1       sous-vecteur 2         sous-vecteur 6\n",
    "\n",
    "AprÃ¨s PQ :\n",
    "[câ‚ | câ‚‚ | câ‚ƒ | câ‚„ | câ‚… | câ‚†]  (6 codes d'un octet chacun)\n",
    " â†‘ index centroÃ¯de dans codebook Mâ‚\n",
    "```\n",
    "\n",
    "#### HyperparamÃ¨tres\n",
    "\n",
    "| ParamÃ¨tre | RÃ´le | Valeur typique |\n",
    "|-----------|------|----------------|\n",
    "| `M` | Nombre de sous-vecteurs | 8, 16, 32, 64, 96 |\n",
    "| `nbits` | Bits par code (ksub=2^nbits) | 8 (256 centroides par sous-espace) |\n",
    "| `nlist` | Cellules IVF | $\\sqrt{N}$ |\n",
    "\n",
    "#### Quand utiliser ?\n",
    "âœ… Datasets > 10M vecteurs  \n",
    "âœ… Contraintes mÃ©moire strictes  \n",
    "âœ… Trade-off mÃ©moire/prÃ©cision acceptÃ©  \n",
    "âš ï¸ Rappel infÃ©rieur Ã  IVFFlat (typiquement 90-98%)  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ•¸ï¸ 4. IndexHNSWFlat â€” Hierarchical Navigable Small World\n",
    "\n",
    "#### Principe\n",
    "\n",
    "HNSW est basÃ© sur les **graphes \"petit monde\"** (small world graphs). L'idÃ©e vient du phÃ©nomÃ¨ne des \"six degrÃ©s de sÃ©paration\" : dans un rÃ©seau social, n'importe qui peut Ãªtre rejoint en quelques Ã©tapes.\n",
    "\n",
    "**Construction :**\n",
    "1. Chaque vecteur devient un nÅ“ud dans un graphe multi-couches\n",
    "2. Les connexions (arÃªtes) sont crÃ©Ã©es vers les voisins proches\n",
    "3. Les couches supÃ©rieures = liens \"longue distance\" (navigation rapide)\n",
    "4. Les couches infÃ©rieures = liens \"courte distance\" (prÃ©cision fine)\n",
    "\n",
    "**Recherche :**\n",
    "1. Entrer par le haut (longue portÃ©e) et descendre vers la requÃªte\n",
    "2. Greedy search : toujours aller vers le nÅ“ud le plus proche de q\n",
    "3. Affiner au niveau le plus bas\n",
    "\n",
    "#### Structure en couches\n",
    "\n",
    "```\n",
    "Couche 2 (peu de nÅ“uds, liens longue distance):\n",
    "    A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ F\n",
    "    \n",
    "Couche 1 (plus de nÅ“uds):\n",
    "    A â”€â”€â”€â”€ C â”€â”€â”€â”€ F\n",
    "           |\n",
    "           D\n",
    "           \n",
    "Couche 0 (tous les nÅ“uds, liens locaux):\n",
    "    A â”€ B â”€ C â”€ D â”€ E â”€ F\n",
    "    \n",
    "RequÃªte q proche de E:\n",
    "â†’ Entrer en A (couche 2), aller vers F (plus proche de q)\n",
    "â†’ Descendre en couche 1: Fâ†’D (plus proche de q)\n",
    "â†’ Couche 0: Dâ†’E (solution!)\n",
    "```\n",
    "\n",
    "#### HyperparamÃ¨tres\n",
    "\n",
    "| ParamÃ¨tre | RÃ´le | Valeur typique |\n",
    "|-----------|------|----------------|\n",
    "| `M` | Connexions par nÅ“ud | 16-64 (plus = meilleur recall, plus de mÃ©moire) |\n",
    "| `efConstruction` | QualitÃ© de construction | 40-200 (plus = meilleur index, plus lent Ã  construire) |\n",
    "| `efSearch` | QualitÃ© de recherche | 16-512 (plus = meilleur recall, plus lent) |\n",
    "\n",
    "#### Quand utiliser HNSW ?\n",
    "âœ… RequÃªtes trÃ¨s basses latences requises  \n",
    "âœ… Bon recall sans trop de tuning  \n",
    "âœ… DonnÃ©es < 10M vecteurs (mÃ©moire)  \n",
    "âš ï¸ **Consommation mÃ©moire Ã©levÃ©e** : N Ã— M Ã— 4 bytes pour les graphes  \n",
    "âš ï¸ Construction plus lente que IVF  \n",
    "\n",
    "---\n",
    "\n",
    "## Tableau Comparatif Complet des Index FAISS\n",
    "\n",
    "| Index | Exact | EntraÃ®nement | MÃ©moire | Build Time | Search Time | Recall | Cas d'usage |\n",
    "|-------|-------|-------------|---------|------------|-------------|--------|-------------|\n",
    "| **IndexFlatL2** | âœ… 100% | Non | $N \\times D \\times 4$ bytes | ImmÃ©diat | $O(N\\cdot D)$ | 100% | <100K vecteurs, prÃ©cision absolue |\n",
    "| **IndexFlatIP** | âœ… 100% | Non | $N \\times D \\times 4$ bytes | ImmÃ©diat | $O(N\\cdot D)$ | 100% | Comme Flat L2, cosine search |\n",
    "| **IndexIVFFlat** | âŒ ANN | Oui (k-means) | $N \\times D \\times 4$ bytes | Moyen | $O(nprobe \\times N/nlist \\times D)$ | 95-99% | 100K-10M vecteurs |\n",
    "| **IndexIVFPQ** | âŒ ANN | Oui (k-means + PQ) | $N \\times M$ bytes | Long | TrÃ¨s rapide | 90-98% | >10M vecteurs, contrainte mÃ©moire |\n",
    "| **IndexHNSWFlat** | âŒ ANN | Non (graph) | $N \\times D \\times 4 + graph$ | Long | $O(\\log N)$ | 97-99% | Basse latence, <10M vecteurs |\n",
    "\n",
    "---\n",
    "\n",
    "## Guide de SÃ©lection d'Index FAISS\n",
    "\n",
    "```\n",
    "Combien de vecteurs ?\n",
    "â”‚\n",
    "â”œâ”€ < 100K â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ IndexFlatL2 (exact, simple)\n",
    "â”‚\n",
    "â”œâ”€ 100K Ã  1M â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ IndexIVFFlat (bon compromis)\n",
    "â”‚                              nlist = sqrt(N), nprobe = 10-50\n",
    "â”‚\n",
    "â”œâ”€ 1M Ã  10M\n",
    "â”‚   â”œâ”€ MÃ©moire OK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ IndexHNSWFlat (basse latence)\n",
    "â”‚   â””â”€ MÃ©moire limitÃ©e â”€â”€â”€â”€â”€â†’ IndexIVFFlat ou IndexIVFPQ\n",
    "â”‚\n",
    "â””â”€ > 10M â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ IndexIVFPQ (compression requise)\n",
    "                               ou moteur dÃ©diÃ© (Milvus, Qdrant)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. FAISS: Index de Recherche Vectorielle\n",
    "\n",
    "### 5.1 Index Flat (Brute Force)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index_flat(embeddings: np.ndarray) -> faiss.IndexFlatL2:\n",
    "    \"\"\"\n",
    "    CrÃ©er un index FAISS Flat (recherche exhaustive)\n",
    "    \"\"\"\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)  # L2 distance\n",
    "    index.add(embeddings)  # Ajouter les vecteurs\n",
    "    return index\n",
    "\n",
    "# CrÃ©er l'index\n",
    "index_flat = create_faiss_index_flat(embeddings)\n",
    "\n",
    "print(f\"âœ… Index Flat crÃ©Ã©\")\n",
    "print(f\"   - Nombre de vecteurs: {index_flat.ntotal}\")\n",
    "print(f\"   - Dimension: {index_flat.d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Recherche de similaritÃ©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar(query: str, index: faiss.Index, model: SentenceTransformer, \n",
    "                   df: pd.DataFrame, k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rechercher les k documents les plus similaires Ã  une query\n",
    "    \"\"\"\n",
    "    # CrÃ©er l'embedding de la query\n",
    "    query_embedding = model.encode([query]).astype('float32')\n",
    "    \n",
    "    # Rechercher les k plus proches voisins\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    # CrÃ©er un DataFrame des rÃ©sultats\n",
    "    results = []\n",
    "    for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "        results.append({\n",
    "            \"rank\": i + 1,\n",
    "            \"id\": df.iloc[idx]['id'],\n",
    "            \"text\": df.iloc[idx]['text'],\n",
    "            \"category\": df.iloc[idx]['category'],\n",
    "            \"distance\": dist,\n",
    "            \"similarity\": 1 / (1 + dist)  # Approximation de similaritÃ©\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Test de recherche\n",
    "query = \"Comment fonctionne le deep learning?\"\n",
    "print(f\"ğŸ” Query: {query}\\n\")\n",
    "\n",
    "results = search_similar(query, index_flat, embedding_model, df, k=5)\n",
    "print(\"ğŸ“Š Top 5 rÃ©sultats:\")\n",
    "print(results[['rank', 'text', 'category', 'similarity']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Tests avec diffÃ©rentes queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    \"Qu'est-ce qu'un transformer?\",\n",
    "    \"Comment dÃ©ployer une application?\",\n",
    "    \"Python pour l'analyse de donnÃ©es\",\n",
    "    \"Recherche vectorielle et embeddings\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸ” Query: {query}\")\n",
    "    print('='*80)\n",
    "    \n",
    "    results = search_similar(query, index_flat, embedding_model, df, k=3)\n",
    "    \n",
    "    for _, row in results.iterrows():\n",
    "        print(f\"{row['rank']}. [{row['category']}] {row['text'][:60]}... (sim: {row['similarity']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Index IVFPQ (OptimisÃ© pour Large Scale)\n",
    "\n",
    "### 6.1 CrÃ©ation d'un index IVF (Inverted File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index_ivf(embeddings: np.ndarray, nlist: int = 10) -> faiss.IndexIVFFlat:\n",
    "    \"\"\"\n",
    "    CrÃ©er un index FAISS IVF (plus rapide pour large scale)\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Vecteurs Ã  indexer\n",
    "        nlist: Nombre de clusters (voronoi cells)\n",
    "    \"\"\"\n",
    "    dimension = embeddings.shape[1]\n",
    "    \n",
    "    # CrÃ©er le quantizer (index flat pour les centroids)\n",
    "    quantizer = faiss.IndexFlatL2(dimension)\n",
    "    \n",
    "    # CrÃ©er l'index IVF\n",
    "    index = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n",
    "    \n",
    "    # EntraÃ®ner l'index (clustering)\n",
    "    index.train(embeddings)\n",
    "    \n",
    "    # Ajouter les vecteurs\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return index\n",
    "\n",
    "# Pour notre petit dataset, utilisons nlist=5\n",
    "index_ivf = create_faiss_index_ivf(embeddings, nlist=5)\n",
    "\n",
    "print(f\"âœ… Index IVF crÃ©Ã©\")\n",
    "print(f\"   - Nombre de vecteurs: {index_ivf.ntotal}\")\n",
    "print(f\"   - Nombre de clusters: {index_ivf.nlist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Comparaison Flat vs IVF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_search(query: str, index: faiss.Index, model: SentenceTransformer, k: int = 5):\n",
    "    \"\"\"\n",
    "    Mesurer le temps de recherche\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query]).astype('float32')\n",
    "    \n",
    "    start = time.time()\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    return elapsed * 1000  # en ms\n",
    "\n",
    "# Benchmark\n",
    "query = \"Qu'est-ce que le machine learning?\"\n",
    "\n",
    "time_flat = benchmark_search(query, index_flat, embedding_model)\n",
    "time_ivf = benchmark_search(query, index_ivf, embedding_model)\n",
    "\n",
    "print(f\"â±ï¸ Temps de recherche:\")\n",
    "print(f\"   - Flat (exact): {time_flat:.3f} ms\")\n",
    "print(f\"   - IVF (approx): {time_ivf:.3f} ms\")\n",
    "print(f\"   - Speedup: {time_flat/time_ivf:.2f}x\")\n",
    "print(f\"\\nâš ï¸ Note: Sur de petits datasets, Flat peut Ãªtre plus rapide.\")\n",
    "print(f\"   IVF devient intÃ©ressant Ã  partir de 10k-100k+ vecteurs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ã‰valuation: Recall et Precision\n",
    "\n",
    "### 7.1 CrÃ©ation d'un ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DÃ©finir des queries avec leurs documents pertinents attendus (ground truth)\n",
    "ground_truth = [\n",
    "    {\n",
    "        \"query\": \"Qu'est-ce qu'un transformer?\",\n",
    "        \"relevant_ids\": [5, 6, 15]  # Docs sur transformers, BERT, attention\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Comment utiliser Python?\",\n",
    "        \"relevant_ids\": [3, 4, 16]  # Docs sur Python, FastAPI, Pandas\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Machine learning et optimisation\",\n",
    "        \"relevant_ids\": [1, 7, 12]  # Docs sur ML, gradient descent, fine-tuning\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Recherche vectorielle\",\n",
    "        \"relevant_ids\": [8, 18, 19]  # Docs sur embeddings, FAISS, vector DB\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"âœ… Ground truth dÃ©fini pour l'Ã©valuation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Calcul de Recall@k et Precision@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(query: str, relevant_ids: List[int], index: faiss.Index, \n",
    "                      model: SentenceTransformer, df: pd.DataFrame, k: int = 5):\n",
    "    \"\"\"\n",
    "    Calculer Recall@k et Precision@k\n",
    "    \"\"\"\n",
    "    # RÃ©cupÃ©rer les rÃ©sultats\n",
    "    query_embedding = model.encode([query]).astype('float32')\n",
    "    _, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    # IDs rÃ©cupÃ©rÃ©s\n",
    "    retrieved_ids = [df.iloc[idx]['id'] for idx in indices[0]]\n",
    "    \n",
    "    # Calculer les mÃ©triques\n",
    "    relevant_retrieved = set(relevant_ids) & set(retrieved_ids)\n",
    "    \n",
    "    recall = len(relevant_retrieved) / len(relevant_ids) if relevant_ids else 0\n",
    "    precision = len(relevant_retrieved) / k\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"k\": k,\n",
    "        \"relevant_ids\": relevant_ids,\n",
    "        \"retrieved_ids\": retrieved_ids,\n",
    "        \"relevant_retrieved\": list(relevant_retrieved),\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision\n",
    "    }\n",
    "\n",
    "# Ã‰valuer toutes les queries\n",
    "evaluation_results = []\n",
    "\n",
    "for item in ground_truth:\n",
    "    result = evaluate_retrieval(\n",
    "        query=item[\"query\"],\n",
    "        relevant_ids=item[\"relevant_ids\"],\n",
    "        index=index_flat,\n",
    "        model=embedding_model,\n",
    "        df=df,\n",
    "        k=5\n",
    "    )\n",
    "    evaluation_results.append(result)\n",
    "    \n",
    "    print(f\"\\nğŸ” Query: {result['query']}\")\n",
    "    print(f\"   Relevant: {result['relevant_ids']}\")\n",
    "    print(f\"   Retrieved: {result['retrieved_ids']}\")\n",
    "    print(f\"   âœ… Found: {result['relevant_retrieved']}\")\n",
    "    print(f\"   ğŸ“Š Recall@{result['k']}: {result['recall']:.2%}\")\n",
    "    print(f\"   ğŸ“Š Precision@{result['k']}: {result['precision']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 MÃ©triques moyennes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_recall = np.mean([r['recall'] for r in evaluation_results])\n",
    "avg_precision = np.mean([r['precision'] for r in evaluation_results])\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ğŸ“Š MÃ‰TRIQUES MOYENNES\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Recall@5:    {avg_recall:.2%}\")\n",
    "print(f\"Precision@5: {avg_precision:.2%}\")\n",
    "\n",
    "# Visualisation\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "queries = [r['query'][:30] + '...' for r in evaluation_results]\n",
    "recalls = [r['recall'] for r in evaluation_results]\n",
    "precisions = [r['precision'] for r in evaluation_results]\n",
    "\n",
    "ax[0].barh(queries, recalls, color='skyblue')\n",
    "ax[0].set_xlabel('Recall@5')\n",
    "ax[0].set_title('Recall par Query')\n",
    "ax[0].set_xlim(0, 1)\n",
    "\n",
    "ax[1].barh(queries, precisions, color='lightcoral')\n",
    "ax[1].set_xlabel('Precision@5')\n",
    "ax[1].set_title('Precision par Query')\n",
    "ax[1].set_xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sauvegarde et Chargement de l'Index\n",
    "\n",
    "### 8.1 Sauvegarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_index(index: faiss.Index, embeddings: np.ndarray, df: pd.DataFrame, \n",
    "               index_path: str = \"faiss_index.bin\", \n",
    "               data_path: str = \"documents.pkl\"):\n",
    "    \"\"\"\n",
    "    Sauvegarder l'index FAISS et les donnÃ©es associÃ©es\n",
    "    \"\"\"\n",
    "    # Sauvegarder l'index FAISS\n",
    "    faiss.write_index(index, index_path)\n",
    "    \n",
    "    # Sauvegarder les donnÃ©es (documents + embeddings)\n",
    "    data = {\n",
    "        \"documents\": df,\n",
    "        \"embeddings\": embeddings\n",
    "    }\n",
    "    with open(data_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    \n",
    "    print(f\"âœ… Index sauvegardÃ©: {index_path}\")\n",
    "    print(f\"âœ… DonnÃ©es sauvegardÃ©es: {data_path}\")\n",
    "\n",
    "# Sauvegarder\n",
    "save_index(index_flat, embeddings, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Chargement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index(index_path: str = \"faiss_index.bin\", \n",
    "               data_path: str = \"documents.pkl\"):\n",
    "    \"\"\"\n",
    "    Charger l'index FAISS et les donnÃ©es associÃ©es\n",
    "    \"\"\"\n",
    "    # Charger l'index\n",
    "    index = faiss.read_index(index_path)\n",
    "    \n",
    "    # Charger les donnÃ©es\n",
    "    with open(data_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    print(f\"âœ… Index chargÃ©: {index.ntotal} vecteurs\")\n",
    "    print(f\"âœ… DonnÃ©es chargÃ©es: {len(data['documents'])} documents\")\n",
    "    \n",
    "    return index, data['documents'], data['embeddings']\n",
    "\n",
    "# Test de chargement\n",
    "loaded_index, loaded_df, loaded_embeddings = load_index()\n",
    "\n",
    "# VÃ©rifier que Ã§a fonctionne\n",
    "test_query = \"Qu'est-ce que l'IA?\"\n",
    "test_results = search_similar(test_query, loaded_index, embedding_model, loaded_df, k=3)\n",
    "print(f\"\\nğŸ” Test query: {test_query}\")\n",
    "print(test_results[['rank', 'text']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercices Pratiques\n",
    "\n",
    "### Exercice 1: Ajouter de nouveaux documents\n",
    "1. CrÃ©er 10 nouveaux documents\n",
    "2. GÃ©nÃ©rer leurs embeddings\n",
    "3. Les ajouter Ã  l'index existant\n",
    "4. Tester la recherche\n",
    "\n",
    "### Exercice 2: Optimiser les hyperparamÃ¨tres\n",
    "1. Tester diffÃ©rentes valeurs de `nlist` pour IVF\n",
    "2. Mesurer l'impact sur la vitesse et le recall\n",
    "3. Trouver le meilleur compromis\n",
    "\n",
    "### Exercice 3: ImplÃ©menter un filtre de mÃ©tadonnÃ©es\n",
    "1. Modifier `search_similar` pour filtrer par catÃ©gorie\n",
    "2. Exemple: \"Chercher uniquement dans les docs NLP\"\n",
    "3. Comparer les rÃ©sultats avec/sans filtre\n",
    "\n",
    "### Exercice 4: Ã‰valuation avancÃ©e\n",
    "1. ImplÃ©menter Mean Reciprocal Rank (MRR)\n",
    "2. Calculer NDCG@k\n",
    "3. Comparer Flat vs IVF sur ces mÃ©triques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Concepts AvancÃ©s\n",
    "\n",
    "### 10.1 Types d'index FAISS\n",
    "\n",
    "| Type | Description | Usage |\n",
    "|------|-------------|-------|\n",
    "| **Flat** | Brute force, exact | < 10K vecteurs, besoin de prÃ©cision maximale |\n",
    "| **IVF** | Inverted file, approximatif | 10K - 10M vecteurs |\n",
    "| **IVFPQ** | IVF + Product Quantization | 10M+ vecteurs, compression |\n",
    "| **HNSW** | Hierarchical NSW graph | TrÃ¨s rapide, mÃ©moire Ã©levÃ©e |\n",
    "\n",
    "### 10.2 MÃ©triques de distance\n",
    "- **L2 (Euclidean)**: Distance euclidienne classique\n",
    "- **Inner Product**: Produit scalaire (pour vecteurs normalisÃ©s = cosine)\n",
    "- **Cosine**: Angle entre vecteurs\n",
    "\n",
    "### 10.3 Librairies alternatives\n",
    "- **Milvus**: Vector DB distribuÃ©e, production-ready\n",
    "- **Weaviate**: Vector DB avec GraphQL\n",
    "- **Pinecone**: Vector DB managÃ©e (cloud)\n",
    "- **Qdrant**: Vector DB en Rust, performante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ§­ Section 11 â€” Guide de DÃ©cision Global\n",
    "\n",
    "## 11.1 Choisir le Bon ModÃ¨le d'Embedding\n",
    "\n",
    "```\n",
    "Quel type de donnÃ©es ?\n",
    "â”‚\n",
    "â”œâ”€ Texte franÃ§ais/multilingue â”€â”€â”€â”€â”€â†’ paraphrase-multilingual-MiniLM-L12-v2\n",
    "â”‚                                     ou multilingual-e5-large\n",
    "â”‚\n",
    "â”œâ”€ Texte anglais haute qualitÃ© â”€â”€â”€â”€â†’ sentence-t5-xxl, E5-large-v2\n",
    "â”‚                                     ou text-embedding-3-large (OpenAI)\n",
    "â”‚\n",
    "â”œâ”€ Texte anglais efficace â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ all-MiniLM-L6-v2 (rapide, 384 dims)\n",
    "â”‚                                     ou text-embedding-3-small (OpenAI)\n",
    "â”‚\n",
    "â”œâ”€ Images â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ CLIP (ViT-L/14), DINOv2\n",
    "â”‚\n",
    "â”œâ”€ Images + Texte â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ CLIP (mÃªme espace vectoriel !)\n",
    "â”‚\n",
    "â””â”€ Domaine spÃ©cifique â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Fine-tuner un modÃ¨le gÃ©nÃ©ral\n",
    "   (mÃ©dical, juridique, code...)       sur vos donnÃ©es\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 11.2 Choisir la Bonne MÃ©trique de SimilaritÃ©\n",
    "\n",
    "| Situation | MÃ©trique recommandÃ©e | Raison |\n",
    "|-----------|---------------------|--------|\n",
    "| Sentence-BERT, paraphrase models | **Cosine** | EntraÃ®nÃ© pour cosine |\n",
    "| OpenAI embeddings | **Cosine** ou **IP** | Vecteurs normalisÃ©s |\n",
    "| Word2Vec raw | **Cosine** | Magnitude non significative |\n",
    "| Embeddings binaires | **Hamming** | Comparaison bit-Ã -bit |\n",
    "| SystÃ¨mes de recommandation | **Inner Product** | Magnitude = popularitÃ© |\n",
    "| Images features | **L2** | Distance euclidienne naturelle |\n",
    "| Documents bag-of-words | **Jaccard** ou **Cosine** | SimilaritÃ© d'ensembles |\n",
    "\n",
    "---\n",
    "\n",
    "## 11.3 Choisir le Bon Index FAISS\n",
    "\n",
    "| Taille dataset | Contrainte mÃ©moire | PrioritÃ© | Index recommandÃ© |\n",
    "|---------------|-------------------|----------|------------------|\n",
    "| < 10K | Quelconque | PrÃ©cision | **IndexFlatL2** |\n",
    "| 10K - 500K | MÃ©moire suffisante | Ã‰quilibre | **IndexIVFFlat** (nlist=âˆšN) |\n",
    "| 500K - 5M | MÃ©moire suffisante | Vitesse | **IndexHNSWFlat** (M=32) |\n",
    "| 500K - 5M | MÃ©moire limitÃ©e | Compression | **IndexIVFPQ** |\n",
    "| > 5M | Quelconque | ScalabilitÃ© | **Milvus / Qdrant** |\n",
    "\n",
    "---\n",
    "\n",
    "## 11.4 Choisir le Bon Moteur Vectoriel\n",
    "\n",
    "| CritÃ¨re | Meilleur choix |\n",
    "|---------|----------------|\n",
    "| Prototypage rapide | FAISS (local) ou Chroma |\n",
    "| Production simple | Qdrant (Docker) |\n",
    "| Grande Ã©chelle (>100M) | Milvus |\n",
    "| Recherche hybride (texte + vecteurs) | Weaviate ou Elasticsearch |\n",
    "| ZÃ©ro maintenance infra | Pinecone (cloud) |\n",
    "| DÃ©jÃ  sur PostgreSQL | pgvector |\n",
    "| Temps rÃ©el (<10ms) | Redis Vector Search |\n",
    "\n",
    "---\n",
    "\n",
    "## 11.5 Tableau de Bord RÃ©capitulatif Complet\n",
    "\n",
    "| Composant | Choix Budget | Choix Standard | Choix Premium |\n",
    "|-----------|-------------|----------------|---------------|\n",
    "| **ModÃ¨le embedding** | all-MiniLM-L6-v2 (384d) | paraphrase-multilingual (384d) | text-embedding-3-large (3072d) |\n",
    "| **Index** | IndexFlatL2 | IndexIVFFlat | IndexHNSWFlat |\n",
    "| **Moteur** | FAISS local | Qdrant | Pinecone / Milvus |\n",
    "| **MÃ©trique** | Cosine | Cosine | Cosine / IP |\n",
    "| **CoÃ»t** | 0â‚¬ | 0â‚¬ (self-hosted) | Variable |\n",
    "| **Nb vecteurs** | <100K | <10M | IllimitÃ© |\n",
    "| **Latence** | ~10ms | ~5ms | <1ms |\n",
    "\n",
    "---\n",
    "\n",
    "## 11.6 Anti-patterns Ã  Ã‰viter\n",
    "\n",
    "| âŒ Erreur frÃ©quente | âœ… Bonne pratique |\n",
    "|--------------------|-----------------|\n",
    "| Utiliser un modÃ¨le diffÃ©rent pour indexation et requÃªte | Toujours le **mÃªme modÃ¨le** dans tout le pipeline |\n",
    "| Indexer les documents sans chunking | DÃ©couper les documents longs (512 tokens max) |\n",
    "| Ignorer la normalisation des vecteurs | Normaliser quand le modÃ¨le l'attend |\n",
    "| Utiliser L2 sans normaliser pour de l'embedding texte | Utiliser Cosine pour du texte |\n",
    "| Fixer nprobe=1 pour IVF | Tester nprobe=10-50 selon le recall cible |\n",
    "| Ne jamais Ã©valuer le recall | Toujours mesurer Recall@k et Precision@k |\n",
    "| RÃ©indexer Ã  chaque mise Ã  jour unitaire | Batch updates ou incremental indexing |\n",
    "\n",
    "---\n",
    "\n",
    "## 11.7 MÃ©triques d'Ã‰valuation ComplÃ¨tes\n",
    "\n",
    "| MÃ©trique | Formule | Ce qu'elle mesure | IdÃ©al pour |\n",
    "|----------|---------|-------------------|-----------|\n",
    "| **Recall@k** | $\\frac{|Pertinents \\cap Top_k|}{|Pertinents|}$ | % de pertinents retrouvÃ©s | RAG, recherche exhaustive |\n",
    "| **Precision@k** | $\\frac{|Pertinents \\cap Top_k|}{k}$ | QualitÃ© du top-k | Quand l'utilisateur voit peu de rÃ©sultats |\n",
    "| **MRR** | $\\frac{1}{N}\\sum\\frac{1}{rank_1}$ | Position du 1er pertinent | Q&A, moteurs de recherche |\n",
    "| **NDCG@k** | $\\frac{DCG@k}{IDCG@k}$ | QualitÃ© ordonnÃ©e des rÃ©sultats | Ranking, moteurs de recherche |\n",
    "| **Hit Rate@k** | $\\frac{|Queries_{recall>0}|}{|Queries|}$ | % de requÃªtes avec au moins 1 rÃ©sultat | Couverture du systÃ¨me |\n",
    "| **MAP** | Moyenne de AP sur toutes les queries | QualitÃ© globale du systÃ¨me | Ã‰valuation complÃ¨te |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“š Ressources\n",
    "\n",
    "### Documentation Officielle\n",
    "- [FAISS Documentation](https://github.com/facebookresearch/faiss/wiki)\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "- [Understanding Vector Search](https://www.pinecone.io/learn/vector-search/)\n",
    "- [Embeddings Guide OpenAI](https://platform.openai.com/docs/guides/embeddings)\n",
    "\n",
    "### Moteurs Vectoriels\n",
    "- [Qdrant Documentation](https://qdrant.tech/documentation/)\n",
    "- [Milvus Documentation](https://milvus.io/docs)\n",
    "- [Weaviate Documentation](https://weaviate.io/developers/weaviate)\n",
    "- [Chroma Documentation](https://docs.trychroma.com/)\n",
    "- [pgvector GitHub](https://github.com/pgvector/pgvector)\n",
    "\n",
    "### Articles et Tutoriels\n",
    "- [ANN Benchmarks](https://ann-benchmarks.com/) â€” Comparaison des algorithmes ANN\n",
    "- [The Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/)\n",
    "- [HNSW Paper (Malkov & Yashunin, 2018)](https://arxiv.org/abs/1603.09320)\n",
    "- [Product Quantization (JÃ©gou et al., 2011)](https://inria.hal.science/inria-00514462v2/document)\n",
    "\n",
    "## âœ… Checklist\n",
    "\n",
    "- [ ] Comprendre ce qu'est un embedding et comment il est crÃ©Ã©\n",
    "- [ ] ConnaÃ®tre les diffÃ©rents types de modÃ¨les d'embeddings\n",
    "- [ ] Comprendre la recherche vectorielle vs recherche par mots-clÃ©s\n",
    "- [ ] MaÃ®triser les mÃ©triques : Cosine, L2, Inner Product, L1\n",
    "- [ ] Comprendre les 4 types d'index FAISS (Flat, IVF, IVFPQ, HNSW)\n",
    "- [ ] Savoir choisir le bon moteur vectoriel selon le contexte\n",
    "- [ ] Embeddings crÃ©Ã©s pour tous les documents\n",
    "- [ ] Index FAISS Flat implÃ©mentÃ© et testÃ©\n",
    "- [ ] Index FAISS IVF implÃ©mentÃ© et testÃ©\n",
    "- [ ] DÃ©monstration comparative des mÃ©triques de distance\n",
    "- [ ] Recall et Precision calculÃ©s\n",
    "- [ ] Index sauvegardÃ© localement\n",
    "- [ ] Benchmark de performance effectuÃ©\n",
    "\n",
    "---\n",
    "\n",
    "**Session S10 complÃ©tÃ©e! ğŸš€**\n",
    "\n",
    "*Prochaine Ã©tape : S11 â€” RAG (Retrieval-Augmented Generation) â€” combiner embeddings + LLM pour des rÃ©ponses basÃ©es sur vos donnÃ©es !*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}