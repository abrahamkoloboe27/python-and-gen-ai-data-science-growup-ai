{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S10 ‚Äî Embeddings & Moteurs de Recherche Vectorielle (RAG Intro)\n",
    "\n",
    "## üéØ Objectifs\n",
    "- Comprendre les embeddings et leur cr√©ation\n",
    "- Ma√Ætriser la recherche de similarit√© (nearest neighbour)\n",
    "- Impl√©menter un index FAISS local\n",
    "- √âvaluer recall et precision du retrieval\n",
    "\n",
    "## üìã Contenu\n",
    "1. Cr√©ation d'embeddings\n",
    "2. Similarity search et m√©triques de distance\n",
    "3. Index types (Flat, IVFPQ)\n",
    "4. Impl√©mentation avec FAISS\n",
    "5. √âvaluation des performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances\n",
    "# !pip install faiss-cpu openai sentence-transformers pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "import pickle\n",
    "\n",
    "print(\"‚úÖ Biblioth√®ques import√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cr√©ation du Dataset\n",
    "\n",
    "Nous allons cr√©er un dataset simul√© de documents techniques pour la d√©monstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de documents (simul√©)\n",
    "documents = [\n",
    "    {\"id\": 1, \"text\": \"Le machine learning est une branche de l'intelligence artificielle.\", \"category\": \"ML\"},\n",
    "    {\"id\": 2, \"text\": \"Les r√©seaux de neurones profonds sont utilis√©s pour l'apprentissage profond.\", \"category\": \"DL\"},\n",
    "    {\"id\": 3, \"text\": \"Python est un langage de programmation populaire pour la data science.\", \"category\": \"Programming\"},\n",
    "    {\"id\": 4, \"text\": \"FastAPI permet de cr√©er des APIs REST rapidement avec Python.\", \"category\": \"Programming\"},\n",
    "    {\"id\": 5, \"text\": \"Les transformers r√©volutionnent le traitement du langage naturel.\", \"category\": \"NLP\"},\n",
    "    {\"id\": 6, \"text\": \"BERT est un mod√®le pr√©-entra√Æn√© bas√© sur l'architecture transformer.\", \"category\": \"NLP\"},\n",
    "    {\"id\": 7, \"text\": \"Le gradient descent est un algorithme d'optimisation fondamental.\", \"category\": \"ML\"},\n",
    "    {\"id\": 8, \"text\": \"Les embeddings capturent la s√©mantique des mots dans un espace vectoriel.\", \"category\": \"NLP\"},\n",
    "    {\"id\": 9, \"text\": \"Docker permet de conteneuriser des applications pour un d√©ploiement facile.\", \"category\": \"DevOps\"},\n",
    "    {\"id\": 10, \"text\": \"Kubernetes orchestre des conteneurs √† grande √©chelle.\", \"category\": \"DevOps\"},\n",
    "    {\"id\": 11, \"text\": \"Les CNNs sont particuli√®rement efficaces pour la vision par ordinateur.\", \"category\": \"DL\"},\n",
    "    {\"id\": 12, \"text\": \"Le fine-tuning adapte un mod√®le pr√©-entra√Æn√© √† une t√¢che sp√©cifique.\", \"category\": \"ML\"},\n",
    "    {\"id\": 13, \"text\": \"GPT-4 est un mod√®le de langage g√©n√©ratif d√©velopp√© par OpenAI.\", \"category\": \"NLP\"},\n",
    "    {\"id\": 14, \"text\": \"Les API REST utilisent HTTP pour communiquer entre services.\", \"category\": \"Programming\"},\n",
    "    {\"id\": 15, \"text\": \"L'attention est le m√©canisme cl√© des architectures transformer.\", \"category\": \"DL\"},\n",
    "    {\"id\": 16, \"text\": \"Pandas est une biblioth√®que Python pour l'analyse de donn√©es.\", \"category\": \"Programming\"},\n",
    "    {\"id\": 17, \"text\": \"Le RAG combine retrieval et g√©n√©ration pour am√©liorer les LLMs.\", \"category\": \"NLP\"},\n",
    "    {\"id\": 18, \"text\": \"FAISS est une biblioth√®que pour la recherche de similarit√© vectorielle.\", \"category\": \"ML\"},\n",
    "    {\"id\": 19, \"text\": \"Les bases de donn√©es vectorielles stockent et recherchent des embeddings.\", \"category\": \"ML\"},\n",
    "    {\"id\": 20, \"text\": \"Le tokenisation d√©coupe le texte en unit√©s traitables par les mod√®les.\", \"category\": \"NLP\"},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(documents)\n",
    "print(f\"üìä Dataset cr√©√©: {len(df)} documents\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. G√©n√©ration d'Embeddings\n",
    "\n",
    "### 3.1 Chargement du mod√®le d'embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger un mod√®le d'embeddings multilingue\n",
    "# Options: 'paraphrase-multilingual-MiniLM-L12-v2', 'distiluse-base-multilingual-cased-v2'\n",
    "embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "print(f\"‚úÖ Mod√®le charg√©: {embedding_model.get_sentence_embedding_dimension()} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Cr√©ation des embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts: List[str], model: SentenceTransformer) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Cr√©er des embeddings pour une liste de textes\n",
    "    \"\"\"\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    return np.array(embeddings).astype('float32')\n",
    "\n",
    "# G√©n√©rer les embeddings\n",
    "texts = df['text'].tolist()\n",
    "embeddings = create_embeddings(texts, embedding_model)\n",
    "\n",
    "print(f\"‚úÖ Embeddings cr√©√©s: shape = {embeddings.shape}\")\n",
    "print(f\"   - {embeddings.shape[0]} documents\")\n",
    "print(f\"   - {embeddings.shape[1]} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. M√©triques de Similarit√©\n",
    "\n",
    "### 4.1 Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(embedding1: np.ndarray, embedding2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculer la similarit√© cosinus entre deux embeddings\n",
    "    \"\"\"\n",
    "    return cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "\n",
    "# Exemple: Comparer deux documents\n",
    "doc1_emb = embeddings[0]  # \"Le machine learning...\"\n",
    "doc2_emb = embeddings[1]  # \"Les r√©seaux de neurones...\"\n",
    "doc3_emb = embeddings[2]  # \"Python est un langage...\"\n",
    "\n",
    "sim_1_2 = compute_cosine_similarity(doc1_emb, doc2_emb)\n",
    "sim_1_3 = compute_cosine_similarity(doc1_emb, doc3_emb)\n",
    "\n",
    "print(f\"Similarit√© entre doc1 et doc2 (tous deux ML): {sim_1_2:.4f}\")\n",
    "print(f\"Similarit√© entre doc1 et doc3 (ML vs Programming): {sim_1_3:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Matrice de similarit√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer la matrice de similarit√© pour les 10 premiers documents\n",
    "sample_embeddings = embeddings[:10]\n",
    "similarity_matrix = cosine_similarity(sample_embeddings)\n",
    "\n",
    "# Visualiser\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(similarity_matrix, cmap='YlOrRd', aspect='auto')\n",
    "plt.colorbar(label='Cosine Similarity')\n",
    "plt.xlabel('Document ID')\n",
    "plt.ylabel('Document ID')\n",
    "plt.title('Matrice de Similarit√© (10 premiers documents)')\n",
    "plt.xticks(range(10), [f\"Doc {i+1}\" for i in range(10)], rotation=45)\n",
    "plt.yticks(range(10), [f\"Doc {i+1}\" for i in range(10)])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. FAISS: Index de Recherche Vectorielle\n",
    "\n",
    "### 5.1 Index Flat (Brute Force)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index_flat(embeddings: np.ndarray) -> faiss.IndexFlatL2:\n",
    "    \"\"\"\n",
    "    Cr√©er un index FAISS Flat (recherche exhaustive)\n",
    "    \"\"\"\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)  # L2 distance\n",
    "    index.add(embeddings)  # Ajouter les vecteurs\n",
    "    return index\n",
    "\n",
    "# Cr√©er l'index\n",
    "index_flat = create_faiss_index_flat(embeddings)\n",
    "\n",
    "print(f\"‚úÖ Index Flat cr√©√©\")\n",
    "print(f\"   - Nombre de vecteurs: {index_flat.ntotal}\")\n",
    "print(f\"   - Dimension: {index_flat.d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Recherche de similarit√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar(query: str, index: faiss.Index, model: SentenceTransformer, \n",
    "                   df: pd.DataFrame, k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rechercher les k documents les plus similaires √† une query\n",
    "    \"\"\"\n",
    "    # Cr√©er l'embedding de la query\n",
    "    query_embedding = model.encode([query]).astype('float32')\n",
    "    \n",
    "    # Rechercher les k plus proches voisins\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    # Cr√©er un DataFrame des r√©sultats\n",
    "    results = []\n",
    "    for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "        results.append({\n",
    "            \"rank\": i + 1,\n",
    "            \"id\": df.iloc[idx]['id'],\n",
    "            \"text\": df.iloc[idx]['text'],\n",
    "            \"category\": df.iloc[idx]['category'],\n",
    "            \"distance\": dist,\n",
    "            \"similarity\": 1 / (1 + dist)  # Approximation de similarit√©\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Test de recherche\n",
    "query = \"Comment fonctionne le deep learning?\"\n",
    "print(f\"üîç Query: {query}\\n\")\n",
    "\n",
    "results = search_similar(query, index_flat, embedding_model, df, k=5)\n",
    "print(\"üìä Top 5 r√©sultats:\")\n",
    "print(results[['rank', 'text', 'category', 'similarity']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Tests avec diff√©rentes queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    \"Qu'est-ce qu'un transformer?\",\n",
    "    \"Comment d√©ployer une application?\",\n",
    "    \"Python pour l'analyse de donn√©es\",\n",
    "    \"Recherche vectorielle et embeddings\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üîç Query: {query}\")\n",
    "    print('='*80)\n",
    "    \n",
    "    results = search_similar(query, index_flat, embedding_model, df, k=3)\n",
    "    \n",
    "    for _, row in results.iterrows():\n",
    "        print(f\"{row['rank']}. [{row['category']}] {row['text'][:60]}... (sim: {row['similarity']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Index IVFPQ (Optimis√© pour Large Scale)\n",
    "\n",
    "### 6.1 Cr√©ation d'un index IVF (Inverted File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index_ivf(embeddings: np.ndarray, nlist: int = 10) -> faiss.IndexIVFFlat:\n",
    "    \"\"\"\n",
    "    Cr√©er un index FAISS IVF (plus rapide pour large scale)\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Vecteurs √† indexer\n",
    "        nlist: Nombre de clusters (voronoi cells)\n",
    "    \"\"\"\n",
    "    dimension = embeddings.shape[1]\n",
    "    \n",
    "    # Cr√©er le quantizer (index flat pour les centroids)\n",
    "    quantizer = faiss.IndexFlatL2(dimension)\n",
    "    \n",
    "    # Cr√©er l'index IVF\n",
    "    index = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n",
    "    \n",
    "    # Entra√Æner l'index (clustering)\n",
    "    index.train(embeddings)\n",
    "    \n",
    "    # Ajouter les vecteurs\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return index\n",
    "\n",
    "# Pour notre petit dataset, utilisons nlist=5\n",
    "index_ivf = create_faiss_index_ivf(embeddings, nlist=5)\n",
    "\n",
    "print(f\"‚úÖ Index IVF cr√©√©\")\n",
    "print(f\"   - Nombre de vecteurs: {index_ivf.ntotal}\")\n",
    "print(f\"   - Nombre de clusters: {index_ivf.nlist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Comparaison Flat vs IVF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_search(query: str, index: faiss.Index, model: SentenceTransformer, k: int = 5):\n",
    "    \"\"\"\n",
    "    Mesurer le temps de recherche\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query]).astype('float32')\n",
    "    \n",
    "    start = time.time()\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    return elapsed * 1000  # en ms\n",
    "\n",
    "# Benchmark\n",
    "query = \"Qu'est-ce que le machine learning?\"\n",
    "\n",
    "time_flat = benchmark_search(query, index_flat, embedding_model)\n",
    "time_ivf = benchmark_search(query, index_ivf, embedding_model)\n",
    "\n",
    "print(f\"‚è±Ô∏è Temps de recherche:\")\n",
    "print(f\"   - Flat (exact): {time_flat:.3f} ms\")\n",
    "print(f\"   - IVF (approx): {time_ivf:.3f} ms\")\n",
    "print(f\"   - Speedup: {time_flat/time_ivf:.2f}x\")\n",
    "print(f\"\\n‚ö†Ô∏è Note: Sur de petits datasets, Flat peut √™tre plus rapide.\")\n",
    "print(f\"   IVF devient int√©ressant √† partir de 10k-100k+ vecteurs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. √âvaluation: Recall et Precision\n",
    "\n",
    "### 7.1 Cr√©ation d'un ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir des queries avec leurs documents pertinents attendus (ground truth)\n",
    "ground_truth = [\n",
    "    {\n",
    "        \"query\": \"Qu'est-ce qu'un transformer?\",\n",
    "        \"relevant_ids\": [5, 6, 15]  # Docs sur transformers, BERT, attention\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Comment utiliser Python?\",\n",
    "        \"relevant_ids\": [3, 4, 16]  # Docs sur Python, FastAPI, Pandas\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Machine learning et optimisation\",\n",
    "        \"relevant_ids\": [1, 7, 12]  # Docs sur ML, gradient descent, fine-tuning\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Recherche vectorielle\",\n",
    "        \"relevant_ids\": [8, 18, 19]  # Docs sur embeddings, FAISS, vector DB\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Ground truth d√©fini pour l'√©valuation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Calcul de Recall@k et Precision@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(query: str, relevant_ids: List[int], index: faiss.Index, \n",
    "                      model: SentenceTransformer, df: pd.DataFrame, k: int = 5):\n",
    "    \"\"\"\n",
    "    Calculer Recall@k et Precision@k\n",
    "    \"\"\"\n",
    "    # R√©cup√©rer les r√©sultats\n",
    "    query_embedding = model.encode([query]).astype('float32')\n",
    "    _, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    # IDs r√©cup√©r√©s\n",
    "    retrieved_ids = [df.iloc[idx]['id'] for idx in indices[0]]\n",
    "    \n",
    "    # Calculer les m√©triques\n",
    "    relevant_retrieved = set(relevant_ids) & set(retrieved_ids)\n",
    "    \n",
    "    recall = len(relevant_retrieved) / len(relevant_ids) if relevant_ids else 0\n",
    "    precision = len(relevant_retrieved) / k\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"k\": k,\n",
    "        \"relevant_ids\": relevant_ids,\n",
    "        \"retrieved_ids\": retrieved_ids,\n",
    "        \"relevant_retrieved\": list(relevant_retrieved),\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision\n",
    "    }\n",
    "\n",
    "# √âvaluer toutes les queries\n",
    "evaluation_results = []\n",
    "\n",
    "for item in ground_truth:\n",
    "    result = evaluate_retrieval(\n",
    "        query=item[\"query\"],\n",
    "        relevant_ids=item[\"relevant_ids\"],\n",
    "        index=index_flat,\n",
    "        model=embedding_model,\n",
    "        df=df,\n",
    "        k=5\n",
    "    )\n",
    "    evaluation_results.append(result)\n",
    "    \n",
    "    print(f\"\\nüîç Query: {result['query']}\")\n",
    "    print(f\"   Relevant: {result['relevant_ids']}\")\n",
    "    print(f\"   Retrieved: {result['retrieved_ids']}\")\n",
    "    print(f\"   ‚úÖ Found: {result['relevant_retrieved']}\")\n",
    "    print(f\"   üìä Recall@{result['k']}: {result['recall']:.2%}\")\n",
    "    print(f\"   üìä Precision@{result['k']}: {result['precision']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 M√©triques moyennes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_recall = np.mean([r['recall'] for r in evaluation_results])\n",
    "avg_precision = np.mean([r['precision'] for r in evaluation_results])\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"üìä M√âTRIQUES MOYENNES\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Recall@5:    {avg_recall:.2%}\")\n",
    "print(f\"Precision@5: {avg_precision:.2%}\")\n",
    "\n",
    "# Visualisation\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "queries = [r['query'][:30] + '...' for r in evaluation_results]\n",
    "recalls = [r['recall'] for r in evaluation_results]\n",
    "precisions = [r['precision'] for r in evaluation_results]\n",
    "\n",
    "ax[0].barh(queries, recalls, color='skyblue')\n",
    "ax[0].set_xlabel('Recall@5')\n",
    "ax[0].set_title('Recall par Query')\n",
    "ax[0].set_xlim(0, 1)\n",
    "\n",
    "ax[1].barh(queries, precisions, color='lightcoral')\n",
    "ax[1].set_xlabel('Precision@5')\n",
    "ax[1].set_title('Precision par Query')\n",
    "ax[1].set_xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sauvegarde et Chargement de l'Index\n",
    "\n",
    "### 8.1 Sauvegarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_index(index: faiss.Index, embeddings: np.ndarray, df: pd.DataFrame, \n",
    "               index_path: str = \"faiss_index.bin\", \n",
    "               data_path: str = \"documents.pkl\"):\n",
    "    \"\"\"\n",
    "    Sauvegarder l'index FAISS et les donn√©es associ√©es\n",
    "    \"\"\"\n",
    "    # Sauvegarder l'index FAISS\n",
    "    faiss.write_index(index, index_path)\n",
    "    \n",
    "    # Sauvegarder les donn√©es (documents + embeddings)\n",
    "    data = {\n",
    "        \"documents\": df,\n",
    "        \"embeddings\": embeddings\n",
    "    }\n",
    "    with open(data_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    \n",
    "    print(f\"‚úÖ Index sauvegard√©: {index_path}\")\n",
    "    print(f\"‚úÖ Donn√©es sauvegard√©es: {data_path}\")\n",
    "\n",
    "# Sauvegarder\n",
    "save_index(index_flat, embeddings, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Chargement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index(index_path: str = \"faiss_index.bin\", \n",
    "               data_path: str = \"documents.pkl\"):\n",
    "    \"\"\"\n",
    "    Charger l'index FAISS et les donn√©es associ√©es\n",
    "    \"\"\"\n",
    "    # Charger l'index\n",
    "    index = faiss.read_index(index_path)\n",
    "    \n",
    "    # Charger les donn√©es\n",
    "    with open(data_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Index charg√©: {index.ntotal} vecteurs\")\n",
    "    print(f\"‚úÖ Donn√©es charg√©es: {len(data['documents'])} documents\")\n",
    "    \n",
    "    return index, data['documents'], data['embeddings']\n",
    "\n",
    "# Test de chargement\n",
    "loaded_index, loaded_df, loaded_embeddings = load_index()\n",
    "\n",
    "# V√©rifier que √ßa fonctionne\n",
    "test_query = \"Qu'est-ce que l'IA?\"\n",
    "test_results = search_similar(test_query, loaded_index, embedding_model, loaded_df, k=3)\n",
    "print(f\"\\nüîç Test query: {test_query}\")\n",
    "print(test_results[['rank', 'text']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercices Pratiques\n",
    "\n",
    "### Exercice 1: Ajouter de nouveaux documents\n",
    "1. Cr√©er 10 nouveaux documents\n",
    "2. G√©n√©rer leurs embeddings\n",
    "3. Les ajouter √† l'index existant\n",
    "4. Tester la recherche\n",
    "\n",
    "### Exercice 2: Optimiser les hyperparam√®tres\n",
    "1. Tester diff√©rentes valeurs de `nlist` pour IVF\n",
    "2. Mesurer l'impact sur la vitesse et le recall\n",
    "3. Trouver le meilleur compromis\n",
    "\n",
    "### Exercice 3: Impl√©menter un filtre de m√©tadonn√©es\n",
    "1. Modifier `search_similar` pour filtrer par cat√©gorie\n",
    "2. Exemple: \"Chercher uniquement dans les docs NLP\"\n",
    "3. Comparer les r√©sultats avec/sans filtre\n",
    "\n",
    "### Exercice 4: √âvaluation avanc√©e\n",
    "1. Impl√©menter Mean Reciprocal Rank (MRR)\n",
    "2. Calculer NDCG@k\n",
    "3. Comparer Flat vs IVF sur ces m√©triques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Concepts Avanc√©s\n",
    "\n",
    "### 10.1 Types d'index FAISS\n",
    "\n",
    "| Type | Description | Usage |\n",
    "|------|-------------|-------|\n",
    "| **Flat** | Brute force, exact | < 10K vecteurs, besoin de pr√©cision maximale |\n",
    "| **IVF** | Inverted file, approximatif | 10K - 10M vecteurs |\n",
    "| **IVFPQ** | IVF + Product Quantization | 10M+ vecteurs, compression |\n",
    "| **HNSW** | Hierarchical NSW graph | Tr√®s rapide, m√©moire √©lev√©e |\n",
    "\n",
    "### 10.2 M√©triques de distance\n",
    "- **L2 (Euclidean)**: Distance euclidienne classique\n",
    "- **Inner Product**: Produit scalaire (pour vecteurs normalis√©s = cosine)\n",
    "- **Cosine**: Angle entre vecteurs\n",
    "\n",
    "### 10.3 Librairies alternatives\n",
    "- **Milvus**: Vector DB distribu√©e, production-ready\n",
    "- **Weaviate**: Vector DB avec GraphQL\n",
    "- **Pinecone**: Vector DB manag√©e (cloud)\n",
    "- **Qdrant**: Vector DB en Rust, performante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Ressources\n",
    "\n",
    "- [FAISS Documentation](https://github.com/facebookresearch/faiss/wiki)\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "- [Understanding Vector Search](https://www.pinecone.io/learn/vector-search/)\n",
    "- [Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)\n",
    "\n",
    "## ‚úÖ Checklist\n",
    "\n",
    "- [ ] Embeddings cr√©√©s pour tous les documents\n",
    "- [ ] Index FAISS Flat impl√©ment√©\n",
    "- [ ] Index FAISS IVF impl√©ment√©\n",
    "- [ ] Recherche de similarit√© test√©e\n",
    "- [ ] Recall et Precision calcul√©s\n",
    "- [ ] Index sauvegard√© localement\n",
    "- [ ] Benchmark de performance effectu√©\n",
    "\n",
    "---\n",
    "\n",
    "**Session S10 compl√©t√©e! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
