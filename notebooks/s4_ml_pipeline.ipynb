{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# S√©ance 4 ‚Äî Scikit-learn : Pipeline ML rapide\n",
    "\n",
    "## üéØ Objectifs\n",
    "- Comprendre et utiliser les pipelines scikit-learn\n",
    "- Ma√Ætriser le preprocessing de donn√©es textuelles\n",
    "- Impl√©menter une √©valuation rigoureuse des mod√®les\n",
    "- Construire une baseline de classification binaire\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Introduction\n",
    "\n",
    "**Scikit-learn** est la biblioth√®que de r√©f√©rence pour le machine learning en Python. Les **pipelines** permettent d'encha√Æner plusieurs √©tapes de preprocessing et de mod√©lisation de mani√®re propre et reproductible.\n",
    "\n",
    "**Installation :**\n",
    "```bash\n",
    "pip install scikit-learn pandas numpy matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-creation",
   "metadata": {},
   "source": [
    "## üìä 1. Cr√©ation du dataset\n",
    "\n",
    "Nous allons cr√©er un dataset de documents textuels class√©s en deux cat√©gories : **FAQ** et **Blog**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemples de textes FAQ (questions-r√©ponses courtes)\n",
    "faq_texts = [\n",
    "    \"Comment r√©initialiser mon mot de passe ?\",\n",
    "    \"Quels sont vos horaires d'ouverture ?\",\n",
    "    \"O√π puis-je trouver ma facture ?\",\n",
    "    \"Comment contacter le support technique ?\",\n",
    "    \"Quels modes de paiement acceptez-vous ?\",\n",
    "    \"Comment suivre ma commande ?\",\n",
    "    \"Puis-je retourner un article ?\",\n",
    "    \"Quelle est votre politique de remboursement ?\",\n",
    "    \"Comment cr√©er un compte ?\",\n",
    "    \"O√π est situ√© votre magasin ?\"\n",
    "] * 15  # R√©p√©ter pour avoir plus de donn√©es\n",
    "\n",
    "# Exemples de textes Blog (articles plus longs)\n",
    "blog_texts = [\n",
    "    \"Dans cet article, nous allons explorer les tendances du machine learning pour 2024. L'intelligence artificielle continue de r√©volutionner notre fa√ßon de travailler et d'interagir avec la technologie. Les mod√®les de langage comme GPT-4 et leurs applications sont de plus en plus sophistiqu√©s.\",\n",
    "    \"Le d√©veloppement web moderne n√©cessite une compr√©hension approfondie de plusieurs technologies. JavaScript, React, et les frameworks backend comme Django ou FastAPI sont essentiels. Dans ce guide complet, nous aborderons les meilleures pratiques pour construire des applications robustes et scalables.\",\n",
    "    \"L'analyse de donn√©es est devenue une comp√©tence cruciale dans le monde professionnel actuel. Python et ses biblioth√®ques comme Pandas, NumPy et Matplotlib offrent des outils puissants. D√©couvrez comment transformer vos donn√©es brutes en insights actionnables gr√¢ce √† ces techniques avanc√©es.\",\n",
    "    \"La cybers√©curit√© est un enjeu majeur pour toutes les entreprises. Les attaques informatiques se multiplient et deviennent de plus en plus sophistiqu√©es. Dans cet article d√©taill√©, nous explorons les meilleures strat√©gies de protection et les outils essentiels pour s√©curiser votre infrastructure.\",\n",
    "    \"Le cloud computing a transform√© la fa√ßon dont les entreprises d√©ploient et g√®rent leurs applications. AWS, Azure et Google Cloud Platform offrent des services vari√©s et puissants. Nous allons examiner les avantages et inconv√©nients de chaque plateforme pour vous aider √† faire le bon choix.\",\n",
    "    \"L'optimisation des performances web est cruciale pour l'exp√©rience utilisateur. Des temps de chargement rapides peuvent significativement am√©liorer les taux de conversion. Dans ce tutoriel approfondi, nous verrons comment optimiser vos images, minimiser le JavaScript et impl√©menter le lazy loading.\",\n",
    "    \"Les bases de donn√©es NoSQL comme MongoDB et Cassandra offrent une alternative flexible aux bases relationnelles traditionnelles. Leur architecture distribu√©e permet une scalabilit√© horizontale impressionnante. D√©couvrez quand et comment utiliser ces technologies dans vos projets.\",\n",
    "    \"Le DevOps est une culture et un ensemble de pratiques qui visent √† unifier le d√©veloppement et les op√©rations. L'automatisation avec des outils comme Jenkins, Docker et Kubernetes est essentielle. Apprenez √† mettre en place une pipeline CI/CD efficace pour votre √©quipe.\",\n",
    "    \"L'apprentissage profond r√©volutionne la vision par ordinateur et le traitement du langage naturel. Les r√©seaux de neurones convolutifs et les transformers sont au c≈ìur de ces avanc√©es. Dans ce guide technique, nous explorerons l'architecture de ces mod√®les et leurs applications pratiques.\",\n",
    "    \"Le design UX/UI est bien plus qu'une simple question d'esth√©tique. Il s'agit de cr√©er des exp√©riences utilisateur intuitives et agr√©ables. Nous explorerons les principes fondamentaux du design centr√© sur l'utilisateur et comment les appliquer dans vos projets.\"\n",
    "] * 15  # R√©p√©ter pour avoir plus de donn√©es\n",
    "\n",
    "# Cr√©ation du DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'text': faq_texts + blog_texts,\n",
    "    'category': ['FAQ'] * len(faq_texts) + ['Blog'] * len(blog_texts)\n",
    "})\n",
    "\n",
    "# M√©langer les donn√©es\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset cr√©√© avec {len(df)} documents\")\n",
    "print(f\"\\nDistribution des classes:\")\n",
    "print(df['category'].value_counts())\n",
    "print(\"\\nExemples:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda",
   "metadata": {},
   "source": [
    "## üîç 2. Analyse exploratoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exploratory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longueur des textes\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df['word_count'] = df['text'].str.split().str.len()\n",
    "\n",
    "# Statistiques par cat√©gorie\n",
    "print(\"=\" * 60)\n",
    "print(\"STATISTIQUES PAR CAT√âGORIE\")\n",
    "print(\"=\" * 60)\n",
    "stats = df.groupby('category')[['text_length', 'word_count']].agg(['mean', 'std', 'min', 'max'])\n",
    "print(stats)\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogramme longueur des textes\n",
    "for cat in df['category'].unique():\n",
    "    data = df[df['category'] == cat]['text_length']\n",
    "    axes[0].hist(data, alpha=0.6, label=cat, bins=20)\n",
    "axes[0].set_xlabel('Longueur du texte (caract√®res)')\n",
    "axes[0].set_ylabel('Fr√©quence')\n",
    "axes[0].set_title('Distribution de la longueur des textes')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Histogramme nombre de mots\n",
    "for cat in df['category'].unique():\n",
    "    data = df[df['category'] == cat]['word_count']\n",
    "    axes[1].hist(data, alpha=0.6, label=cat, bins=20)\n",
    "axes[1].set_xlabel('Nombre de mots')\n",
    "axes[1].set_ylabel('Fr√©quence')\n",
    "axes[1].set_title('Distribution du nombre de mots')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-data",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è 3. Split des donn√©es (Train/Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-test-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©paration des donn√©es\n",
    "X = df['text']\n",
    "y = df['category']\n",
    "\n",
    "# Split stratifi√© (maintient les proportions de classes)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Taille ensemble d'entra√Ænement: {len(X_train)}\")\n",
    "print(f\"Taille ensemble de test: {len(X_test)}\")\n",
    "print(f\"\\nDistribution train:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nDistribution test:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tfidf",
   "metadata": {},
   "source": [
    "## üî§ 4. TF-IDF Vectorization\n",
    "\n",
    "**TF-IDF** (Term Frequency - Inverse Document Frequency) convertit le texte en vecteurs num√©riques en pond√©rant l'importance des mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tfidf-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation et entra√Ænement du vectoriseur\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=100,  # Garder les 100 mots les plus importants\n",
    "    stop_words='english',  # Retirer les mots vides anglais\n",
    "    ngram_range=(1, 2),  # Unigrammes et bigrammes\n",
    "    min_df=2  # Mot doit appara√Ætre dans au moins 2 documents\n",
    ")\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Shape matrice TF-IDF (train): {X_train_tfidf.shape}\")\n",
    "print(f\"Shape matrice TF-IDF (test): {X_test_tfidf.shape}\")\n",
    "print(f\"\\nNombre de features: {len(vectorizer.get_feature_names_out())}\")\n",
    "print(f\"\\nExemples de features:\")\n",
    "print(vectorizer.get_feature_names_out()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline",
   "metadata": {},
   "source": [
    "## üîó 5. Construction du Pipeline\n",
    "\n",
    "Un **Pipeline** combine plusieurs √©tapes (preprocessing + model) en un seul objet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline avec Logistic Regression\n",
    "pipeline_lr = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=100, ngram_range=(1, 2))),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# Pipeline avec Naive Bayes\n",
    "pipeline_nb = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=100, ngram_range=(1, 2))),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Pipeline avec Random Forest\n",
    "pipeline_rf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=100, ngram_range=(1, 2))),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Pipelines cr√©√©s!\")\n",
    "print(f\"\\nPipeline 1 (Logistic Regression):\\n{pipeline_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training",
   "metadata": {},
   "source": [
    "## üéì 6. Entra√Ænement des mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire de pipelines\n",
    "pipelines = {\n",
    "    'Logistic Regression': pipeline_lr,\n",
    "    'Naive Bayes': pipeline_nb,\n",
    "    'Random Forest': pipeline_rf\n",
    "}\n",
    "\n",
    "# Entra√Ænement\n",
    "print(\"Entra√Ænement des mod√®les...\\n\")\n",
    "for name, pipeline in pipelines.items():\n",
    "    print(f\"Entra√Ænement {name}...\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    print(f\"‚úÖ {name} entra√Æn√©!\\n\")\n",
    "\n",
    "print(\"Tous les mod√®les sont entra√Æn√©s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation",
   "metadata": {},
   "source": [
    "## üìä 7. √âvaluation et m√©triques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation de tous les mod√®les\n",
    "results = []\n",
    "\n",
    "for name, pipeline in pipelines.items():\n",
    "    # Pr√©dictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calcul des m√©triques\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, pos_label='Blog')\n",
    "    recall = recall_score(y_test, y_pred, pos_label='Blog')\n",
    "    f1 = f1_score(y_test, y_pred, pos_label='Blog')\n",
    "    \n",
    "    results.append({\n",
    "        'Mod√®le': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    })\n",
    "\n",
    "# Affichage des r√©sultats\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"=\" * 80)\n",
    "print(\"R√âSULTATS DES MOD√àLES\")\n",
    "print(\"=\" * 80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\nNote: Les m√©triques sont calcul√©es pour la classe 'Blog' (positive)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des m√©triques\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.2\n",
    "\n",
    "ax.bar(x - 1.5*width, results_df['Accuracy'], width, label='Accuracy', alpha=0.8)\n",
    "ax.bar(x - 0.5*width, results_df['Precision'], width, label='Precision', alpha=0.8)\n",
    "ax.bar(x + 0.5*width, results_df['Recall'], width, label='Recall', alpha=0.8)\n",
    "ax.bar(x + 1.5*width, results_df['F1-Score'], width, label='F1-Score', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Mod√®les', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Comparaison des m√©triques par mod√®le', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results_df['Mod√®le'])\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1.1])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confusion-matrix",
   "metadata": {},
   "source": [
    "## üéØ 8. Matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrices de confusion pour tous les mod√®les\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for idx, (name, pipeline) in enumerate(pipelines.items()):\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Affichage\n",
    "    im = axes[idx].imshow(cm, cmap='Blues', aspect='auto')\n",
    "    axes[idx].set_xticks([0, 1])\n",
    "    axes[idx].set_yticks([0, 1])\n",
    "    axes[idx].set_xticklabels(['Blog', 'FAQ'])\n",
    "    axes[idx].set_yticklabels(['Blog', 'FAQ'])\n",
    "    axes[idx].set_xlabel('Pr√©diction')\n",
    "    axes[idx].set_ylabel('V√©rit√©')\n",
    "    axes[idx].set_title(f'Matrice de confusion - {name}')\n",
    "    \n",
    "    # Ajouter les valeurs\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            text = axes[idx].text(j, i, cm[i, j],\n",
    "                                 ha=\"center\", va=\"center\", color=\"black\", fontsize=16)\n",
    "    \n",
    "    plt.colorbar(im, ax=axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-report",
   "metadata": {},
   "source": [
    "## üìã 9. Rapport de classification d√©taill√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rapport d√©taill√© pour le meilleur mod√®le (Logistic Regression)\n",
    "best_model = pipelines['Logistic Regression']\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RAPPORT DE CLASSIFICATION - Logistic Regression\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-validation",
   "metadata": {},
   "source": [
    "## üîÑ 10. Validation crois√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-val",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation pour le meilleur mod√®le\n",
    "print(\"Cross-validation (5-fold)...\\n\")\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    best_model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring='f1_weighted'\n",
    ")\n",
    "\n",
    "print(f\"Scores par fold: {cv_scores}\")\n",
    "print(f\"\\nMoyenne: {cv_scores.mean():.4f}\")\n",
    "print(f\"√âcart-type: {cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prediction-examples",
   "metadata": {},
   "source": [
    "## üîÆ 11. Pr√©dictions sur nouveaux exemples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "predict-new",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nouveaux textes √† classifier\n",
    "new_texts = [\n",
    "    \"Comment puis-je annuler ma commande ?\",\n",
    "    \"Dans cet article approfondi, nous explorons les derni√®res avanc√©es en intelligence artificielle et leurs implications pour l'avenir de la technologie. Les chercheurs travaillent sur des mod√®les de plus en plus sophistiqu√©s.\",\n",
    "    \"Quelle est votre adresse email ?\",\n",
    "    \"Le d√©veloppement d'applications mobiles n√©cessite une compr√©hension des frameworks modernes comme React Native et Flutter. Ces outils permettent de cr√©er des applications performantes pour iOS et Android.\"\n",
    "]\n",
    "\n",
    "# Pr√©dictions\n",
    "predictions = best_model.predict(new_texts)\n",
    "probabilities = best_model.predict_proba(new_texts)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PR√âDICTIONS SUR NOUVEAUX TEXTES\")\n",
    "print(\"=\" * 80)\n",
    "for i, text in enumerate(new_texts):\n",
    "    print(f\"\\nTexte {i+1}: {text[:80]}...\")\n",
    "    print(f\"Pr√©diction: {predictions[i]}\")\n",
    "    print(f\"Confiance: Blog={probabilities[i][0]:.2%}, FAQ={probabilities[i][1]:.2%}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise",
   "metadata": {},
   "source": [
    "## üéØ EXERCICE : Cr√©ez votre propre pipeline\n",
    "\n",
    "**Objectif**: Construire et √©valuer un pipeline de classification binaire.\n",
    "\n",
    "**Instructions**:\n",
    "1. Utilisez le m√™me dataset ou cr√©ez-en un nouveau\n",
    "2. Exp√©rimentez avec diff√©rents param√®tres TF-IDF:\n",
    "   - `max_features`: nombre de features √† garder\n",
    "   - `ngram_range`: unigrammes, bigrammes, trigrammes\n",
    "   - `min_df`, `max_df`: fr√©quence minimale/maximale\n",
    "3. Testez d'autres algorithmes (SVM, XGBoost, etc.)\n",
    "4. Comparez les performances avec diff√©rentes m√©triques\n",
    "5. Analysez les erreurs de classification\n",
    "\n",
    "**Questions √† explorer**:\n",
    "- Quel mod√®le donne les meilleurs r√©sultats ?\n",
    "- Quels param√®tres TF-IDF sont les plus efficaces ?\n",
    "- Y a-t-il du surapprentissage (overfitting) ?\n",
    "- Comment am√©liorer les performances ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOTRE CODE ICI\n",
    "# Exemple de structure:\n",
    "\n",
    "# 1. Cr√©er un nouveau pipeline\n",
    "# my_pipeline = Pipeline([\n",
    "#     ('tfidf', TfidfVectorizer(...)),\n",
    "#     ('classifier', ...)\n",
    "# ])\n",
    "\n",
    "# 2. Entra√Æner\n",
    "# my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 3. √âvaluer\n",
    "# y_pred = my_pipeline.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 4. Comparer avec les autres mod√®les\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## üìù Conclusion\n",
    "\n",
    "Dans cette s√©ance, nous avons appris √† :\n",
    "- Cr√©er des pipelines scikit-learn pour encha√Æner preprocessing et mod√©lisation\n",
    "- Utiliser TF-IDF pour vectoriser du texte\n",
    "- Impl√©menter une classification binaire baseline\n",
    "- √âvaluer les mod√®les avec plusieurs m√©triques (accuracy, precision, recall, F1)\n",
    "- Comparer diff√©rents algorithmes de classification\n",
    "- Utiliser la validation crois√©e pour estimer la performance\n",
    "\n",
    "**Points cl√©s √† retenir** :\n",
    "1. Les **pipelines** rendent le code plus propre et √©vitent les fuites de donn√©es\n",
    "2. **TF-IDF** est une m√©thode simple mais efficace pour le texte\n",
    "3. Le **train/test split stratifi√©** maintient les proportions de classes\n",
    "4. Plusieurs **m√©triques** sont n√©cessaires pour √©valuer un mod√®le (pas seulement accuracy)\n",
    "5. La **validation crois√©e** donne une estimation plus robuste des performances\n",
    "\n",
    "**Prochaines √©tapes** :\n",
    "- Exp√©rimenter avec d'autres features (longueur, mots sp√©ciaux, etc.)\n",
    "- Tester des mod√®les plus complexes (ensembles, deep learning)\n",
    "- Optimiser les hyperparam√®tres avec GridSearch\n",
    "- D√©ployer le mod√®le en production\n",
    "\n",
    "**Ressources suppl√©mentaires** :\n",
    "- Documentation scikit-learn: https://scikit-learn.org/\n",
    "- Guide sur les pipelines: https://scikit-learn.org/stable/modules/compose.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
